{
    "cost": 0.46907000000000004,
    "rewrites": {
        "triggerdotdev__trigger.dev.d1c3bfb9/apps/webapp/app/services/realtime/redisRealtimeStreams.server.ts": {
            "output": "import { Logger, LogLevel } from \"@trigger.dev/core/logger\";\nimport Redis, { RedisOptions } from \"ioredis\";\nimport { env } from \"~/env.server\";\nimport { StreamIngestor, StreamResponder, StreamResponseOptions } from \"./types\";\n\nexport type RealtimeStreamsOptions = {\n  redis: RedisOptions | undefined;\n  logger?: Logger;\n  logLevel?: LogLevel;\n  inactivityTimeoutMs?: number; // Close stream after this many ms of no new data (default: 60000)\n};\n\n// Legacy constant for backward compatibility (no longer written, but still recognized when reading)\nconst END_SENTINEL = \"<<CLOSE_STREAM>>\";\n\n// Internal types for stream pipeline\ntype StreamChunk =\n  | { type: \"ping\" }\n  | { type: \"data\"; redisId: string; data: string }\n  | { type: \"legacy-data\"; redisId: string; data: string };\n\n// Class implementing both interfaces\nexport class RedisRealtimeStreams implements StreamIngestor, StreamResponder {\n  private logger: Logger;\n  private inactivityTimeoutMs: number;\n\n  constructor(private options: RealtimeStreamsOptions) {\n    this.logger = options.logger ?? new Logger(\"RedisRealtimeStreams\", options.logLevel ?? \"info\");\n    this.inactivityTimeoutMs = options.inactivityTimeoutMs ?? 60000; // Default: 60 seconds\n  }\n\n  async initializeStream(\n    runId: string,\n    streamId: string\n  ): Promise<{ responseHeaders?: Record<string, string> }> {\n    return {};\n  }\n\n  async streamResponse(\n    request: Request,\n    runId: string,\n    streamId: string,\n    signal: AbortSignal,\n    options?: StreamResponseOptions\n  ): Promise<Response> {\n    const redis = new Redis(this.options.redis ?? {});\n    const streamKey = `stream:${runId}:${streamId}`;\n    let isCleanedUp = false;\n\n    const stream = new ReadableStream<StreamChunk>({\n      start: async (controller) => {\n        // Start from lastEventId if provided, otherwise from beginning\n        let lastId = options?.lastEventId ?? \"0\";\n        let retryCount = 0;\n        const maxRetries = 3;\n        let lastDataTime = Date.now();\n        let lastEnqueueTime = Date.now();\n        const blockTimeMs = 5000;\n        const pingIntervalMs = 10000; // 10 seconds\n\n        if (options?.lastEventId) {\n          this.logger.debug(\"[RealtimeStreams][streamResponse] Resuming from lastEventId\", {\n            streamKey,\n            lastEventId: options?.lastEventId,\n          });\n        }\n\n        try {\n          while (!signal.aborted) {\n            // Check if we need to send a ping\n            const timeSinceLastEnqueue = Date.now() - lastEnqueueTime;\n            if (timeSinceLastEnqueue >= pingIntervalMs) {\n              controller.enqueue({ type: \"ping\" });\n              lastEnqueueTime = Date.now();\n            }\n\n            // Compute inactivity threshold once to use consistently in both branches\n            const inactivityThresholdMs = options?.timeoutInSeconds\n              ? options.timeoutInSeconds * 1000\n              : this.inactivityTimeoutMs;\n\n            try {\n              const messages = await redis.xread(\n                \"COUNT\",\n                100,\n                \"BLOCK\",\n                blockTimeMs,\n                \"STREAMS\",\n                streamKey,\n                lastId\n              );\n\n              retryCount = 0;\n\n              if (messages && messages.length > 0) {\n                const [_key, entries] = messages[0];\n                let foundData = false;\n\n                for (let i = 0; i < entries.length; i++) {\n                  const [id, fields] = entries[i];\n                  lastId = id;\n\n                  if (fields && fields.length >= 2) {\n                    // Extract the data field from the Redis entry\n                    // Fields format: [\"field1\", \"value1\", \"field2\", \"value2\", ...]\n                    let data: string | null = null;\n\n                    for (let j = 0; j < fields.length; j += 2) {\n                      if (fields[j] === \"data\") {\n                        data = fields[j + 1];\n                        break;\n                      }\n                    }\n\n                    // Handle legacy entries that don't have field names (just data at index 1)\n                    if (data === null && fields.length >= 2) {\n                      data = fields[1];\n                    }\n\n                    if (data) {\n                      // Skip legacy END_SENTINEL entries (backward compatibility)\n                      if (data === END_SENTINEL) {\n                        continue;\n                      }\n\n                      // Enqueue structured chunk with Redis stream ID\n                      controller.enqueue({\n                        type: \"data\",\n                        redisId: id,\n                        data,\n                      });\n\n                      foundData = true;\n                      lastDataTime = Date.now();\n                      lastEnqueueTime = Date.now();\n\n                      if (signal.aborted) {\n                        controller.close();\n                        return;\n                      }\n                    }\n                  }\n                }\n\n                // If we didn't find any data in this batch, might have only seen sentinels\n                if (!foundData) {\n                  // Check for inactivity timeout\n                  const inactiveMs = Date.now() - lastDataTime;\n                  if (inactiveMs >= inactivityThresholdMs) {\n                    this.logger.debug(\n                      \"[RealtimeStreams][streamResponse] Closing stream due to inactivity\",\n                      {\n                        streamKey,\n                        inactiveMs,\n                        threshold: inactivityThresholdMs,\n                      }\n                    );\n                    controller.close();\n                    return;\n                  }\n                }\n              } else {\n                // No messages received (timed out on BLOCK)\n                // Check for inactivity timeout\n                const inactiveMs = Date.now() - lastDataTime;\n                if (inactiveMs >= inactivityThresholdMs) {\n                  this.logger.debug(\n                    \"[RealtimeStreams][streamResponse] Closing stream due to inactivity\",\n                    {\n                      streamKey,\n                      inactiveMs,\n                      threshold: inactivityThresholdMs,\n                    }\n                  );\n                  controller.close();\n                  return;\n                }\n              }\n            } catch (error) {\n              if (signal.aborted) break;\n\n              this.logger.error(\n                \"[RealtimeStreams][streamResponse] Error reading from Redis stream:\",\n                {\n                  error,\n                }\n              );\n              retryCount++;\n              if (retryCount >= maxRetries) throw error;\n              await new Promise((resolve) => setTimeout(resolve, 1000 * retryCount));\n            }\n          }\n        } catch (error) {\n          this.logger.error(\"[RealtimeStreams][streamResponse] Fatal error in stream processing:\", {\n            error,\n          });\n          controller.error(error);\n        } finally {\n          await cleanup();\n        }\n      },\n      cancel: async () => {\n        await cleanup();\n      },\n    })\n      .pipeThrough(\n        // Transform 1: Split data content by newlines, preserving metadata\n        new TransformStream<StreamChunk, StreamChunk & { line?: string }>({\n          transform(chunk, controller) {\n            if (chunk.type === \"ping\") {\n              controller.enqueue(chunk);\n            } else if (chunk.type === \"data\" || chunk.type === \"legacy-data\") {\n              // Split data by newlines, emit separate chunks with same metadata\n              const lines = chunk.data.split(\"\\n\").filter((line) => line.trim().length > 0);\n              for (const line of lines) {\n                controller.enqueue({ ...chunk, line });\n              }\n            }\n          },\n        })\n      )\n      .pipeThrough(\n        // Transform 2: Format as SSE\n        new TransformStream<StreamChunk & { line?: string }, string>({\n          transform(chunk, controller) {\n            if (chunk.type === \"ping\") {\n              controller.enqueue(`: ping\\n\\n`);\n            } else if ((chunk.type === \"data\" || chunk.type === \"legacy-data\") && chunk.line) {\n              // Use Redis stream ID as SSE event ID\n              controller.enqueue(`id: ${chunk.redisId}\\ndata: ${chunk.line}\\n\\n`);\n            }\n          },\n        })\n      )\n      .pipeThrough(new TextEncoderStream());\n\n    async function cleanup() {\n      if (isCleanedUp) return;\n      isCleanedUp = true;\n      await redis.quit().catch(console.error);\n    }\n\n    signal.addEventListener(\"abort\", cleanup);\n\n    return new Response(stream, {\n      headers: {\n        \"Content-Type\": \"text/event-stream\",\n        \"Cache-Control\": \"no-cache\",\n        Connection: \"keep-alive\",\n      },\n    });\n  }\n\n  async ingestData(\n    stream: ReadableStream<Uint8Array>,\n    runId: string,\n    streamId: string,\n    clientId: string,\n    resumeFromChunk?: number\n  ): Promise<Response> {\n    const redis = new Redis(this.options.redis ?? {});\n    const streamKey = `stream:${runId}:${streamId}`;\n    const startChunk = resumeFromChunk ?? 0;\n    // Start counting from the resume point, not from 0\n    let currentChunkIndex = startChunk;\n\n    const self = this;\n\n    async function cleanup() {\n      try {\n        await redis.quit();\n      } catch (error) {\n        self.logger.error(\"[RedisRealtimeStreams][ingestData] Error in cleanup:\", { error });\n      }\n    }\n\n    try {\n      const textStream = stream.pipeThrough(new TextDecoderStream());\n      const reader = textStream.getReader();\n\n      while (true) {\n        const { done, value } = await reader.read();\n\n        if (done || !value) {\n          break;\n        }\n\n        // Write each chunk with its index and clientId\n        this.logger.debug(\"[RedisRealtimeStreams][ingestData] Writing chunk\", {\n          streamKey,\n          runId,\n          clientId,\n          chunkIndex: currentChunkIndex,\n          resumeFromChunk: startChunk,\n          value,\n        });\n\n        await redis.xadd(\n          streamKey,\n          \"MAXLEN\",\n          \"~\",\n          String(env.REALTIME_STREAM_MAX_LENGTH),\n          \"*\",\n          \"clientId\",\n          clientId,\n          \"chunkIndex\",\n          currentChunkIndex.toString(),\n          \"data\",\n          value\n        );\n\n        currentChunkIndex++;\n      }\n\n      // Set TTL for cleanup when stream is done\n      await redis.expire(streamKey, env.REALTIME_STREAM_TTL);\n\n      return new Response(null, { status: 200 });\n    } catch (error) {\n      if (error instanceof Error) {\n        if (\"code\" in error && error.code === \"ECONNRESET\") {\n          this.logger.info(\"[RealtimeStreams][ingestData] Connection reset during ingestData:\", {\n            error,\n          });\n          return new Response(null, { status: 500 });\n        }\n      }\n\n      this.logger.error(\"[RealtimeStreams][ingestData] Error in ingestData:\", { error });\n\n      return new Response(null, { status: 500 });\n    } finally {\n      await cleanup();\n    }\n  }\n\n  async appendPart(part: string, partId: string, runId: string, streamId: string): Promise<void> {\n    const redis = new Redis(this.options.redis ?? {});\n    const streamKey = `stream:${runId}:${streamId}`;\n\n    await redis.xadd(\n      streamKey,\n      \"MAXLEN\",\n      \"~\",\n      String(env.REALTIME_STREAM_MAX_LENGTH),\n      \"*\",\n      \"clientId\",\n      \"\",\n      \"chunkIndex\",\n      \"0\",\n      \"data\",\n      JSON.stringify(part) + \"\\n\"\n    );\n\n    // Set TTL for cleanup when stream is done\n    await redis.expire(streamKey, env.REALTIME_STREAM_TTL);\n\n    await redis.quit();\n  }\n\n  async getLastChunkIndex(runId: string, streamId: string, clientId: string): Promise<number> {\n    const redis = new Redis(this.options.redis ?? {});\n    const streamKey = `stream:${runId}:${streamId}`;\n\n    try {\n      // Paginate through the stream from newest to oldest until we find this client's last chunk\n      const batchSize = 100;\n      let lastId = \"+\"; // Start from newest\n\n      while (true) {\n        const entries = await redis.xrevrange(streamKey, lastId, \"-\", \"COUNT\", batchSize);\n\n        if (!entries || entries.length === 0) {\n          // Reached the beginning of the stream, no chunks from this client\n          this.logger.debug(\n            \"[RedisRealtimeStreams][getLastChunkIndex] No chunks found for client\",\n            {\n              streamKey,\n              clientId,\n            }\n          );\n          return -1;\n        }\n\n        // Search through this batch for the client's last chunk\n        for (const [id, fields] of entries) {\n          let entryClientId: string | null = null;\n          let chunkIndex: number | null = null;\n          let data: string | null = null;\n\n          for (let i = 0; i < fields.length; i += 2) {\n            if (fields[i] === \"clientId\") {\n              entryClientId = fields[i + 1];\n            }\n            if (fields[i] === \"chunkIndex\") {\n              chunkIndex = parseInt(fields[i + 1], 10);\n            }\n            if (fields[i] === \"data\") {\n              data = fields[i + 1];\n            }\n          }\n\n          // Skip legacy END_SENTINEL entries (backward compatibility)\n          if (data === END_SENTINEL) {\n            continue;\n          }\n\n          // Check if this entry is from our client and has a chunkIndex\n          if (entryClientId === clientId && chunkIndex !== null) {\n            this.logger.debug(\"[RedisRealtimeStreams][getLastChunkIndex] Found last chunk\", {\n              streamKey,\n              clientId,\n              chunkIndex,\n            });\n            return chunkIndex;\n          }\n        }\n\n        // Move to next batch (older entries)\n        // Use the ID of the last entry in this batch as the new cursor\n        lastId = `(${entries[entries.length - 1][0]}`; // Exclusive range with (\n      }\n    } catch (error) {\n      this.logger.error(\"[RedisRealtimeStreams][getLastChunkIndex] Error getting last chunk:\", {\n        error,\n        streamKey,\n        clientId,\n      });\n      // Return -1 to indicate we don't know what the server has\n      return -1;\n    } finally {\n      await redis.quit().catch((err) => {\n        this.logger.error(\"[RedisRealtimeStreams][getLastChunkIndex] Error in cleanup:\", { err });\n      });\n    }\n  }\n}",
            "output_extracted": "import { Logger, LogLevel } from \"@trigger.dev/core/logger\";\nimport Redis, { RedisOptions } from \"ioredis\";\nimport { env } from \"~/env.server\";\nimport { StreamIngestor, StreamResponder, StreamResponseOptions } from \"./types\";\n\nexport type RealtimeStreamsOptions = {\n  redis: RedisOptions | undefined;\n  logger?: Logger;\n  logLevel?: LogLevel;\n  inactivityTimeoutMs?: number; // Close stream after this many ms of no new data (default: 60000)\n};\n\n// Legacy constant for backward compatibility (no longer written, but still recognized when reading)\nconst END_SENTINEL = \"<<CLOSE_STREAM>>\";\n\n// Internal types for stream pipeline\ntype StreamChunk =\n  | { type: \"ping\" }\n  | { type: \"data\"; redisId: string; data: string }\n  | { type: \"legacy-data\"; redisId: string; data: string };\n\n// Class implementing both interfaces\nexport class RedisRealtimeStreams implements StreamIngestor, StreamResponder {\n  private logger: Logger;\n  private inactivityTimeoutMs: number;\n\n  constructor(private options: RealtimeStreamsOptions) {\n    this.logger = options.logger ?? new Logger(\"RedisRealtimeStreams\", options.logLevel ?? \"info\");\n    this.inactivityTimeoutMs = options.inactivityTimeoutMs ?? 60000; // Default: 60 seconds\n  }\n\n  async initializeStream(\n    runId: string,\n    streamId: string\n  ): Promise<{ responseHeaders?: Record<string, string> }> {\n    return {};\n  }\n\n  async streamResponse(\n    request: Request,\n    runId: string,\n    streamId: string,\n    signal: AbortSignal,\n    options?: StreamResponseOptions\n  ): Promise<Response> {\n    const redis = new Redis(this.options.redis ?? {});\n    const streamKey = `stream:${runId}:${streamId}`;\n    let isCleanedUp = false;\n\n    const stream = new ReadableStream<StreamChunk>({\n      start: async (controller) => {\n        // Start from lastEventId if provided, otherwise from beginning\n        let lastId = options?.lastEventId ?? \"0\";\n        let retryCount = 0;\n        const maxRetries = 3;\n        let lastDataTime = Date.now();\n        let lastEnqueueTime = Date.now();\n        const blockTimeMs = 5000;\n        const pingIntervalMs = 10000; // 10 seconds\n\n        if (options?.lastEventId) {\n          this.logger.debug(\"[RealtimeStreams][streamResponse] Resuming from lastEventId\", {\n            streamKey,\n            lastEventId: options?.lastEventId,\n          });\n        }\n\n        try {\n          while (!signal.aborted) {\n            // Check if we need to send a ping\n            const timeSinceLastEnqueue = Date.now() - lastEnqueueTime;\n            if (timeSinceLastEnqueue >= pingIntervalMs) {\n              controller.enqueue({ type: \"ping\" });\n              lastEnqueueTime = Date.now();\n            }\n\n            // Compute inactivity threshold once to use consistently in both branches\n            const inactivityThresholdMs = options?.timeoutInSeconds\n              ? options.timeoutInSeconds * 1000\n              : this.inactivityTimeoutMs;\n\n            try {\n              const messages = await redis.xread(\n                \"COUNT\",\n                100,\n                \"BLOCK\",\n                blockTimeMs,\n                \"STREAMS\",\n                streamKey,\n                lastId\n              );\n\n              retryCount = 0;\n\n              if (messages && messages.length > 0) {\n                const [_key, entries] = messages[0];\n                let foundData = false;\n\n                for (let i = 0; i < entries.length; i++) {\n                  const [id, fields] = entries[i];\n                  lastId = id;\n\n                  if (fields && fields.length >= 2) {\n                    // Extract the data field from the Redis entry\n                    // Fields format: [\"field1\", \"value1\", \"field2\", \"value2\", ...]\n                    let data: string | null = null;\n\n                    for (let j = 0; j < fields.length; j += 2) {\n                      if (fields[j] === \"data\") {\n                        data = fields[j + 1];\n                        break;\n                      }\n                    }\n\n                    // Handle legacy entries that don't have field names (just data at index 1)\n                    if (data === null && fields.length >= 2) {\n                      data = fields[1];\n                    }\n\n                    if (data) {\n                      // Skip legacy END_SENTINEL entries (backward compatibility)\n                      if (data === END_SENTINEL) {\n                        continue;\n                      }\n\n                      // Enqueue structured chunk with Redis stream ID\n                      controller.enqueue({\n                        type: \"data\",\n                        redisId: id,\n                        data,\n                      });\n\n                      foundData = true;\n                      lastDataTime = Date.now();\n                      lastEnqueueTime = Date.now();\n\n                      if (signal.aborted) {\n                        controller.close();\n                        return;\n                      }\n                    }\n                  }\n                }\n\n                // If we didn't find any data in this batch, might have only seen sentinels\n                if (!foundData) {\n                  // Check for inactivity timeout\n                  const inactiveMs = Date.now() - lastDataTime;\n                  if (inactiveMs >= inactivityThresholdMs) {\n                    this.logger.debug(\n                      \"[RealtimeStreams][streamResponse] Closing stream due to inactivity\",\n                      {\n                        streamKey,\n                        inactiveMs,\n                        threshold: inactivityThresholdMs,\n                      }\n                    );\n                    controller.close();\n                    return;\n                  }\n                }\n              } else {\n                // No messages received (timed out on BLOCK)\n                // Check for inactivity timeout\n                const inactiveMs = Date.now() - lastDataTime;\n                if (inactiveMs >= inactivityThresholdMs) {\n                  this.logger.debug(\n                    \"[RealtimeStreams][streamResponse] Closing stream due to inactivity\",\n                    {\n                      streamKey,\n                      inactiveMs,\n                      threshold: inactivityThresholdMs,\n                    }\n                  );\n                  controller.close();\n                  return;\n                }\n              }\n            } catch (error) {\n              if (signal.aborted) break;\n\n              this.logger.error(\n                \"[RealtimeStreams][streamResponse] Error reading from Redis stream:\",\n                {\n                  error,\n                }\n              );\n              retryCount++;\n              if (retryCount >= maxRetries) throw error;\n              await new Promise((resolve) => setTimeout(resolve, 1000 * retryCount));\n            }\n          }\n        } catch (error) {\n          this.logger.error(\"[RealtimeStreams][streamResponse] Fatal error in stream processing:\", {\n            error,\n          });\n          controller.error(error);\n        } finally {\n          await cleanup();\n        }\n      },\n      cancel: async () => {\n        await cleanup();\n      },\n    })\n      .pipeThrough(\n        // Transform 1: Split data content by newlines, preserving metadata\n        new TransformStream<StreamChunk, StreamChunk & { line?: string }>({\n          transform(chunk, controller) {\n            if (chunk.type === \"ping\") {\n              controller.enqueue(chunk);\n            } else if (chunk.type === \"data\" || chunk.type === \"legacy-data\") {\n              // Split data by newlines, emit separate chunks with same metadata\n              const lines = chunk.data.split(\"\\n\").filter((line) => line.trim().length > 0);\n              for (const line of lines) {\n                controller.enqueue({ ...chunk, line });\n              }\n            }\n          },\n        })\n      )\n      .pipeThrough(\n        // Transform 2: Format as SSE\n        new TransformStream<StreamChunk & { line?: string }, string>({\n          transform(chunk, controller) {\n            if (chunk.type === \"ping\") {\n              controller.enqueue(`: ping\\n\\n`);\n            } else if ((chunk.type === \"data\" || chunk.type === \"legacy-data\") && chunk.line) {\n              // Use Redis stream ID as SSE event ID\n              controller.enqueue(`id: ${chunk.redisId}\\ndata: ${chunk.line}\\n\\n`);\n            }\n          },\n        })\n      )\n      .pipeThrough(new TextEncoderStream());\n\n    async function cleanup() {\n      if (isCleanedUp) return;\n      isCleanedUp = true;\n      await redis.quit().catch(console.error);\n    }\n\n    signal.addEventListener(\"abort\", cleanup);\n\n    return new Response(stream, {\n      headers: {\n        \"Content-Type\": \"text/event-stream\",\n        \"Cache-Control\": \"no-cache\",\n        Connection: \"keep-alive\",\n      },\n    });\n  }\n\n  async ingestData(\n    stream: ReadableStream<Uint8Array>,\n    runId: string,\n    streamId: string,\n    clientId: string,\n    resumeFromChunk?: number\n  ): Promise<Response> {\n    const redis = new Redis(this.options.redis ?? {});\n    const streamKey = `stream:${runId}:${streamId}`;\n    const startChunk = resumeFromChunk ?? 0;\n    // Start counting from the resume point, not from 0\n    let currentChunkIndex = startChunk;\n\n    const self = this;\n\n    async function cleanup() {\n      try {\n        await redis.quit();\n      } catch (error) {\n        self.logger.error(\"[RedisRealtimeStreams][ingestData] Error in cleanup:\", { error });\n      }\n    }\n\n    try {\n      const textStream = stream.pipeThrough(new TextDecoderStream());\n      const reader = textStream.getReader();\n\n      while (true) {\n        const { done, value } = await reader.read();\n\n        if (done || !value) {\n          break;\n        }\n\n        // Write each chunk with its index and clientId\n        this.logger.debug(\"[RedisRealtimeStreams][ingestData] Writing chunk\", {\n          streamKey,\n          runId,\n          clientId,\n          chunkIndex: currentChunkIndex,\n          resumeFromChunk: startChunk,\n          value,\n        });\n\n        await redis.xadd(\n          streamKey,\n          \"MAXLEN\",\n          \"~\",\n          String(env.REALTIME_STREAM_MAX_LENGTH),\n          \"*\",\n          \"clientId\",\n          clientId,\n          \"chunkIndex\",\n          currentChunkIndex.toString(),\n          \"data\",\n          value\n        );\n\n        currentChunkIndex++;\n      }\n\n      // Set TTL for cleanup when stream is done\n      await redis.expire(streamKey, env.REALTIME_STREAM_TTL);\n\n      return new Response(null, { status: 200 });\n    } catch (error) {\n      if (error instanceof Error) {\n        if (\"code\" in error && error.code === \"ECONNRESET\") {\n          this.logger.info(\"[RealtimeStreams][ingestData] Connection reset during ingestData:\", {\n            error,\n          });\n          return new Response(null, { status: 500 });\n        }\n      }\n\n      this.logger.error(\"[RealtimeStreams][ingestData] Error in ingestData:\", { error });\n\n      return new Response(null, { status: 500 });\n    } finally {\n      await cleanup();\n    }\n  }\n\n  async appendPart(part: string, partId: string, runId: string, streamId: string): Promise<void> {\n    const redis = new Redis(this.options.redis ?? {});\n    const streamKey = `stream:${runId}:${streamId}`;\n\n    await redis.xadd(\n      streamKey,\n      \"MAXLEN\",\n      \"~\",\n      String(env.REALTIME_STREAM_MAX_LENGTH),\n      \"*\",\n      \"clientId\",\n      \"\",\n      \"chunkIndex\",\n      \"0\",\n      \"data\",\n      JSON.stringify(part) + \"\\n\"\n    );\n\n    // Set TTL for cleanup when stream is done\n    await redis.expire(streamKey, env.REALTIME_STREAM_TTL);\n\n    await redis.quit();\n  }\n\n  async getLastChunkIndex(runId: string, streamId: string, clientId: string): Promise<number> {\n    const redis = new Redis(this.options.redis ?? {});\n    const streamKey = `stream:${runId}:${streamId}`;\n\n    try {\n      // Paginate through the stream from newest to oldest until we find this client's last chunk\n      const batchSize = 100;\n      let lastId = \"+\"; // Start from newest\n\n      while (true) {\n        const entries = await redis.xrevrange(streamKey, lastId, \"-\", \"COUNT\", batchSize);\n\n        if (!entries || entries.length === 0) {\n          // Reached the beginning of the stream, no chunks from this client\n          this.logger.debug(\n            \"[RedisRealtimeStreams][getLastChunkIndex] No chunks found for client\",\n            {\n              streamKey,\n              clientId,\n            }\n          );\n          return -1;\n        }\n\n        // Search through this batch for the client's last chunk\n        for (const [id, fields] of entries) {\n          let entryClientId: string | null = null;\n          let chunkIndex: number | null = null;\n          let data: string | null = null;\n\n          for (let i = 0; i < fields.length; i += 2) {\n            if (fields[i] === \"clientId\") {\n              entryClientId = fields[i + 1];\n            }\n            if (fields[i] === \"chunkIndex\") {\n              chunkIndex = parseInt(fields[i + 1], 10);\n            }\n            if (fields[i] === \"data\") {\n              data = fields[i + 1];\n            }\n          }\n\n          // Skip legacy END_SENTINEL entries (backward compatibility)\n          if (data === END_SENTINEL) {\n            continue;\n          }\n\n          // Check if this entry is from our client and has a chunkIndex\n          if (entryClientId === clientId && chunkIndex !== null) {\n            this.logger.debug(\"[RedisRealtimeStreams][getLastChunkIndex] Found last chunk\", {\n              streamKey,\n              clientId,\n              chunkIndex,\n            });\n            return chunkIndex;\n          }\n        }\n\n        // Move to next batch (older entries)\n        // Use the ID of the last entry in this batch as the new cursor\n        lastId = `(${entries[entries.length - 1][0]}`; // Exclusive range with (\n      }\n    } catch (error) {\n      this.logger.error(\"[RedisRealtimeStreams][getLastChunkIndex] Error getting last chunk:\", {\n        error,\n        streamKey,\n        clientId,\n      });\n      // Return -1 to indicate we don't know what the server has\n      return -1;\n    } finally {\n      await redis.quit().catch((err) => {\n        this.logger.error(\"[RedisRealtimeStreams][getLastChunkIndex] Error in cleanup:\", { err });\n      });\n    }\n  }\n}",
            "cost": 0.12467
        },
        "triggerdotdev__trigger.dev.d1c3bfb9/apps/webapp/test/redisRealtimeStreams.test.ts": {
            "output": "import { redisTest } from \"@internal/testcontainers\";\nimport Redis from \"ioredis\";\nimport { describe, expect } from \"vitest\";\nimport { RedisRealtimeStreams } from \"~/services/realtime/redisRealtimeStreams.server.js\";\n\ndescribe(\"RedisRealtimeStreams\", () => {\n  redisTest(\n    \"Should ingest chunks with correct indices and retrieve last chunk index\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_test123\";\n      const streamId = \"test-stream\";\n\n      // Create a mock stream with 5 chunks\n      const chunks = [\n        JSON.stringify({ chunk: 0, data: \"chunk 0\" }),\n        JSON.stringify({ chunk: 1, data: \"chunk 1\" }),\n        JSON.stringify({ chunk: 2, data: \"chunk 2\" }),\n        JSON.stringify({ chunk: 3, data: \"chunk 3\" }),\n        JSON.stringify({ chunk: 4, data: \"chunk 4\" }),\n      ];\n\n      // Create a ReadableStream from the chunks\n      const encoder = new TextEncoder();\n      const stream = new ReadableStream({\n        start(controller) {\n          for (const chunk of chunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      // Ingest the data with default client ID\n      const response = await redisRealtimeStreams.ingestData(stream, runId, streamId, \"default\");\n\n      // Verify response\n      expect(response.status).toBe(200);\n\n      // Verify chunks were stored with correct indices\n      const streamKey = `stream:${runId}:${streamId}`;\n      const entries = await redis.xrange(streamKey, \"-\", \"+\");\n\n      // Should have 5 chunks (no END_SENTINEL anymore)\n      expect(entries.length).toBe(5);\n\n      // Verify each chunk has the correct index\n      for (let i = 0; i < 5; i++) {\n        const [_id, fields] = entries[i];\n\n        // Find chunkIndex and data fields\n        let chunkIndex: number | null = null;\n        let data: string | null = null;\n\n        for (let j = 0; j < fields.length; j += 2) {\n          if (fields[j] === \"chunkIndex\") {\n            chunkIndex = parseInt(fields[j + 1], 10);\n          }\n          if (fields[j] === \"data\") {\n            data = fields[j + 1];\n          }\n        }\n\n        expect(chunkIndex).toBe(i);\n        expect(data).toBe(chunks[i] + \"\\n\");\n      }\n\n      // Test getLastChunkIndex for the default client\n      const lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(\n        runId,\n        streamId,\n        \"default\"\n      );\n      expect(lastChunkIndex).toBe(4); // Last chunk should be index 4\n\n      // Cleanup\n      await redis.del(streamKey);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\n    \"Should resume from specified chunk index and skip duplicates\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_test456\";\n      const streamId = \"test-stream-resume\";\n\n      // First, ingest chunks 0-2\n      const initialChunks = [\n        JSON.stringify({ chunk: 0, data: \"chunk 0\" }),\n        JSON.stringify({ chunk: 1, data: \"chunk 1\" }),\n        JSON.stringify({ chunk: 2, data: \"chunk 2\" }),\n      ];\n\n      const encoder = new TextEncoder();\n      const initialStream = new ReadableStream({\n        start(controller) {\n          for (const chunk of initialChunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      await redisRealtimeStreams.ingestData(initialStream, runId, streamId, \"default\");\n\n      // Verify we have 3 chunks\n      let lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(runId, streamId, \"default\");\n      expect(lastChunkIndex).toBe(2);\n\n      // Now \"resume\" from chunk 3 with new chunks (simulating a retry)\n      // When client queries server, server says \"I have up to chunk 2\"\n      // So client resumes from chunk 3 onwards\n      const resumeChunks = [\n        JSON.stringify({ chunk: 3, data: \"chunk 3\" }), // New\n        JSON.stringify({ chunk: 4, data: \"chunk 4\" }), // New\n      ];\n\n      const resumeStream = new ReadableStream({\n        start(controller) {\n          for (const chunk of resumeChunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      // Resume from chunk 3 (server tells us it already has 0-2)\n      await redisRealtimeStreams.ingestData(resumeStream, runId, streamId, \"default\", 3);\n\n      // Verify we now have 5 chunks total (0, 1, 2, 3, 4)\n      const streamKey = `stream:${runId}:${streamId}`;\n      const entries = await redis.xrange(streamKey, \"-\", \"+\");\n\n      expect(entries.length).toBe(5);\n\n      // Verify last chunk index is 4\n      lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(runId, streamId, \"default\");\n      expect(lastChunkIndex).toBe(4);\n\n      // Verify chunk indices are sequential\n      for (let i = 0; i < 5; i++) {\n        const [_id, fields] = entries[i];\n\n        let chunkIndex: number | null = null;\n        for (let j = 0; j < fields.length; j += 2) {\n          if (fields[j] === \"chunkIndex\") {\n            chunkIndex = parseInt(fields[j + 1], 10);\n          }\n        }\n\n        expect(chunkIndex).toBe(i);\n      }\n\n      // Cleanup\n      await redis.del(streamKey);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\n    \"Should return -1 for getLastChunkIndex when stream does not exist\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(\n        \"run_nonexistent\",\n        \"nonexistent-stream\",\n        \"default\"\n      );\n\n      expect(lastChunkIndex).toBe(-1);\n    }\n  );\n\n  redisTest(\n    \"Should correctly stream response data back to consumers\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_stream_test\";\n      const streamId = \"test-stream-response\";\n\n      // Ingest some data first\n      const chunks = [\n        JSON.stringify({ message: \"chunk 0\" }),\n        JSON.stringify({ message: \"chunk 1\" }),\n        JSON.stringify({ message: \"chunk 2\" }),\n      ];\n\n      const encoder = new TextEncoder();\n      const ingestStream = new ReadableStream({\n        start(controller) {\n          for (const chunk of chunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      await redisRealtimeStreams.ingestData(ingestStream, runId, streamId, \"default\");\n\n      // Now stream the response\n      const mockRequest = new Request(\"http://localhost/test\");\n      const abortController = new AbortController();\n\n      const response = await redisRealtimeStreams.streamResponse(\n        mockRequest,\n        runId,\n        streamId,\n        abortController.signal\n      );\n\n      expect(response.status).toBe(200);\n      expect(response.headers.get(\"Content-Type\")).toBe(\"text/event-stream\");\n\n      // Read the stream\n      const reader = response.body!.getReader();\n      const decoder = new TextDecoder();\n      const receivedData: string[] = [];\n\n      let done = false;\n      while (!done && receivedData.length < 3) {\n        const { value, done: streamDone } = await reader.read();\n        done = streamDone;\n\n        if (value) {\n          const text = decoder.decode(value);\n          // Parse SSE format: \"id: ...\\ndata: {json}\\n\\n\"\n          const events = text.split(\"\\n\\n\").filter((event) => event.trim());\n          for (const event of events) {\n            const lines = event.split(\"\\n\");\n            for (const line of lines) {\n              if (line.startsWith(\"data: \")) {\n                const data = line.substring(6).trim();\n                if (data) {\n                  receivedData.push(data);\n                }\n              }\n            }\n          }\n        }\n      }\n\n      // Cancel the stream\n      abortController.abort();\n      reader.releaseLock();\n\n      // Verify we received all chunks\n      // Note: LineTransformStream strips newlines, so we don't expect them in output\n      expect(receivedData.length).toBe(3);\n      for (let i = 0; i < 3; i++) {\n        expect(receivedData[i]).toBe(chunks[i]);\n      }\n\n      // Cleanup\n      await redis.del(`stream:${runId}:${streamId}`);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\n    \"Should handle empty stream ingestion\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_empty_test\";\n      const streamId = \"empty-stream\";\n\n      // Create an empty stream\n      const emptyStream = new ReadableStream({\n        start(controller) {\n          controller.close();\n        },\n      });\n\n      const response = await redisRealtimeStreams.ingestData(\n        emptyStream,\n        runId,\n        streamId,\n        \"default\"\n      );\n\n      expect(response.status).toBe(200);\n\n      // Should have no entries (empty stream)\n      const streamKey = `stream:${runId}:${streamId}`;\n      const entries = await redis.xrange(streamKey, \"-\", \"+\");\n      expect(entries.length).toBe(0);\n\n      // getLastChunkIndex should return -1 for empty stream\n      const lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(\n        runId,\n        streamId,\n        \"default\"\n      );\n      expect(lastChunkIndex).toBe(-1);\n\n      // Cleanup\n      await redis.del(streamKey);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\"Should handle resume from chunk 0\", { timeout: 30_000 }, async ({ redisOptions }) => {\n    const redis = new Redis(redisOptions);\n    const redisRealtimeStreams = new RedisRealtimeStreams({\n      redis: redisOptions,\n    });\n\n    const runId = \"run_resume_zero\";\n    const streamId = \"test-stream-zero\";\n\n    const chunks = [\n      JSON.stringify({ chunk: 0, data: \"chunk 0\" }),\n      JSON.stringify({ chunk: 1, data: \"chunk 1\" }),\n    ];\n\n    const encoder = new TextEncoder();\n    const stream = new ReadableStream({\n      start(controller) {\n        for (const chunk of chunks) {\n          controller.enqueue(encoder.encode(chunk + \"\\n\"));\n        }\n        controller.close();\n      },\n    });\n\n    // Explicitly resume from chunk 0 (should write all chunks)\n    await redisRealtimeStreams.ingestData(stream, runId, streamId, \"default\", 0);\n\n    const streamKey = `stream:${runId}:${streamId}`;\n    const entries = await redis.xrange(streamKey, \"-\", \"+\");\n\n    expect(entries.length).toBe(2);\n\n    // Verify indices start at 0\n    for (let i = 0; i < 2; i++) {\n      const [_id, fields] = entries[i];\n      let chunkIndex: number | null = null;\n      for (let j = 0; j < fields.length; j += 2) {\n        if (fields[j] === \"chunkIndex\") {\n          chunkIndex = parseInt(fields[j + 1], 10);\n        }\n      }\n      expect(chunkIndex).toBe(i);\n    }\n\n    // Cleanup\n    await redis.del(streamKey);\n    await redis.quit();\n  });\n\n  redisTest(\n    \"Should handle large number of chunks\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_large_test\";\n      const streamId = \"large-stream\";\n      const chunkCount = 100;\n\n      // Create 100 chunks\n      const chunks: string[] = [];\n      for (let i = 0; i < chunkCount; i++) {\n        chunks.push(JSON.stringify({ chunk: i, data: `chunk ${i}` }));\n      }\n\n      const encoder = new TextEncoder();\n      const stream = new ReadableStream({\n        start(controller) {\n          for (const chunk of chunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      await redisRealtimeStreams.ingestData(stream, runId, streamId, \"default\");\n\n      // Verify last chunk index\n      const lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(\n        runId,\n        streamId,\n        \"default\"\n      );\n      expect(lastChunkIndex).toBe(chunkCount - 1);\n\n      // Verify all chunks stored\n      const streamKey = `stream:${runId}:${streamId}`;\n      const entries = await redis.xrange(streamKey, \"-\", \"+\");\n\n      expect(entries.length).toBe(chunkCount);\n\n      // Cleanup\n      await redis.del(streamKey);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\n    \"Should handle streamResponse with legacy data format (backward compatibility)\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_legacy_test\";\n      const streamId = \"legacy-stream\";\n      const streamKey = `stream:${runId}:${streamId}`;\n\n      // Manually add entries in legacy format (without chunkIndex or clientId fields)\n      await redis.xadd(streamKey, \"*\", \"data\", \"legacy chunk 1\\n\");\n      await redis.xadd(streamKey, \"*\", \"data\", \"legacy chunk 2\\n\");\n\n      // Stream the response\n      const mockRequest = new Request(\"http://localhost/test\");\n      const abortController = new AbortController();\n\n      const response = await redisRealtimeStreams",
            "output_extracted": "import { redisTest } from \"@internal/testcontainers\";\nimport Redis from \"ioredis\";\nimport { describe, expect } from \"vitest\";\nimport { RedisRealtimeStreams } from \"~/services/realtime/redisRealtimeStreams.server.js\";\n\ndescribe(\"RedisRealtimeStreams\", () => {\n  redisTest(\n    \"Should ingest chunks with correct indices and retrieve last chunk index\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_test123\";\n      const streamId = \"test-stream\";\n\n      // Create a mock stream with 5 chunks\n      const chunks = [\n        JSON.stringify({ chunk: 0, data: \"chunk 0\" }),\n        JSON.stringify({ chunk: 1, data: \"chunk 1\" }),\n        JSON.stringify({ chunk: 2, data: \"chunk 2\" }),\n        JSON.stringify({ chunk: 3, data: \"chunk 3\" }),\n        JSON.stringify({ chunk: 4, data: \"chunk 4\" }),\n      ];\n\n      // Create a ReadableStream from the chunks\n      const encoder = new TextEncoder();\n      const stream = new ReadableStream({\n        start(controller) {\n          for (const chunk of chunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      // Ingest the data with default client ID\n      const response = await redisRealtimeStreams.ingestData(stream, runId, streamId, \"default\");\n\n      // Verify response\n      expect(response.status).toBe(200);\n\n      // Verify chunks were stored with correct indices\n      const streamKey = `stream:${runId}:${streamId}`;\n      const entries = await redis.xrange(streamKey, \"-\", \"+\");\n\n      // Should have 5 chunks (no END_SENTINEL anymore)\n      expect(entries.length).toBe(5);\n\n      // Verify each chunk has the correct index\n      for (let i = 0; i < 5; i++) {\n        const [_id, fields] = entries[i];\n\n        // Find chunkIndex and data fields\n        let chunkIndex: number | null = null;\n        let data: string | null = null;\n\n        for (let j = 0; j < fields.length; j += 2) {\n          if (fields[j] === \"chunkIndex\") {\n            chunkIndex = parseInt(fields[j + 1], 10);\n          }\n          if (fields[j] === \"data\") {\n            data = fields[j + 1];\n          }\n        }\n\n        expect(chunkIndex).toBe(i);\n        expect(data).toBe(chunks[i] + \"\\n\");\n      }\n\n      // Test getLastChunkIndex for the default client\n      const lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(\n        runId,\n        streamId,\n        \"default\"\n      );\n      expect(lastChunkIndex).toBe(4); // Last chunk should be index 4\n\n      // Cleanup\n      await redis.del(streamKey);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\n    \"Should resume from specified chunk index and skip duplicates\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_test456\";\n      const streamId = \"test-stream-resume\";\n\n      // First, ingest chunks 0-2\n      const initialChunks = [\n        JSON.stringify({ chunk: 0, data: \"chunk 0\" }),\n        JSON.stringify({ chunk: 1, data: \"chunk 1\" }),\n        JSON.stringify({ chunk: 2, data: \"chunk 2\" }),\n      ];\n\n      const encoder = new TextEncoder();\n      const initialStream = new ReadableStream({\n        start(controller) {\n          for (const chunk of initialChunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      await redisRealtimeStreams.ingestData(initialStream, runId, streamId, \"default\");\n\n      // Verify we have 3 chunks\n      let lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(runId, streamId, \"default\");\n      expect(lastChunkIndex).toBe(2);\n\n      // Now \"resume\" from chunk 3 with new chunks (simulating a retry)\n      // When client queries server, server says \"I have up to chunk 2\"\n      // So client resumes from chunk 3 onwards\n      const resumeChunks = [\n        JSON.stringify({ chunk: 3, data: \"chunk 3\" }), // New\n        JSON.stringify({ chunk: 4, data: \"chunk 4\" }), // New\n      ];\n\n      const resumeStream = new ReadableStream({\n        start(controller) {\n          for (const chunk of resumeChunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      // Resume from chunk 3 (server tells us it already has 0-2)\n      await redisRealtimeStreams.ingestData(resumeStream, runId, streamId, \"default\", 3);\n\n      // Verify we now have 5 chunks total (0, 1, 2, 3, 4)\n      const streamKey = `stream:${runId}:${streamId}`;\n      const entries = await redis.xrange(streamKey, \"-\", \"+\");\n\n      expect(entries.length).toBe(5);\n\n      // Verify last chunk index is 4\n      lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(runId, streamId, \"default\");\n      expect(lastChunkIndex).toBe(4);\n\n      // Verify chunk indices are sequential\n      for (let i = 0; i < 5; i++) {\n        const [_id, fields] = entries[i];\n\n        let chunkIndex: number | null = null;\n        for (let j = 0; j < fields.length; j += 2) {\n          if (fields[j] === \"chunkIndex\") {\n            chunkIndex = parseInt(fields[j + 1], 10);\n          }\n        }\n\n        expect(chunkIndex).toBe(i);\n      }\n\n      // Cleanup\n      await redis.del(streamKey);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\n    \"Should return -1 for getLastChunkIndex when stream does not exist\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(\n        \"run_nonexistent\",\n        \"nonexistent-stream\",\n        \"default\"\n      );\n\n      expect(lastChunkIndex).toBe(-1);\n    }\n  );\n\n  redisTest(\n    \"Should correctly stream response data back to consumers\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_stream_test\";\n      const streamId = \"test-stream-response\";\n\n      // Ingest some data first\n      const chunks = [\n        JSON.stringify({ message: \"chunk 0\" }),\n        JSON.stringify({ message: \"chunk 1\" }),\n        JSON.stringify({ message: \"chunk 2\" }),\n      ];\n\n      const encoder = new TextEncoder();\n      const ingestStream = new ReadableStream({\n        start(controller) {\n          for (const chunk of chunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      await redisRealtimeStreams.ingestData(ingestStream, runId, streamId, \"default\");\n\n      // Now stream the response\n      const mockRequest = new Request(\"http://localhost/test\");\n      const abortController = new AbortController();\n\n      const response = await redisRealtimeStreams.streamResponse(\n        mockRequest,\n        runId,\n        streamId,\n        abortController.signal\n      );\n\n      expect(response.status).toBe(200);\n      expect(response.headers.get(\"Content-Type\")).toBe(\"text/event-stream\");\n\n      // Read the stream\n      const reader = response.body!.getReader();\n      const decoder = new TextDecoder();\n      const receivedData: string[] = [];\n\n      let done = false;\n      while (!done && receivedData.length < 3) {\n        const { value, done: streamDone } = await reader.read();\n        done = streamDone;\n\n        if (value) {\n          const text = decoder.decode(value);\n          // Parse SSE format: \"id: ...\\ndata: {json}\\n\\n\"\n          const events = text.split(\"\\n\\n\").filter((event) => event.trim());\n          for (const event of events) {\n            const lines = event.split(\"\\n\");\n            for (const line of lines) {\n              if (line.startsWith(\"data: \")) {\n                const data = line.substring(6).trim();\n                if (data) {\n                  receivedData.push(data);\n                }\n              }\n            }\n          }\n        }\n      }\n\n      // Cancel the stream\n      abortController.abort();\n      reader.releaseLock();\n\n      // Verify we received all chunks\n      // Note: LineTransformStream strips newlines, so we don't expect them in output\n      expect(receivedData.length).toBe(3);\n      for (let i = 0; i < 3; i++) {\n        expect(receivedData[i]).toBe(chunks[i]);\n      }\n\n      // Cleanup\n      await redis.del(`stream:${runId}:${streamId}`);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\n    \"Should handle empty stream ingestion\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_empty_test\";\n      const streamId = \"empty-stream\";\n\n      // Create an empty stream\n      const emptyStream = new ReadableStream({\n        start(controller) {\n          controller.close();\n        },\n      });\n\n      const response = await redisRealtimeStreams.ingestData(\n        emptyStream,\n        runId,\n        streamId,\n        \"default\"\n      );\n\n      expect(response.status).toBe(200);\n\n      // Should have no entries (empty stream)\n      const streamKey = `stream:${runId}:${streamId}`;\n      const entries = await redis.xrange(streamKey, \"-\", \"+\");\n      expect(entries.length).toBe(0);\n\n      // getLastChunkIndex should return -1 for empty stream\n      const lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(\n        runId,\n        streamId,\n        \"default\"\n      );\n      expect(lastChunkIndex).toBe(-1);\n\n      // Cleanup\n      await redis.del(streamKey);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\"Should handle resume from chunk 0\", { timeout: 30_000 }, async ({ redisOptions }) => {\n    const redis = new Redis(redisOptions);\n    const redisRealtimeStreams = new RedisRealtimeStreams({\n      redis: redisOptions,\n    });\n\n    const runId = \"run_resume_zero\";\n    const streamId = \"test-stream-zero\";\n\n    const chunks = [\n      JSON.stringify({ chunk: 0, data: \"chunk 0\" }),\n      JSON.stringify({ chunk: 1, data: \"chunk 1\" }),\n    ];\n\n    const encoder = new TextEncoder();\n    const stream = new ReadableStream({\n      start(controller) {\n        for (const chunk of chunks) {\n          controller.enqueue(encoder.encode(chunk + \"\\n\"));\n        }\n        controller.close();\n      },\n    });\n\n    // Explicitly resume from chunk 0 (should write all chunks)\n    await redisRealtimeStreams.ingestData(stream, runId, streamId, \"default\", 0);\n\n    const streamKey = `stream:${runId}:${streamId}`;\n    const entries = await redis.xrange(streamKey, \"-\", \"+\");\n\n    expect(entries.length).toBe(2);\n\n    // Verify indices start at 0\n    for (let i = 0; i < 2; i++) {\n      const [_id, fields] = entries[i];\n      let chunkIndex: number | null = null;\n      for (let j = 0; j < fields.length; j += 2) {\n        if (fields[j] === \"chunkIndex\") {\n          chunkIndex = parseInt(fields[j + 1], 10);\n        }\n      }\n      expect(chunkIndex).toBe(i);\n    }\n\n    // Cleanup\n    await redis.del(streamKey);\n    await redis.quit();\n  });\n\n  redisTest(\n    \"Should handle large number of chunks\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_large_test\";\n      const streamId = \"large-stream\";\n      const chunkCount = 100;\n\n      // Create 100 chunks\n      const chunks: string[] = [];\n      for (let i = 0; i < chunkCount; i++) {\n        chunks.push(JSON.stringify({ chunk: i, data: `chunk ${i}` }));\n      }\n\n      const encoder = new TextEncoder();\n      const stream = new ReadableStream({\n        start(controller) {\n          for (const chunk of chunks) {\n            controller.enqueue(encoder.encode(chunk + \"\\n\"));\n          }\n          controller.close();\n        },\n      });\n\n      await redisRealtimeStreams.ingestData(stream, runId, streamId, \"default\");\n\n      // Verify last chunk index\n      const lastChunkIndex = await redisRealtimeStreams.getLastChunkIndex(\n        runId,\n        streamId,\n        \"default\"\n      );\n      expect(lastChunkIndex).toBe(chunkCount - 1);\n\n      // Verify all chunks stored\n      const streamKey = `stream:${runId}:${streamId}`;\n      const entries = await redis.xrange(streamKey, \"-\", \"+\");\n\n      expect(entries.length).toBe(chunkCount);\n\n      // Cleanup\n      await redis.del(streamKey);\n      await redis.quit();\n    }\n  );\n\n  redisTest(\n    \"Should handle streamResponse with legacy data format (backward compatibility)\",\n    { timeout: 30_000 },\n    async ({ redisOptions }) => {\n      const redis = new Redis(redisOptions);\n      const redisRealtimeStreams = new RedisRealtimeStreams({\n        redis: redisOptions,\n      });\n\n      const runId = \"run_legacy_test\";\n      const streamId = \"legacy-stream\";\n      const streamKey = `stream:${runId}:${streamId}`;\n\n      // Manually add entries in legacy format (without chunkIndex or clientId fields)\n      await redis.xadd(streamKey, \"*\", \"data\", \"legacy chunk 1\\n\");\n      await redis.xadd(streamKey, \"*\", \"data\", \"legacy chunk 2\\n\");\n\n      // Stream the response\n      const mockRequest = new Request(\"http://localhost/test\");\n      const abortController = new AbortController();\n\n      const response = await redisRealtimeStreams",
            "cost": 0.18250500000000003
        },
        "triggerdotdev__trigger.dev.d1c3bfb9/references/realtime-streams/src/trigger/streams.ts": {
            "output": "import { demoStream } from \"@/app/streams\";\nimport { logger, metadata, streams, task } from \"@trigger.dev/sdk\";\nimport assert from \"assert\";\nimport { setTimeout } from \"timers/promises\";\n\nexport type STREAMS = {\n  demo: string;\n};\n\nexport type PerformanceChunk = {\n  timestamp: number; // When the chunk was sent from the task\n  chunkIndex: number;\n  data: string;\n};\n\nexport type StreamScenario =\n  | \"stall\"\n  | \"continuous\"\n  | \"burst\"\n  | \"slow-steady\"\n  | \"markdown\"\n  | \"performance\";\n\nexport type StreamPayload = {\n  scenario?: StreamScenario;\n  // Stall scenario options\n  stallDurationMs?: number;\n  includePing?: boolean;\n  // Continuous scenario options\n  durationSec?: number;\n  intervalMs?: number;\n  // Burst scenario options\n  burstCount?: number;\n  tokensPerBurst?: number;\n  burstIntervalMs?: number;\n  pauseBetweenBurstsMs?: number;\n  // Slow steady scenario options\n  durationMin?: number;\n  tokenIntervalSec?: number;\n  // Markdown scenario options\n  tokenDelayMs?: number;\n  // Performance scenario options\n  chunkCount?: number;\n  chunkIntervalMs?: number;\n};\n\nexport const streamsTask = task({\n  id: \"streams\",\n  run: async (payload: StreamPayload = {}, { ctx }) => {\n    const scenario = payload.scenario ?? \"continuous\";\n    logger.info(\"Starting stream scenario\", { scenario });\n\n    let generator: AsyncGenerator<string>;\n    let scenarioDescription: string;\n\n    switch (scenario) {\n      case \"stall\": {\n        const stallDurationMs = payload.stallDurationMs ?? 3 * 60 * 1000; // Default 3 minutes\n        const includePing = payload.includePing ?? false;\n        generator = generateLLMTokenStream(includePing, stallDurationMs);\n        scenarioDescription = `Stall scenario: ${stallDurationMs / 1000}s with ${\n          includePing ? \"ping tokens\" : \"no pings\"\n        }`;\n        break;\n      }\n      case \"continuous\": {\n        const durationSec = payload.durationSec ?? 45;\n        const intervalMs = payload.intervalMs ?? 10;\n        generator = generateContinuousTokenStream(durationSec, intervalMs);\n        scenarioDescription = `Continuous scenario: ${durationSec}s with ${intervalMs}ms intervals`;\n        break;\n      }\n      case \"burst\": {\n        const burstCount = payload.burstCount ?? 10;\n        const tokensPerBurst = payload.tokensPerBurst ?? 20;\n        const burstIntervalMs = payload.burstIntervalMs ?? 5;\n        const pauseBetweenBurstsMs = payload.pauseBetweenBurstsMs ?? 2000;\n        generator = generateBurstTokenStream(\n          burstCount,\n          tokensPerBurst,\n          burstIntervalMs,\n          pauseBetweenBurstsMs\n        );\n        scenarioDescription = `Burst scenario: ${burstCount} bursts of ${tokensPerBurst} tokens`;\n        break;\n      }\n      case \"slow-steady\": {\n        const durationMin = payload.durationMin ?? 5;\n        const tokenIntervalSec = payload.tokenIntervalSec ?? 5;\n        generator = generateSlowSteadyTokenStream(durationMin, tokenIntervalSec);\n        scenarioDescription = `Slow steady scenario: ${durationMin}min with ${tokenIntervalSec}s intervals`;\n        break;\n      }\n      case \"markdown\": {\n        const tokenDelayMs = payload.tokenDelayMs ?? 15;\n        generator = generateMarkdownTokenStream(tokenDelayMs);\n        scenarioDescription = `Markdown scenario: generating formatted content with ${tokenDelayMs}ms delays`;\n        break;\n      }\n      case \"performance\": {\n        const chunkCount = payload.chunkCount ?? 500;\n        const chunkIntervalMs = payload.chunkIntervalMs ?? 10;\n        generator = generatePerformanceStream(chunkCount, chunkIntervalMs);\n        scenarioDescription = `Performance scenario: ${chunkCount} chunks with ${chunkIntervalMs}ms intervals`;\n        break;\n      }\n      default: {\n        throw new Error(`Unknown scenario: ${scenario}`);\n      }\n    }\n\n    logger.info(\"Starting stream\", { scenarioDescription });\n\n    const mockStream = createStreamFromGenerator(generator);\n\n    const { waitUntilComplete } = demoStream.pipe(mockStream);\n\n    await waitUntilComplete();\n\n    logger.info(\"Stream completed\", { scenario });\n\n    return {\n      scenario,\n      scenarioDescription,\n    };\n  },\n});\n\nexport const streamsChildTask = task({\n  id: \"streams-child\",\n  run: async (payload: any, { ctx }) => {\n    demoStream.writer({\n      execute: ({ write, merge }) => {\n        write(JSON.stringify({ step: \"one\" }));\n        write(JSON.stringify({ step: \"two\" }));\n        write(JSON.stringify({ step: \"three\" }));\n        merge(\n          new ReadableStream({\n            start(controller) {\n              controller.enqueue(JSON.stringify({ step: \"four\" }));\n              controller.enqueue(JSON.stringify({ step: \"five\" }));\n              controller.close();\n            },\n          })\n        );\n      },\n      target: ctx.run.rootTaskRunId,\n    });\n  },\n});\n\nexport const streamsTesterTask = task({\n  id: \"streams-tester\",\n  run: async (payload: any, { ctx }) => {\n    logger.info(\"Starting multiple source streams tester task\");\n\n    await multipleSourceStreamsTesterTask\n      .triggerAndWait(\n        {},\n        {},\n        {\n          clientConfig: {\n            future: {\n              v2RealtimeStreams: false,\n            },\n          },\n        }\n      )\n      .unwrap();\n\n    await multipleSourceStreamsTesterTask\n      .triggerAndWait(\n        {},\n        {},\n        {\n          clientConfig: {\n            future: {\n              v2RealtimeStreams: true,\n            },\n          },\n        }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 Multiple source streams tester tasks completed\");\n\n    logger.info(\"Starting stream append tester task\");\n\n    await streamAppendTesterTask\n      .triggerAndWait(\n        {},\n        {},\n        {\n          clientConfig: {\n            future: {\n              v2RealtimeStreams: false,\n            },\n          },\n        }\n      )\n      .unwrap();\n\n    await streamAppendTesterTask\n      .triggerAndWait(\n        {},\n        {},\n        {\n          clientConfig: {\n            future: {\n              v2RealtimeStreams: true,\n            },\n          },\n        }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 Stream append tester task completed\");\n\n    logger.info(\"Starting stream pipe tester task\");\n\n    await streamPipeTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream pipe tester task completed\");\n\n    await streamPipeTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: false } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream pipe tester task completed\");\n\n    logger.info(\"Starting stream writer tester task\");\n\n    await streamWriterTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream writer tester task completed\");\n\n    await streamWriterTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: false } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream writer tester task completed\");\n\n    logger.info(\"Starting stream wait until tester task\");\n\n    await streamWaitUntilTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream wait until tester task completed\");\n\n    await streamWaitUntilTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: false } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream wait until tester task completed\");\n\n    logger.info(\"Starting metadata tester task\");\n\n    await metadataTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Metadata tester task completed\");\n\n    await metadataTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: false } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Metadata tester task completed\");\n\n    logger.info(\"Starting stream read tester task\");\n\n    await streamReadTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream read tester task completed\");\n\n    logger.info(\"Starting streams stress tester task\");\n\n    await streamsStressTesterTask\n      .triggerAndWait(\n        { streamsVersion: \"v1\" },\n        {},\n        { clientConfig: { future: { v2RealtimeStreams: false } } }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 Streams stress tester task completed\");\n\n    await streamsStressTesterTask\n      .triggerAndWait(\n        { streamsVersion: \"v2\" },\n        {},\n        { clientConfig: { future: { v2RealtimeStreams: true } } }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 Streams stress tester task completed\");\n\n    logger.info(\"Starting end to end latency tester task\");\n\n    await endToEndLatencyTesterTask\n      .triggerAndWait(\n        { streamsVersion: \"v1\" },\n        {},\n        { clientConfig: { future: { v2RealtimeStreams: false } } }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 End to end latency tester task completed\");\n\n    await endToEndLatencyTesterTask\n      .triggerAndWait(\n        { streamsVersion: \"v2\" },\n        {},\n        { clientConfig: { future: { v2RealtimeStreams: true } } }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 End to end latency tester task completed\");\n\n    return {\n      message: \"Multiple source streams tester tasks completed\",\n    };\n  },\n});\n\nconst testStream = streams.define<string>({\n  id: \"test\",\n});\n\nconst multipleSourceStreamsTesterTask = task({\n  id: \"multiple-source-streams-tester\",\n  run: async (payload: any, { ctx }) => {\n    const stream1 = new ReadableStream<string>({\n      start(controller) {\n        controller.enqueue(\"stream 1 chunk 1\");\n        controller.enqueue(\"stream 1 chunk 2\");\n        controller.enqueue(\"stream 1 chunk 3\");\n        controller.close();\n      },\n    });\n\n    const stream2 = new ReadableStream<string>({\n      start(controller) {\n        controller.enqueue(\"stream 2 chunk 1\");\n        controller.enqueue(\"stream 2 chunk 2\");\n        controller.enqueue(\"stream 2 chunk 3\");\n        controller.close();\n      },\n    });\n\n    const { waitUntilComplete: waitUntilComplete1, stream: stream1Stream } = streams.pipe(stream1);\n    const { waitUntilComplete: waitUntilComplete2, stream: stream2Stream } = streams.pipe(stream2);\n\n    await Promise.all([waitUntilComplete1(), waitUntilComplete2()]);\n\n    const stream1Chunks = await convertReadableStreamToArray(stream1Stream);\n    const stream2Chunks = await convertReadableStreamToArray(stream2Stream);\n\n    assert.strictEqual(stream1Chunks.length, 3, \"Expected 3 chunks\");\n    assert.ok(stream1Chunks.includes(\"stream 1 chunk 1\"), \"Expected stream 1 chunk 1\");\n    assert.ok(stream1Chunks.includes(\"stream 1 chunk 2\"), \"Expected stream 1 chunk 2\");\n    assert.ok(stream1Chunks.includes(\"stream 1 chunk 3\"), \"Expected stream 1 chunk 3\");\n    assert.strictEqual(stream2Chunks.length, 3, \"Expected 3 chunks\");\n    assert.ok(stream2Chunks.includes(\"stream 2 chunk 1\"), \"Expected stream 2 chunk 1\");\n    assert.ok(stream2Chunks.includes(\"stream 2 chunk 2\"), \"Expected stream 2 chunk 2\");\n    assert.ok(stream2Chunks.includes(\"stream 2 chunk 3\"), \"Expected stream 2 chunk 3\");\n\n    const chunks = [];\n\n    for await (const chunk of await streams.read(ctx.run.id, { timeoutInSeconds: 5 })) {\n      chunks.push(chunk);\n    }\n\n    assert.strictEqual(chunks.length, 6, \"Expected 6 chunks\");\n    assert.ok(chunks.includes(\"stream 1 chunk 1\"), \"Expected stream 1 chunk 1\");\n    assert.ok(chunks.includes(\"stream 1 chunk 2\"), \"Expected stream 1 chunk 2\");\n    assert.ok(chunks.includes(\"stream 1 chunk 3\"), \"Expected stream 1 chunk 3\");\n    assert.ok(chunks.includes(\"stream 2 chunk 1\"), \"Expected stream 2 chunk 1\");\n    assert.ok(chunks.includes(\"stream 2 chunk 2\"), \"Expected stream 2 chunk 2\");\n    assert.ok(chunks.includes(\"stream 2 chunk 3\"), \"Expected stream 2 chunk 3\");\n\n    return {\n      message: \"Streams completed\",\n    };\n  },\n});\n\nconst streamAppendTesterTask = task({\n  id: \"stream-append-tester\",\n  run: async (payload: any, { ctx }) => {\n    await streams.append(\"chunk 1\");\n    await streams.append(\"chunk 2\");\n    await streams.append(\"chunk 3\");\n\n    const chunks = [];\n\n    for await (const chunk of await streams.read(ctx.run.id, { timeoutInSeconds: 5 })) {\n      chunks.push(chunk);\n    }\n\n    assert.strictEqual(chunks.length, 3, \"Expected 3 chunks\");\n    assert.ok(chunks.includes(\"chunk 1\"), \"Expected chunk 1\");\n    assert.ok(chunks.includes(\"chunk 2\"), \"Expected chunk 2\");\n    assert.ok(chunks.includes(\"chunk 3\"), \"Expected chunk 3\");\n\n    await streams.append(\"named\", \"chunk 1\");\n    await streams.append(\"named\", \"chunk 2\");\n    await streams.append(\"named\", \"chunk 3\");\n\n    const namedChunks = [];\n\n    for await (const chunk of await streams.read(ctx.run.id, \"named\", { timeoutInSeconds: 5 })) {\n      namedChunks.push(chunk);\n    }\n\n    assert.strictEqual(namedChunks.length, 3, \"Expected 3 chunks\");\n    assert.ok(namedChunks.includes(\"chunk 1\"), \"Expected chunk 1\");\n    assert.ok(namedChunks.includes(\"chunk 2\"), \"Expected chunk 2\");\n    assert.ok(namedChunks.includes(\"chunk 3\"), \"Expected chunk 3\");\n\n    await testStream.append(\"chunk 1\");\n    await testStream.append(\"chunk 2\");\n    await testStream.append(\"chunk 3\");\n\n    const testChunks = [];\n\n    for await (const chunk of await testStream.read(ctx.run.id, { timeoutInSeconds:",
            "output_extracted": "import { demoStream } from \"@/app/streams\";\nimport { logger, metadata, streams, task } from \"@trigger.dev/sdk\";\nimport assert from \"assert\";\nimport { setTimeout } from \"timers/promises\";\n\nexport type STREAMS = {\n  demo: string;\n};\n\nexport type PerformanceChunk = {\n  timestamp: number; // When the chunk was sent from the task\n  chunkIndex: number;\n  data: string;\n};\n\nexport type StreamScenario =\n  | \"stall\"\n  | \"continuous\"\n  | \"burst\"\n  | \"slow-steady\"\n  | \"markdown\"\n  | \"performance\";\n\nexport type StreamPayload = {\n  scenario?: StreamScenario;\n  // Stall scenario options\n  stallDurationMs?: number;\n  includePing?: boolean;\n  // Continuous scenario options\n  durationSec?: number;\n  intervalMs?: number;\n  // Burst scenario options\n  burstCount?: number;\n  tokensPerBurst?: number;\n  burstIntervalMs?: number;\n  pauseBetweenBurstsMs?: number;\n  // Slow steady scenario options\n  durationMin?: number;\n  tokenIntervalSec?: number;\n  // Markdown scenario options\n  tokenDelayMs?: number;\n  // Performance scenario options\n  chunkCount?: number;\n  chunkIntervalMs?: number;\n};\n\nexport const streamsTask = task({\n  id: \"streams\",\n  run: async (payload: StreamPayload = {}, { ctx }) => {\n    const scenario = payload.scenario ?? \"continuous\";\n    logger.info(\"Starting stream scenario\", { scenario });\n\n    let generator: AsyncGenerator<string>;\n    let scenarioDescription: string;\n\n    switch (scenario) {\n      case \"stall\": {\n        const stallDurationMs = payload.stallDurationMs ?? 3 * 60 * 1000; // Default 3 minutes\n        const includePing = payload.includePing ?? false;\n        generator = generateLLMTokenStream(includePing, stallDurationMs);\n        scenarioDescription = `Stall scenario: ${stallDurationMs / 1000}s with ${\n          includePing ? \"ping tokens\" : \"no pings\"\n        }`;\n        break;\n      }\n      case \"continuous\": {\n        const durationSec = payload.durationSec ?? 45;\n        const intervalMs = payload.intervalMs ?? 10;\n        generator = generateContinuousTokenStream(durationSec, intervalMs);\n        scenarioDescription = `Continuous scenario: ${durationSec}s with ${intervalMs}ms intervals`;\n        break;\n      }\n      case \"burst\": {\n        const burstCount = payload.burstCount ?? 10;\n        const tokensPerBurst = payload.tokensPerBurst ?? 20;\n        const burstIntervalMs = payload.burstIntervalMs ?? 5;\n        const pauseBetweenBurstsMs = payload.pauseBetweenBurstsMs ?? 2000;\n        generator = generateBurstTokenStream(\n          burstCount,\n          tokensPerBurst,\n          burstIntervalMs,\n          pauseBetweenBurstsMs\n        );\n        scenarioDescription = `Burst scenario: ${burstCount} bursts of ${tokensPerBurst} tokens`;\n        break;\n      }\n      case \"slow-steady\": {\n        const durationMin = payload.durationMin ?? 5;\n        const tokenIntervalSec = payload.tokenIntervalSec ?? 5;\n        generator = generateSlowSteadyTokenStream(durationMin, tokenIntervalSec);\n        scenarioDescription = `Slow steady scenario: ${durationMin}min with ${tokenIntervalSec}s intervals`;\n        break;\n      }\n      case \"markdown\": {\n        const tokenDelayMs = payload.tokenDelayMs ?? 15;\n        generator = generateMarkdownTokenStream(tokenDelayMs);\n        scenarioDescription = `Markdown scenario: generating formatted content with ${tokenDelayMs}ms delays`;\n        break;\n      }\n      case \"performance\": {\n        const chunkCount = payload.chunkCount ?? 500;\n        const chunkIntervalMs = payload.chunkIntervalMs ?? 10;\n        generator = generatePerformanceStream(chunkCount, chunkIntervalMs);\n        scenarioDescription = `Performance scenario: ${chunkCount} chunks with ${chunkIntervalMs}ms intervals`;\n        break;\n      }\n      default: {\n        throw new Error(`Unknown scenario: ${scenario}`);\n      }\n    }\n\n    logger.info(\"Starting stream\", { scenarioDescription });\n\n    const mockStream = createStreamFromGenerator(generator);\n\n    const { waitUntilComplete } = demoStream.pipe(mockStream);\n\n    await waitUntilComplete();\n\n    logger.info(\"Stream completed\", { scenario });\n\n    return {\n      scenario,\n      scenarioDescription,\n    };\n  },\n});\n\nexport const streamsChildTask = task({\n  id: \"streams-child\",\n  run: async (payload: any, { ctx }) => {\n    demoStream.writer({\n      execute: ({ write, merge }) => {\n        write(JSON.stringify({ step: \"one\" }));\n        write(JSON.stringify({ step: \"two\" }));\n        write(JSON.stringify({ step: \"three\" }));\n        merge(\n          new ReadableStream({\n            start(controller) {\n              controller.enqueue(JSON.stringify({ step: \"four\" }));\n              controller.enqueue(JSON.stringify({ step: \"five\" }));\n              controller.close();\n            },\n          })\n        );\n      },\n      target: ctx.run.rootTaskRunId,\n    });\n  },\n});\n\nexport const streamsTesterTask = task({\n  id: \"streams-tester\",\n  run: async (payload: any, { ctx }) => {\n    logger.info(\"Starting multiple source streams tester task\");\n\n    await multipleSourceStreamsTesterTask\n      .triggerAndWait(\n        {},\n        {},\n        {\n          clientConfig: {\n            future: {\n              v2RealtimeStreams: false,\n            },\n          },\n        }\n      )\n      .unwrap();\n\n    await multipleSourceStreamsTesterTask\n      .triggerAndWait(\n        {},\n        {},\n        {\n          clientConfig: {\n            future: {\n              v2RealtimeStreams: true,\n            },\n          },\n        }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 Multiple source streams tester tasks completed\");\n\n    logger.info(\"Starting stream append tester task\");\n\n    await streamAppendTesterTask\n      .triggerAndWait(\n        {},\n        {},\n        {\n          clientConfig: {\n            future: {\n              v2RealtimeStreams: false,\n            },\n          },\n        }\n      )\n      .unwrap();\n\n    await streamAppendTesterTask\n      .triggerAndWait(\n        {},\n        {},\n        {\n          clientConfig: {\n            future: {\n              v2RealtimeStreams: true,\n            },\n          },\n        }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 Stream append tester task completed\");\n\n    logger.info(\"Starting stream pipe tester task\");\n\n    await streamPipeTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream pipe tester task completed\");\n\n    await streamPipeTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: false } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream pipe tester task completed\");\n\n    logger.info(\"Starting stream writer tester task\");\n\n    await streamWriterTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream writer tester task completed\");\n\n    await streamWriterTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: false } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream writer tester task completed\");\n\n    logger.info(\"Starting stream wait until tester task\");\n\n    await streamWaitUntilTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream wait until tester task completed\");\n\n    await streamWaitUntilTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: false } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream wait until tester task completed\");\n\n    logger.info(\"Starting metadata tester task\");\n\n    await metadataTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Metadata tester task completed\");\n\n    await metadataTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: false } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Metadata tester task completed\");\n\n    logger.info(\"Starting stream read tester task\");\n\n    await streamReadTesterTask\n      .triggerAndWait({}, {}, { clientConfig: { future: { v2RealtimeStreams: true } } })\n      .unwrap();\n\n    logger.info(\"\u2705 Stream read tester task completed\");\n\n    logger.info(\"Starting streams stress tester task\");\n\n    await streamsStressTesterTask\n      .triggerAndWait(\n        { streamsVersion: \"v1\" },\n        {},\n        { clientConfig: { future: { v2RealtimeStreams: false } } }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 Streams stress tester task completed\");\n\n    await streamsStressTesterTask\n      .triggerAndWait(\n        { streamsVersion: \"v2\" },\n        {},\n        { clientConfig: { future: { v2RealtimeStreams: true } } }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 Streams stress tester task completed\");\n\n    logger.info(\"Starting end to end latency tester task\");\n\n    await endToEndLatencyTesterTask\n      .triggerAndWait(\n        { streamsVersion: \"v1\" },\n        {},\n        { clientConfig: { future: { v2RealtimeStreams: false } } }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 End to end latency tester task completed\");\n\n    await endToEndLatencyTesterTask\n      .triggerAndWait(\n        { streamsVersion: \"v2\" },\n        {},\n        { clientConfig: { future: { v2RealtimeStreams: true } } }\n      )\n      .unwrap();\n\n    logger.info(\"\u2705 End to end latency tester task completed\");\n\n    return {\n      message: \"Multiple source streams tester tasks completed\",\n    };\n  },\n});\n\nconst testStream = streams.define<string>({\n  id: \"test\",\n});\n\nconst multipleSourceStreamsTesterTask = task({\n  id: \"multiple-source-streams-tester\",\n  run: async (payload: any, { ctx }) => {\n    const stream1 = new ReadableStream<string>({\n      start(controller) {\n        controller.enqueue(\"stream 1 chunk 1\");\n        controller.enqueue(\"stream 1 chunk 2\");\n        controller.enqueue(\"stream 1 chunk 3\");\n        controller.close();\n      },\n    });\n\n    const stream2 = new ReadableStream<string>({\n      start(controller) {\n        controller.enqueue(\"stream 2 chunk 1\");\n        controller.enqueue(\"stream 2 chunk 2\");\n        controller.enqueue(\"stream 2 chunk 3\");\n        controller.close();\n      },\n    });\n\n    const { waitUntilComplete: waitUntilComplete1, stream: stream1Stream } = streams.pipe(stream1);\n    const { waitUntilComplete: waitUntilComplete2, stream: stream2Stream } = streams.pipe(stream2);\n\n    await Promise.all([waitUntilComplete1(), waitUntilComplete2()]);\n\n    const stream1Chunks = await convertReadableStreamToArray(stream1Stream);\n    const stream2Chunks = await convertReadableStreamToArray(stream2Stream);\n\n    assert.strictEqual(stream1Chunks.length, 3, \"Expected 3 chunks\");\n    assert.ok(stream1Chunks.includes(\"stream 1 chunk 1\"), \"Expected stream 1 chunk 1\");\n    assert.ok(stream1Chunks.includes(\"stream 1 chunk 2\"), \"Expected stream 1 chunk 2\");\n    assert.ok(stream1Chunks.includes(\"stream 1 chunk 3\"), \"Expected stream 1 chunk 3\");\n    assert.strictEqual(stream2Chunks.length, 3, \"Expected 3 chunks\");\n    assert.ok(stream2Chunks.includes(\"stream 2 chunk 1\"), \"Expected stream 2 chunk 1\");\n    assert.ok(stream2Chunks.includes(\"stream 2 chunk 2\"), \"Expected stream 2 chunk 2\");\n    assert.ok(stream2Chunks.includes(\"stream 2 chunk 3\"), \"Expected stream 2 chunk 3\");\n\n    const chunks = [];\n\n    for await (const chunk of await streams.read(ctx.run.id, { timeoutInSeconds: 5 })) {\n      chunks.push(chunk);\n    }\n\n    assert.strictEqual(chunks.length, 6, \"Expected 6 chunks\");\n    assert.ok(chunks.includes(\"stream 1 chunk 1\"), \"Expected stream 1 chunk 1\");\n    assert.ok(chunks.includes(\"stream 1 chunk 2\"), \"Expected stream 1 chunk 2\");\n    assert.ok(chunks.includes(\"stream 1 chunk 3\"), \"Expected stream 1 chunk 3\");\n    assert.ok(chunks.includes(\"stream 2 chunk 1\"), \"Expected stream 2 chunk 1\");\n    assert.ok(chunks.includes(\"stream 2 chunk 2\"), \"Expected stream 2 chunk 2\");\n    assert.ok(chunks.includes(\"stream 2 chunk 3\"), \"Expected stream 2 chunk 3\");\n\n    return {\n      message: \"Streams completed\",\n    };\n  },\n});\n\nconst streamAppendTesterTask = task({\n  id: \"stream-append-tester\",\n  run: async (payload: any, { ctx }) => {\n    await streams.append(\"chunk 1\");\n    await streams.append(\"chunk 2\");\n    await streams.append(\"chunk 3\");\n\n    const chunks = [];\n\n    for await (const chunk of await streams.read(ctx.run.id, { timeoutInSeconds: 5 })) {\n      chunks.push(chunk);\n    }\n\n    assert.strictEqual(chunks.length, 3, \"Expected 3 chunks\");\n    assert.ok(chunks.includes(\"chunk 1\"), \"Expected chunk 1\");\n    assert.ok(chunks.includes(\"chunk 2\"), \"Expected chunk 2\");\n    assert.ok(chunks.includes(\"chunk 3\"), \"Expected chunk 3\");\n\n    await streams.append(\"named\", \"chunk 1\");\n    await streams.append(\"named\", \"chunk 2\");\n    await streams.append(\"named\", \"chunk 3\");\n\n    const namedChunks = [];\n\n    for await (const chunk of await streams.read(ctx.run.id, \"named\", { timeoutInSeconds: 5 })) {\n      namedChunks.push(chunk);\n    }\n\n    assert.strictEqual(namedChunks.length, 3, \"Expected 3 chunks\");\n    assert.ok(namedChunks.includes(\"chunk 1\"), \"Expected chunk 1\");\n    assert.ok(namedChunks.includes(\"chunk 2\"), \"Expected chunk 2\");\n    assert.ok(namedChunks.includes(\"chunk 3\"), \"Expected chunk 3\");\n\n    await testStream.append(\"chunk 1\");\n    await testStream.append(\"chunk 2\");\n    await testStream.append(\"chunk 3\");\n\n    const testChunks = [];\n\n    for await (const chunk of await testStream.read(ctx.run.id, { timeoutInSeconds:",
            "cost": 0.161895
        }
    },
    "recover_status": "success",
    "instance_ref": {
        "instance_id": "triggerdotdev__trigger.dev.pr_mirror.2669",
        "repo": "triggerdotdev/trigger.dev",
        "base_commit": "d0ad38d684dda13c2accdd81a818822e4d985f88",
        "head_commit": "1289a3150d3791a3527aeb6b7823d8ea415177e3",
        "title": "fix(streams): buffer v1 streams on read to prevent split chunks",
        "merged_at": "2025-11-11T21:08:49Z",
        "html_url": "https://github.com/triggerdotdev/trigger.dev/pull/2669",
        "test_files": [
            "apps/webapp/test/redisRealtimeStreams.test.ts"
        ],
        "code_files": [
            "apps/webapp/app/services/realtime/redisRealtimeStreams.server.ts",
            "references/realtime-streams/src/trigger/streams.ts"
        ],
        "total_changes": 163,
        "num_files": 3,
        "pull_number": 2669,
        "patch": "diff --git a/apps/webapp/app/services/realtime/redisRealtimeStreams.server.ts b/apps/webapp/app/services/realtime/redisRealtimeStreams.server.ts\nindex 9db3809b52..b5c8c57322 100644\n--- a/apps/webapp/app/services/realtime/redisRealtimeStreams.server.ts\n+++ b/apps/webapp/app/services/realtime/redisRealtimeStreams.server.ts\n@@ -204,20 +204,52 @@ export class RedisRealtimeStreams implements StreamIngestor, StreamResponder {\n       },\n     })\n       .pipeThrough(\n-        // Transform 1: Split data content by newlines, preserving metadata\n-        new TransformStream<StreamChunk, StreamChunk & { line?: string }>({\n-          transform(chunk, controller) {\n-            if (chunk.type === \"ping\") {\n-              controller.enqueue(chunk);\n-            } else if (chunk.type === \"data\" || chunk.type === \"legacy-data\") {\n-              // Split data by newlines, emit separate chunks with same metadata\n-              const lines = chunk.data.split(\"\\n\").filter((line) => line.trim().length > 0);\n-              for (const line of lines) {\n-                controller.enqueue({ ...chunk, line });\n+        // Transform 1: Buffer partial lines across Redis entries\n+        (() => {\n+          let buffer = \"\";\n+          let lastRedisId = \"0\";\n+\n+          return new TransformStream<StreamChunk, StreamChunk & { line: string }>({\n+            transform(chunk, controller) {\n+              if (chunk.type === \"ping\") {\n+                controller.enqueue(chunk as any);\n+              } else if (chunk.type === \"data\" || chunk.type === \"legacy-data\") {\n+                // Buffer partial lines: accumulate until we see newlines\n+                buffer += chunk.data;\n+\n+                // Split on newlines\n+                const lines = buffer.split(\"\\n\");\n+\n+                // The last element might be incomplete, hold it back in buffer\n+                buffer = lines.pop() || \"\";\n+\n+                // Emit complete lines with the Redis ID of the chunk that completed them\n+                for (const line of lines) {\n+                  if (line.trim().length > 0) {\n+                    controller.enqueue({\n+                      ...chunk,\n+                      line,\n+                    });\n+                  }\n+                }\n+\n+                // Update last Redis ID for next iteration\n+                lastRedisId = chunk.redisId;\n               }\n-            }\n-          },\n-        })\n+            },\n+            flush(controller) {\n+              // On stream end, emit any leftover buffered text\n+              if (buffer.trim().length > 0) {\n+                controller.enqueue({\n+                  type: \"data\",\n+                  redisId: lastRedisId,\n+                  data: \"\",\n+                  line: buffer.trim(),\n+                });\n+              }\n+            },\n+          });\n+        })()\n       )\n       .pipeThrough(\n         // Transform 2: Format as SSE\ndiff --git a/apps/webapp/test/redisRealtimeStreams.test.ts b/apps/webapp/test/redisRealtimeStreams.test.ts\nindex e441e4ace6..0511754393 100644\n--- a/apps/webapp/test/redisRealtimeStreams.test.ts\n+++ b/apps/webapp/test/redisRealtimeStreams.test.ts\n@@ -1417,4 +1417,108 @@ describe(\"RedisRealtimeStreams\", () => {\n       await redis.quit();\n     }\n   );\n+\n+  redisTest(\n+    \"Should handle chunks split mid-line (regression test)\",\n+    { timeout: 30_000 },\n+    async ({ redisOptions }) => {\n+      const redis = new Redis(redisOptions);\n+      const redisRealtimeStreams = new RedisRealtimeStreams({\n+        redis: redisOptions,\n+      });\n+\n+      const runId = \"run_split_test\";\n+      const streamId = \"test-split-stream\";\n+\n+      // Simulate what happens in production: a JSON line split across multiple network chunks\n+      // This reproduces the issue where we see partial chunks like:\n+      // - \"{\\\"timestamp\\\":\"\n+      // - \"1762880245493,\\\"chunkIndex\\\":780,\\\"data\\\":\\\"Chunk 781/1000\\\"}\"\n+      const fullLine = JSON.stringify({\n+        timestamp: 1762880245493,\n+        chunkIndex: 780,\n+        data: \"Chunk 781/1000\",\n+      });\n+\n+      // Split the line at an arbitrary position (in the middle of the JSON)\n+      const splitPoint = 16; // Splits after '{\"timestamp\":'\n+      const chunk1 = fullLine.substring(0, splitPoint);\n+      const chunk2 = fullLine.substring(splitPoint);\n+\n+      // Create a ReadableStream that sends split chunks\n+      const encoder = new TextEncoder();\n+      const stream = new ReadableStream({\n+        start(controller) {\n+          controller.enqueue(encoder.encode(chunk1));\n+          controller.enqueue(encoder.encode(chunk2 + \"\\n\")); // Add newline at end\n+          controller.close();\n+        },\n+      });\n+\n+      // Ingest the split data\n+      await redisRealtimeStreams.ingestData(stream, runId, streamId, \"client1\");\n+\n+      // Now consume the stream and verify we get the complete line, not split chunks\n+      const abortController = new AbortController();\n+      const response = await redisRealtimeStreams.streamResponse(\n+        new Request(\"http://localhost/test\"),\n+        runId,\n+        streamId,\n+        abortController.signal\n+      );\n+\n+      const reader = response.body!.getReader();\n+      const decoder = new TextDecoder();\n+      let receivedData = \"\";\n+\n+      // Read all chunks from the response\n+      const readTimeout = setTimeout(() => {\n+        abortController.abort();\n+      }, 5000);\n+\n+      try {\n+        while (true) {\n+          const { done, value } = await reader.read();\n+          if (done) break;\n+\n+          receivedData += decoder.decode(value, { stream: true });\n+\n+          // Once we have data, we can stop\n+          if (receivedData.includes(\"data: \")) {\n+            break;\n+          }\n+        }\n+      } finally {\n+        clearTimeout(readTimeout);\n+        abortController.abort();\n+        reader.releaseLock();\n+      }\n+\n+      // Parse the SSE data\n+      const lines = receivedData.split(\"\\n\").filter((line) => line.startsWith(\"data: \"));\n+\n+      // We should receive exactly ONE complete line, not two partial lines\n+      expect(lines.length).toBe(1);\n+\n+      // Extract the data (remove \"data: \" prefix)\n+      const dataLine = lines[0].substring(6);\n+\n+      // Verify it's the complete, valid JSON\n+      expect(dataLine).toBe(fullLine);\n+\n+      // Verify it parses correctly as JSON\n+      const parsed = JSON.parse(dataLine) as {\n+        timestamp: number;\n+        chunkIndex: number;\n+        data: string;\n+      };\n+      expect(parsed.timestamp).toBe(1762880245493);\n+      expect(parsed.chunkIndex).toBe(780);\n+      expect(parsed.data).toBe(\"Chunk 781/1000\");\n+\n+      // Cleanup\n+      await redis.del(`stream:${runId}:${streamId}`);\n+      await redis.quit();\n+    }\n+  );\n });\ndiff --git a/references/realtime-streams/src/trigger/streams.ts b/references/realtime-streams/src/trigger/streams.ts\nindex cfa64618ce..388ff960aa 100644\n--- a/references/realtime-streams/src/trigger/streams.ts\n+++ b/references/realtime-streams/src/trigger/streams.ts\n@@ -822,7 +822,6 @@ const streamsStressTesterTask = task({\n \n     switch (payload.streamsVersion) {\n       case \"v1\": {\n-        assert.ok(chunks.length < 2000, \"Expected less than 2000 chunks\");\n         break;\n       }\n       case \"v2\": {\n",
        "test_patch": "",
        "problem_statement": "",
        "hints_text": "",
        "pr_mirror": "triggerdotdev__trigger.dev.d1c3bfb9"
    }
}