{
    "cost": 0.48955,
    "rewrites": {
        "triggerdotdev__trigger.dev.d1c3bfb9/apps/webapp/app/routes/resources.taskruns.$runParam.replay.ts": {
            "output": "import { parse } from \"@conform-to/zod\";\nimport { type ActionFunction, json, type LoaderFunctionArgs } from \"@remix-run/node\";\nimport { type EnvironmentType, prettyPrintPacket } from \"@trigger.dev/core/v3\";\nimport { typedjson } from \"remix-typedjson\";\nimport { z } from \"zod\";\nimport { $replica, prisma } from \"~/db.server\";\nimport { redirectWithErrorMessage, redirectWithSuccessMessage } from \"~/models/message.server\";\nimport { displayableEnvironment } from \"~/models/runtimeEnvironment.server\";\nimport { logger } from \"~/services/logger.server\";\nimport { requireUserId } from \"~/services/session.server\";\nimport { sortEnvironments } from \"~/utils/environmentSort\";\nimport { v3RunSpanPath } from \"~/utils/pathBuilder\";\nimport { ReplayTaskRunService } from \"~/v3/services/replayTaskRun.server\";\nimport parseDuration from \"parse-duration\";\nimport { findCurrentWorkerDeployment } from \"~/v3/models/workerDeployment.server\";\nimport { queueTypeFromType } from \"~/presenters/v3/QueueRetrievePresenter.server\";\nimport { ReplayRunData } from \"~/v3/replayTask\";\n\nconst ParamSchema = z.object({\n  runParam: z.string(),\n});\n\nconst QuerySchema = z.object({\n  environmentIdOverride: z.string().optional(),\n});\n\nexport async function loader({ request, params }: LoaderFunctionArgs) {\n  const userId = await requireUserId(request);\n  const { runParam } = ParamSchema.parse(params);\n  const { environmentIdOverride } = QuerySchema.parse(\n    Object.fromEntries(new URL(request.url).searchParams)\n  );\n\n  const run = await $replica.taskRun.findFirst({\n    select: {\n      payload: true,\n      payloadType: true,\n      seedMetadata: true,\n      seedMetadataType: true,\n      runtimeEnvironmentId: true,\n      concurrencyKey: true,\n      maxAttempts: true,\n      maxDurationInSeconds: true,\n      machinePreset: true,\n      ttl: true,\n      idempotencyKey: true,\n      runTags: true,\n      queue: true,\n      taskIdentifier: true,\n      project: {\n        select: {\n          environments: {\n            select: {\n              id: true,\n              type: true,\n              slug: true,\n              branchName: true,\n              orgMember: {\n                select: {\n                  user: true,\n                },\n              },\n            },\n            where: {\n              archivedAt: null,\n              OR: [\n                {\n                  type: {\n                    in: [\"PREVIEW\", \"STAGING\", \"PRODUCTION\"],\n                  },\n                },\n                {\n                  type: \"DEVELOPMENT\",\n                  orgMember: {\n                    userId,\n                  },\n                },\n              ],\n            },\n          },\n        },\n      },\n    },\n    where: { friendlyId: runParam, project: { organization: { members: { some: { userId } } } } },\n  });\n\n  if (!run) {\n    throw new Response(\"Not Found\", { status: 404 });\n  }\n\n  const runEnvironment = run.project.environments.find(\n    (env) => env.id === run.runtimeEnvironmentId\n  );\n  const environmentOverride = run.project.environments.find(\n    (env) => env.id === environmentIdOverride\n  );\n  const environment = environmentOverride ?? runEnvironment;\n  if (!environment) {\n    throw new Response(\"Environment not found\", { status: 404 });\n  }\n\n  const [taskQueue, backgroundWorkers] = await Promise.all([\n    findTaskQueue(environment, run.taskIdentifier),\n    listLatestBackgroundWorkers(environment),\n  ]);\n\n  const latestVersions = backgroundWorkers.map((v) => v.version);\n  const disableVersionSelection = environment.type === \"DEVELOPMENT\";\n  const allowArbitraryQueues = backgroundWorkers.at(0)?.engine === \"V1\";\n\n  return typedjson({\n    concurrencyKey: run.concurrencyKey,\n    maxAttempts: run.maxAttempts,\n    maxDurationSeconds: run.maxDurationInSeconds,\n    machinePreset: run.machinePreset,\n    ttlSeconds: run.ttl ? parseDuration(run.ttl, \"s\") ?? undefined : undefined,\n    idempotencyKey: run.idempotencyKey,\n    runTags: run.runTags,\n    payload: await prettyPrintPacket(run.payload, run.payloadType),\n    payloadType: run.payloadType,\n    queue: run.queue,\n    metadata: run.seedMetadata\n      ? await prettyPrintPacket(run.seedMetadata, run.seedMetadataType)\n      : undefined,\n    defaultTaskQueue: taskQueue\n      ? {\n          id: taskQueue.friendlyId,\n          name: taskQueue.name.replace(/^task\\//, \"\"),\n          type: queueTypeFromType(taskQueue.type),\n          paused: taskQueue.paused,\n        }\n      : undefined,\n    latestVersions,\n    disableVersionSelection,\n    allowArbitraryQueues,\n    environment: {\n      ...displayableEnvironment(environment, userId),\n      branchName: environment.branchName ?? undefined,\n    },\n    environments: sortEnvironments(\n      run.project.environments\n        .filter((env) => env.type !== \"PREVIEW\" || env.branchName)\n        .map((env) => ({\n          ...displayableEnvironment(env, userId),\n          branchName: env.branchName ?? undefined,\n        }))\n    ),\n  });\n}\n\nexport const action: ActionFunction = async ({ request, params }) => {\n  const { runParam } = ParamSchema.parse(params);\n\n  const formData = await request.formData();\n  const submission = parse(formData, { schema: ReplayRunData });\n\n  if (!submission.value) {\n    return json(submission);\n  }\n\n  try {\n    const taskRun = await prisma.taskRun.findFirst({\n      where: {\n        friendlyId: runParam,\n      },\n      include: {\n        runtimeEnvironment: {\n          select: {\n            slug: true,\n          },\n        },\n        project: {\n          include: {\n            organization: true,\n          },\n        },\n      },\n    });\n\n    if (!taskRun) {\n      return redirectWithErrorMessage(submission.value.failedRedirect, request, \"Run not found\");\n    }\n\n    const replayRunService = new ReplayTaskRunService();\n    const newRun = await replayRunService.call(taskRun, {\n      environmentId: submission.value.environment,\n      payload: submission.value.payload,\n      metadata: submission.value.metadata,\n      tags: submission.value.tags,\n      queue: submission.value.queue,\n      concurrencyKey: submission.value.concurrencyKey,\n      maxAttempts: submission.value.maxAttempts,\n      maxDurationSeconds: submission.value.maxDurationSeconds,\n      machine: submission.value.machine,\n      delaySeconds: submission.value.delaySeconds,\n      idempotencyKey: submission.value.idempotencyKey,\n      idempotencyKeyTTLSeconds: submission.value.idempotencyKeyTTLSeconds,\n      ttlSeconds: submission.value.ttlSeconds,\n      version: submission.value.version,\n    });\n\n    if (!newRun) {\n      return redirectWithErrorMessage(\n        submission.value.failedRedirect,\n        request,\n        \"Failed to replay run\"\n      );\n    }\n\n    const runPath = v3RunSpanPath(\n      {\n        slug: taskRun.project.organization.slug,\n      },\n      { slug: taskRun.project.slug },\n      { slug: taskRun.runtimeEnvironment.slug },\n      { friendlyId: newRun.friendlyId },\n      { spanId: newRun.spanId }\n    );\n\n    logger.debug(\"Replayed run\", {\n      taskRunId: taskRun.id,\n      taskRunFriendlyId: taskRun.friendlyId,\n      newRunId: newRun.id,\n      newRunFriendlyId: newRun.friendlyId,\n      runPath,\n    });\n\n    return redirectWithSuccessMessage(runPath, request, `Replaying run`);\n  } catch (error) {\n    if (error instanceof Error) {\n      logger.error(\"Failed to replay run\", {\n        error: {\n          name: error.name,\n          message: error.message,\n          stack: error.stack,\n        },\n      });\n      return redirectWithErrorMessage(submission.value.failedRedirect, request, error.message);\n    }\n\n    logger.error(\"Failed to replay run\", { error });\n    return redirectWithErrorMessage(\n      submission.value.failedRedirect,\n      request,\n      JSON.stringify(error)\n    );\n  }\n};\n\nasync function findTask(\n  environment: { type: EnvironmentType; id: string },\n  taskIdentifier: string\n) {\n  if (environment.type === \"DEVELOPMENT\") {\n    return $replica.backgroundWorkerTask.findFirst({\n      select: {\n        queueId: true,\n      },\n      where: {\n        slug: taskIdentifier,\n        runtimeEnvironmentId: environment.id,\n      },\n      orderBy: {\n        createdAt: \"desc\",\n      },\n    });\n  }\n\n  const currentDeployment = await findCurrentWorkerDeployment({\n    environmentId: environment.id,\n  });\n  return currentDeployment?.worker?.tasks.find((t) => t.slug === taskIdentifier);\n}\n\nasync function findTaskQueue(\n  environment: { type: EnvironmentType; id: string },\n  taskIdentifier: string\n) {\n  const task = await findTask(environment, taskIdentifier);\n\n  if (!task?.queueId) {\n    return undefined;\n  }\n\n  return $replica.taskQueue.findFirst({\n    where: {\n      runtimeEnvironmentId: environment.id,\n      id: task.queueId,\n    },\n    select: {\n      friendlyId: true,\n      name: true,\n      type: true,\n      paused: true,\n    },\n  });\n}\n\nfunction listLatestBackgroundWorkers(environment: { id: string }, limit = 20) {\n  return $replica.backgroundWorker.findMany({\n    where: {\n      runtimeEnvironmentId: environment.id,\n    },\n    select: {\n      version: true,\n      engine: true,\n    },\n    orderBy: {\n      createdAt: \"desc\",\n    },\n    take: limit,\n  });\n}",
            "output_extracted": "import { parse } from \"@conform-to/zod\";\nimport { type ActionFunction, json, type LoaderFunctionArgs } from \"@remix-run/node\";\nimport { type EnvironmentType, prettyPrintPacket } from \"@trigger.dev/core/v3\";\nimport { typedjson } from \"remix-typedjson\";\nimport { z } from \"zod\";\nimport { $replica, prisma } from \"~/db.server\";\nimport { redirectWithErrorMessage, redirectWithSuccessMessage } from \"~/models/message.server\";\nimport { displayableEnvironment } from \"~/models/runtimeEnvironment.server\";\nimport { logger } from \"~/services/logger.server\";\nimport { requireUserId } from \"~/services/session.server\";\nimport { sortEnvironments } from \"~/utils/environmentSort\";\nimport { v3RunSpanPath } from \"~/utils/pathBuilder\";\nimport { ReplayTaskRunService } from \"~/v3/services/replayTaskRun.server\";\nimport parseDuration from \"parse-duration\";\nimport { findCurrentWorkerDeployment } from \"~/v3/models/workerDeployment.server\";\nimport { queueTypeFromType } from \"~/presenters/v3/QueueRetrievePresenter.server\";\nimport { ReplayRunData } from \"~/v3/replayTask\";\n\nconst ParamSchema = z.object({\n  runParam: z.string(),\n});\n\nconst QuerySchema = z.object({\n  environmentIdOverride: z.string().optional(),\n});\n\nexport async function loader({ request, params }: LoaderFunctionArgs) {\n  const userId = await requireUserId(request);\n  const { runParam } = ParamSchema.parse(params);\n  const { environmentIdOverride } = QuerySchema.parse(\n    Object.fromEntries(new URL(request.url).searchParams)\n  );\n\n  const run = await $replica.taskRun.findFirst({\n    select: {\n      payload: true,\n      payloadType: true,\n      seedMetadata: true,\n      seedMetadataType: true,\n      runtimeEnvironmentId: true,\n      concurrencyKey: true,\n      maxAttempts: true,\n      maxDurationInSeconds: true,\n      machinePreset: true,\n      ttl: true,\n      idempotencyKey: true,\n      runTags: true,\n      queue: true,\n      taskIdentifier: true,\n      project: {\n        select: {\n          environments: {\n            select: {\n              id: true,\n              type: true,\n              slug: true,\n              branchName: true,\n              orgMember: {\n                select: {\n                  user: true,\n                },\n              },\n            },\n            where: {\n              archivedAt: null,\n              OR: [\n                {\n                  type: {\n                    in: [\"PREVIEW\", \"STAGING\", \"PRODUCTION\"],\n                  },\n                },\n                {\n                  type: \"DEVELOPMENT\",\n                  orgMember: {\n                    userId,\n                  },\n                },\n              ],\n            },\n          },\n        },\n      },\n    },\n    where: { friendlyId: runParam, project: { organization: { members: { some: { userId } } } } },\n  });\n\n  if (!run) {\n    throw new Response(\"Not Found\", { status: 404 });\n  }\n\n  const runEnvironment = run.project.environments.find(\n    (env) => env.id === run.runtimeEnvironmentId\n  );\n  const environmentOverride = run.project.environments.find(\n    (env) => env.id === environmentIdOverride\n  );\n  const environment = environmentOverride ?? runEnvironment;\n  if (!environment) {\n    throw new Response(\"Environment not found\", { status: 404 });\n  }\n\n  const [taskQueue, backgroundWorkers] = await Promise.all([\n    findTaskQueue(environment, run.taskIdentifier),\n    listLatestBackgroundWorkers(environment),\n  ]);\n\n  const latestVersions = backgroundWorkers.map((v) => v.version);\n  const disableVersionSelection = environment.type === \"DEVELOPMENT\";\n  const allowArbitraryQueues = backgroundWorkers.at(0)?.engine === \"V1\";\n\n  return typedjson({\n    concurrencyKey: run.concurrencyKey,\n    maxAttempts: run.maxAttempts,\n    maxDurationSeconds: run.maxDurationInSeconds,\n    machinePreset: run.machinePreset,\n    ttlSeconds: run.ttl ? parseDuration(run.ttl, \"s\") ?? undefined : undefined,\n    idempotencyKey: run.idempotencyKey,\n    runTags: run.runTags,\n    payload: await prettyPrintPacket(run.payload, run.payloadType),\n    payloadType: run.payloadType,\n    queue: run.queue,\n    metadata: run.seedMetadata\n      ? await prettyPrintPacket(run.seedMetadata, run.seedMetadataType)\n      : undefined,\n    defaultTaskQueue: taskQueue\n      ? {\n          id: taskQueue.friendlyId,\n          name: taskQueue.name.replace(/^task\\//, \"\"),\n          type: queueTypeFromType(taskQueue.type),\n          paused: taskQueue.paused,\n        }\n      : undefined,\n    latestVersions,\n    disableVersionSelection,\n    allowArbitraryQueues,\n    environment: {\n      ...displayableEnvironment(environment, userId),\n      branchName: environment.branchName ?? undefined,\n    },\n    environments: sortEnvironments(\n      run.project.environments\n        .filter((env) => env.type !== \"PREVIEW\" || env.branchName)\n        .map((env) => ({\n          ...displayableEnvironment(env, userId),\n          branchName: env.branchName ?? undefined,\n        }))\n    ),\n  });\n}\n\nexport const action: ActionFunction = async ({ request, params }) => {\n  const { runParam } = ParamSchema.parse(params);\n\n  const formData = await request.formData();\n  const submission = parse(formData, { schema: ReplayRunData });\n\n  if (!submission.value) {\n    return json(submission);\n  }\n\n  try {\n    const taskRun = await prisma.taskRun.findFirst({\n      where: {\n        friendlyId: runParam,\n      },\n      include: {\n        runtimeEnvironment: {\n          select: {\n            slug: true,\n          },\n        },\n        project: {\n          include: {\n            organization: true,\n          },\n        },\n      },\n    });\n\n    if (!taskRun) {\n      return redirectWithErrorMessage(submission.value.failedRedirect, request, \"Run not found\");\n    }\n\n    const replayRunService = new ReplayTaskRunService();\n    const newRun = await replayRunService.call(taskRun, {\n      environmentId: submission.value.environment,\n      payload: submission.value.payload,\n      metadata: submission.value.metadata,\n      tags: submission.value.tags,\n      queue: submission.value.queue,\n      concurrencyKey: submission.value.concurrencyKey,\n      maxAttempts: submission.value.maxAttempts,\n      maxDurationSeconds: submission.value.maxDurationSeconds,\n      machine: submission.value.machine,\n      delaySeconds: submission.value.delaySeconds,\n      idempotencyKey: submission.value.idempotencyKey,\n      idempotencyKeyTTLSeconds: submission.value.idempotencyKeyTTLSeconds,\n      ttlSeconds: submission.value.ttlSeconds,\n      version: submission.value.version,\n    });\n\n    if (!newRun) {\n      return redirectWithErrorMessage(\n        submission.value.failedRedirect,\n        request,\n        \"Failed to replay run\"\n      );\n    }\n\n    const runPath = v3RunSpanPath(\n      {\n        slug: taskRun.project.organization.slug,\n      },\n      { slug: taskRun.project.slug },\n      { slug: taskRun.runtimeEnvironment.slug },\n      { friendlyId: newRun.friendlyId },\n      { spanId: newRun.spanId }\n    );\n\n    logger.debug(\"Replayed run\", {\n      taskRunId: taskRun.id,\n      taskRunFriendlyId: taskRun.friendlyId,\n      newRunId: newRun.id,\n      newRunFriendlyId: newRun.friendlyId,\n      runPath,\n    });\n\n    return redirectWithSuccessMessage(runPath, request, `Replaying run`);\n  } catch (error) {\n    if (error instanceof Error) {\n      logger.error(\"Failed to replay run\", {\n        error: {\n          name: error.name,\n          message: error.message,\n          stack: error.stack,\n        },\n      });\n      return redirectWithErrorMessage(submission.value.failedRedirect, request, error.message);\n    }\n\n    logger.error(\"Failed to replay run\", { error });\n    return redirectWithErrorMessage(\n      submission.value.failedRedirect,\n      request,\n      JSON.stringify(error)\n    );\n  }\n};\n\nasync function findTask(\n  environment: { type: EnvironmentType; id: string },\n  taskIdentifier: string\n) {\n  if (environment.type === \"DEVELOPMENT\") {\n    return $replica.backgroundWorkerTask.findFirst({\n      select: {\n        queueId: true,\n      },\n      where: {\n        slug: taskIdentifier,\n        runtimeEnvironmentId: environment.id,\n      },\n      orderBy: {\n        createdAt: \"desc\",\n      },\n    });\n  }\n\n  const currentDeployment = await findCurrentWorkerDeployment({\n    environmentId: environment.id,\n  });\n  return currentDeployment?.worker?.tasks.find((t) => t.slug === taskIdentifier);\n}\n\nasync function findTaskQueue(\n  environment: { type: EnvironmentType; id: string },\n  taskIdentifier: string\n) {\n  const task = await findTask(environment, taskIdentifier);\n\n  if (!task?.queueId) {\n    return undefined;\n  }\n\n  return $replica.taskQueue.findFirst({\n    where: {\n      runtimeEnvironmentId: environment.id,\n      id: task.queueId,\n    },\n    select: {\n      friendlyId: true,\n      name: true,\n      type: true,\n      paused: true,\n    },\n  });\n}\n\nfunction listLatestBackgroundWorkers(environment: { id: string }, limit = 20) {\n  return $replica.backgroundWorker.findMany({\n    where: {\n      runtimeEnvironmentId: environment.id,\n    },\n    select: {\n      version: true,\n      engine: true,\n    },\n    orderBy: {\n      createdAt: \"desc\",\n    },\n    take: limit,\n  });\n}",
            "cost": 0.0873
        },
        "triggerdotdev__trigger.dev.d1c3bfb9/apps/webapp/test/fairDequeuingStrategy.test.ts": {
            "output": "import { describe, expect, vi } from \"vitest\";\n\n// Mock the db prisma client\nvi.mock(\"~/db.server\", () => ({\n  prisma: {},\n  $replica: {},\n}));\n\nimport { redisTest } from \"@internal/testcontainers\";\nimport { FairDequeuingStrategy } from \"../app/v3/marqs/fairDequeuingStrategy.server.js\";\nimport {\n  calculateStandardDeviation,\n  createKeyProducer,\n  setupConcurrency,\n  setupQueue,\n} from \"./utils/marqs.js\";\nimport { trace } from \"@opentelemetry/api\";\nimport { EnvQueues } from \"~/v3/marqs/types.js\";\nimport { MARQS_RESUME_PRIORITY_TIMESTAMP_OFFSET } from \"~/v3/marqs/constants.server.js\";\nimport { createRedisClient } from \"@internal/redis\";\n\nconst tracer = trace.getTracer(\"test\");\n\nvi.setConfig({ testTimeout: 30_000 }); // 30 seconds timeout\n\ndescribe(\"FairDequeuingStrategy\", () => {\n  redisTest(\"should distribute a single queue from a single env\", async ({ redisOptions }) => {\n    const redis = createRedisClient(redisOptions);\n\n    const keyProducer = createKeyProducer(\"test\");\n    const strategy = new FairDequeuingStrategy({\n      tracer,\n      redis,\n      keys: keyProducer,\n      defaultEnvConcurrency: 5,\n      parentQueueLimit: 100,\n      seed: \"test-seed-1\", // for deterministic shuffling\n    });\n\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: Date.now() - 1000, // 1 second ago\n      queueId: \"queue-1\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    const result = await strategy.distributeFairQueuesFromParentQueue(\"parent-queue\", \"consumer-1\");\n\n    expect(result).toHaveLength(1);\n    expect(result[0]).toEqual({\n      envId: \"env-1\",\n      queues: [\"org:org-1:env:env-1:queue:queue-1\"],\n    });\n  });\n\n  redisTest(\"should respect env concurrency limits\", async ({ redisOptions }) => {\n    const redis = createRedisClient(redisOptions);\n\n    const keyProducer = createKeyProducer(\"test\");\n    const strategy = new FairDequeuingStrategy({\n      tracer,\n      redis,\n      keys: keyProducer,\n      defaultEnvConcurrency: 2,\n      parentQueueLimit: 100,\n      seed: \"test-seed-3\",\n    });\n\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: Date.now() - 1000,\n      queueId: \"queue-1\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    await setupConcurrency({\n      redis,\n      keyProducer,\n      env: { id: \"env-1\", currentConcurrency: 2, limit: 2 },\n    });\n\n    const result = await strategy.distributeFairQueuesFromParentQueue(\"parent-queue\", \"consumer-1\");\n    expect(result).toHaveLength(0);\n  });\n\n  redisTest(\n    \"should give extra concurrency when the env has reserve concurrency\",\n    async ({ redisOptions }) => {\n      const redis = createRedisClient(redisOptions);\n\n      const keyProducer = createKeyProducer(\"test\");\n      const strategy = new FairDequeuingStrategy({\n        tracer,\n        redis,\n        keys: keyProducer,\n        defaultEnvConcurrency: 2,\n        parentQueueLimit: 100,\n        seed: \"test-seed-3\",\n      });\n\n      await setupQueue({\n        redis,\n        keyProducer,\n        parentQueue: \"parent-queue\",\n        score: Date.now() - 1000,\n        queueId: \"queue-1\",\n        orgId: \"org-1\",\n        envId: \"env-1\",\n      });\n\n      await setupConcurrency({\n        redis,\n        keyProducer,\n        env: { id: \"env-1\", currentConcurrency: 2, limit: 2, reserveConcurrency: 1 },\n      });\n\n      const result = await strategy.distributeFairQueuesFromParentQueue(\n        \"parent-queue\",\n        \"consumer-1\"\n      );\n      expect(result).toHaveLength(1);\n      expect(result[0]).toEqual({\n        envId: \"env-1\",\n        queues: [\"org:org-1:env:env-1:queue:queue-1\"],\n      });\n    }\n  );\n\n  redisTest(\"should respect parentQueueLimit\", async ({ redisOptions }) => {\n    const redis = createRedisClient(redisOptions);\n\n    const keyProducer = createKeyProducer(\"test\");\n    const strategy = new FairDequeuingStrategy({\n      tracer,\n      redis,\n      keys: keyProducer,\n      defaultEnvConcurrency: 5,\n      parentQueueLimit: 2, // Only take 2 queues\n      seed: \"test-seed-6\",\n    });\n\n    const now = Date.now();\n\n    // Setup 3 queues but parentQueueLimit is 2\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: now - 3000,\n      queueId: \"queue-1\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: now - 2000,\n      queueId: \"queue-2\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: now - 1000,\n      queueId: \"queue-3\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    const result = await strategy.distributeFairQueuesFromParentQueue(\"parent-queue\", \"consumer-1\");\n\n    expect(result).toHaveLength(1);\n    const queue1 = keyProducer.queueKey(\"org-1\", \"env-1\", \"queue-1\");\n    const queue2 = keyProducer.queueKey(\"org-1\", \"env-1\", \"queue-2\");\n    expect(result[0]).toEqual({\n      envId: \"env-1\",\n      queues: [queue1, queue2],\n    });\n  });\n\n  redisTest(\n    \"should reuse snapshots across calls for the same consumer\",\n    async ({ redisOptions }) => {\n      const redis = createRedisClient(redisOptions);\n\n      const keyProducer = createKeyProducer(\"test\");\n      const strategy = new FairDequeuingStrategy({\n        tracer,\n        redis,\n        keys: keyProducer,\n        defaultEnvConcurrency: 5,\n        parentQueueLimit: 10,\n        seed: \"test-seed-reuse-1\",\n        reuseSnapshotCount: 1,\n      });\n\n      const now = Date.now();\n\n      await setupQueue({\n        redis,\n        keyProducer,\n        parentQueue: \"parent-queue\",\n        score: now - 3000,\n        queueId: \"queue-1\",\n        orgId: \"org-1\",\n        envId: \"env-1\",\n      });\n\n      await setupQueue({\n        redis,\n        keyProducer,\n        parentQueue: \"parent-queue\",\n        score: now - 2000,\n        queueId: \"queue-2\",\n        orgId: \"org-2\",\n        envId: \"env-2\",\n      });\n\n      await setupQueue({\n        redis,\n        keyProducer,\n        parentQueue: \"parent-queue\",\n        score: now - 1000,\n        queueId: \"queue-3\",\n        orgId: \"org-3\",\n        envId: \"env-3\",\n      });\n\n      const startDistribute1 = performance.now();\n\n      const envResult = await strategy.distributeFairQueuesFromParentQueue(\n        \"parent-queue\",\n        \"consumer-1\"\n      );\n      const result = flattenResults(envResult);\n\n      const distribute1Duration = performance.now() - startDistribute1;\n\n      console.log(\"First distribution took\", distribute1Duration, \"ms\");\n\n      expect(result).toHaveLength(3);\n      // Should only get the two oldest queues\n      const queue1 = keyProducer.queueKey(\"org-1\", \"env-1\", \"queue-1\");\n      const queue2 = keyProducer.queueKey(\"org-2\", \"env-2\", \"queue-2\");\n      const queue3 = keyProducer.queueKey(\"org-3\", \"env-3\", \"queue-3\");\n      expect(result).toEqual([queue2, queue1, queue3]);\n\n      const startDistribute2 = performance.now();\n\n      const result2 = await strategy.distributeFairQueuesFromParentQueue(\n        \"parent-queue\",\n        \"consumer-1\"\n      );\n\n      const tolerance = 0.15;\n      const withTolerance = (value: number) => value * (1 + tolerance);\n\n      const distribute2Duration = performance.now() - startDistribute2;\n\n      console.log(\"Second distribution took\", distribute2Duration, \"ms\");\n\n      // Make sure the second call is more than 2 times faster than the first\n      expect(distribute2Duration).toBeLessThan(withTolerance(distribute1Duration / 2));\n\n      const startDistribute3 = performance.now();\n\n      const result3 = await strategy.distributeFairQueuesFromParentQueue(\n        \"parent-queue\",\n        \"consumer-1\"\n      );\n\n      const distribute3Duration = performance.now() - startDistribute3;\n\n      console.log(\"Third distribution took\", distribute3Duration, \"ms\");\n\n      // Make sure the third call is more than 4 times the second\n      expect(withTolerance(distribute3Duration)).toBeGreaterThan(distribute2Duration * 4);\n    }\n  );\n\n  redisTest(\n    \"should fairly distribute queues across environments over time\",\n    async ({ redisOptions }) => {\n      const redis = createRedisClient(redisOptions);\n\n      const keyProducer = createKeyProducer(\"test\");\n      const strategy = new FairDequeuingStrategy({\n        tracer,\n        redis,\n        keys: keyProducer,\n        defaultEnvConcurrency: 5,\n        parentQueueLimit: 100,\n        seed: \"test-seed-5\",\n      });\n\n      const now = Date.now();\n\n      // Test configuration\n      const orgs = [\"org-1\", \"org-2\", \"org-3\"];\n      const envsPerOrg = 3; // Each org has 3 environments\n      const queuesPerEnv = 5; // Each env has 5 queues\n      const iterations = 1000;\n\n      // Setup queues\n      for (const orgId of orgs) {\n        for (let envNum = 1; envNum <= envsPerOrg; envNum++) {\n          const envId = `env-${orgId}-${envNum}`;\n\n          for (let queueNum = 1; queueNum <= queuesPerEnv; queueNum++) {\n            await setupQueue({\n              redis,\n              keyProducer,\n              parentQueue: \"parent-queue\",\n              // Vary the ages slightly\n              score: now - Math.random() * 10000,\n              queueId: `queue-${orgId}-${envId}-${queueNum}`,\n              orgId,\n              envId,\n            });\n          }\n\n          // Setup reasonable concurrency limits\n          await setupConcurrency({\n            redis,\n            keyProducer,\n            env: { id: envId, currentConcurrency: 1, limit: 5 },\n          });\n        }\n      }\n\n      // Track distribution statistics\n      type PositionStats = {\n        firstPosition: number; // Count of times this env/org was first\n        positionSums: number; // Sum of positions (for averaging)\n        appearances: number; // Total number of appearances\n      };\n\n      const envStats: Record<string, PositionStats> = {};\n      const orgStats: Record<string, PositionStats> = {};\n\n      // Initialize stats objects\n      for (const orgId of orgs) {\n        orgStats[orgId] = { firstPosition: 0, positionSums: 0, appearances: 0 };\n        for (let envNum = 1; envNum <= envsPerOrg; envNum++) {\n          const envId = `env-${orgId}-${envNum}`;\n          envStats[envId] = { firstPosition: 0, positionSums: 0, appearances: 0 };\n        }\n      }\n\n      // Run multiple iterations\n      for (let i = 0; i < iterations; i++) {\n        const envResult = await strategy.distributeFairQueuesFromParentQueue(\n          \"parent-queue\",\n          `consumer-${i % 3}` // Simulate 3 different consumers\n        );\n        const result = flattenResults(envResult);\n\n        // Track positions of queues\n        result.forEach((queueId, position) => {\n          const orgId = keyProducer.orgIdFromQueue(queueId);\n          const envId = keyProducer.envIdFromQueue(queueId);\n\n          // Update org stats\n          orgStats[orgId].appearances++;\n          orgStats[orgId].positionSums += position;\n          if (position === 0) orgStats[orgId].firstPosition++;\n\n          // Update env stats\n          envStats[envId].appearances++;\n          envStats[envId].positionSums += position;\n          if (position === 0) envStats[envId].firstPosition++;\n        });\n      }\n\n      // Calculate and log statistics\n      console.log(\"\\nOrganization Statistics:\");\n      for (const [orgId, stats] of Object.entries(orgStats)) {\n        const avgPosition = stats.positionSums / stats.appearances;\n        const firstPositionPercentage = (stats.firstPosition / iterations) * 100;\n        console.log(`${orgId}:\n      First Position: ${firstPositionPercentage.toFixed(2)}%\n      Average Position: ${avgPosition.toFixed(2)}\n      Total Appearances: ${stats.appearances}`);\n      }\n\n      console.log(\"\\nEnvironment Statistics:\");\n      for (const [envId, stats] of Object.entries(envStats)) {\n        const avgPosition = stats.positionSums / stats.appearances;\n        const firstPositionPercentage = (stats.firstPosition / iterations) * 100;\n        console.log(`${envId}:\n      First Position: ${firstPositionPercentage.toFixed(2)}%\n      Average Position: ${avgPosition.toFixed(2)}\n      Total Appearances: ${stats.appearances}`);\n      }\n\n      // Verify fairness of first position distribution\n      const expectedFirstPositionPercentage = 100 / orgs.length;\n      const firstPositionStdDevOrgs = calculateStandardDeviation(\n        Object.values(orgStats).map((stats) => (stats.firstPosition / iterations) * 100)\n      );\n\n      const expectedEnvFirstPositionPercentage = 100 / (orgs.length * envsPerOrg);\n      const firstPositionStdDevEnvs = calculateStandardDeviation(\n        Object.values(envStats).map((stats) => (stats.firstPosition / iterations) * 100)\n      );\n\n      // Assert reasonable fairness for first position\n      expect(firstPositionStdDevOrgs).toBeLessThan(5); // Allow 5% standard deviation for orgs\n      expect(firstPositionStdDevEnvs).toBeLessThan(5); // Allow 5% standard deviation for envs\n\n      // Verify that each org and env gets a fair chance at first position\n      for (const [orgId, stats] of Object.entries(orgStats)) {\n        const firstPositionPercentage = (stats.firstPosition / iterations) * 100;\n        expect(firstPositionPercentage).to",
            "output_extracted": "import { describe, expect, vi } from \"vitest\";\n\n// Mock the db prisma client\nvi.mock(\"~/db.server\", () => ({\n  prisma: {},\n  $replica: {},\n}));\n\nimport { redisTest } from \"@internal/testcontainers\";\nimport { FairDequeuingStrategy } from \"../app/v3/marqs/fairDequeuingStrategy.server.js\";\nimport {\n  calculateStandardDeviation,\n  createKeyProducer,\n  setupConcurrency,\n  setupQueue,\n} from \"./utils/marqs.js\";\nimport { trace } from \"@opentelemetry/api\";\nimport { EnvQueues } from \"~/v3/marqs/types.js\";\nimport { MARQS_RESUME_PRIORITY_TIMESTAMP_OFFSET } from \"~/v3/marqs/constants.server.js\";\nimport { createRedisClient } from \"@internal/redis\";\n\nconst tracer = trace.getTracer(\"test\");\n\nvi.setConfig({ testTimeout: 30_000 }); // 30 seconds timeout\n\ndescribe(\"FairDequeuingStrategy\", () => {\n  redisTest(\"should distribute a single queue from a single env\", async ({ redisOptions }) => {\n    const redis = createRedisClient(redisOptions);\n\n    const keyProducer = createKeyProducer(\"test\");\n    const strategy = new FairDequeuingStrategy({\n      tracer,\n      redis,\n      keys: keyProducer,\n      defaultEnvConcurrency: 5,\n      parentQueueLimit: 100,\n      seed: \"test-seed-1\", // for deterministic shuffling\n    });\n\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: Date.now() - 1000, // 1 second ago\n      queueId: \"queue-1\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    const result = await strategy.distributeFairQueuesFromParentQueue(\"parent-queue\", \"consumer-1\");\n\n    expect(result).toHaveLength(1);\n    expect(result[0]).toEqual({\n      envId: \"env-1\",\n      queues: [\"org:org-1:env:env-1:queue:queue-1\"],\n    });\n  });\n\n  redisTest(\"should respect env concurrency limits\", async ({ redisOptions }) => {\n    const redis = createRedisClient(redisOptions);\n\n    const keyProducer = createKeyProducer(\"test\");\n    const strategy = new FairDequeuingStrategy({\n      tracer,\n      redis,\n      keys: keyProducer,\n      defaultEnvConcurrency: 2,\n      parentQueueLimit: 100,\n      seed: \"test-seed-3\",\n    });\n\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: Date.now() - 1000,\n      queueId: \"queue-1\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    await setupConcurrency({\n      redis,\n      keyProducer,\n      env: { id: \"env-1\", currentConcurrency: 2, limit: 2 },\n    });\n\n    const result = await strategy.distributeFairQueuesFromParentQueue(\"parent-queue\", \"consumer-1\");\n    expect(result).toHaveLength(0);\n  });\n\n  redisTest(\n    \"should give extra concurrency when the env has reserve concurrency\",\n    async ({ redisOptions }) => {\n      const redis = createRedisClient(redisOptions);\n\n      const keyProducer = createKeyProducer(\"test\");\n      const strategy = new FairDequeuingStrategy({\n        tracer,\n        redis,\n        keys: keyProducer,\n        defaultEnvConcurrency: 2,\n        parentQueueLimit: 100,\n        seed: \"test-seed-3\",\n      });\n\n      await setupQueue({\n        redis,\n        keyProducer,\n        parentQueue: \"parent-queue\",\n        score: Date.now() - 1000,\n        queueId: \"queue-1\",\n        orgId: \"org-1\",\n        envId: \"env-1\",\n      });\n\n      await setupConcurrency({\n        redis,\n        keyProducer,\n        env: { id: \"env-1\", currentConcurrency: 2, limit: 2, reserveConcurrency: 1 },\n      });\n\n      const result = await strategy.distributeFairQueuesFromParentQueue(\n        \"parent-queue\",\n        \"consumer-1\"\n      );\n      expect(result).toHaveLength(1);\n      expect(result[0]).toEqual({\n        envId: \"env-1\",\n        queues: [\"org:org-1:env:env-1:queue:queue-1\"],\n      });\n    }\n  );\n\n  redisTest(\"should respect parentQueueLimit\", async ({ redisOptions }) => {\n    const redis = createRedisClient(redisOptions);\n\n    const keyProducer = createKeyProducer(\"test\");\n    const strategy = new FairDequeuingStrategy({\n      tracer,\n      redis,\n      keys: keyProducer,\n      defaultEnvConcurrency: 5,\n      parentQueueLimit: 2, // Only take 2 queues\n      seed: \"test-seed-6\",\n    });\n\n    const now = Date.now();\n\n    // Setup 3 queues but parentQueueLimit is 2\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: now - 3000,\n      queueId: \"queue-1\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: now - 2000,\n      queueId: \"queue-2\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    await setupQueue({\n      redis,\n      keyProducer,\n      parentQueue: \"parent-queue\",\n      score: now - 1000,\n      queueId: \"queue-3\",\n      orgId: \"org-1\",\n      envId: \"env-1\",\n    });\n\n    const result = await strategy.distributeFairQueuesFromParentQueue(\"parent-queue\", \"consumer-1\");\n\n    expect(result).toHaveLength(1);\n    const queue1 = keyProducer.queueKey(\"org-1\", \"env-1\", \"queue-1\");\n    const queue2 = keyProducer.queueKey(\"org-1\", \"env-1\", \"queue-2\");\n    expect(result[0]).toEqual({\n      envId: \"env-1\",\n      queues: [queue1, queue2],\n    });\n  });\n\n  redisTest(\n    \"should reuse snapshots across calls for the same consumer\",\n    async ({ redisOptions }) => {\n      const redis = createRedisClient(redisOptions);\n\n      const keyProducer = createKeyProducer(\"test\");\n      const strategy = new FairDequeuingStrategy({\n        tracer,\n        redis,\n        keys: keyProducer,\n        defaultEnvConcurrency: 5,\n        parentQueueLimit: 10,\n        seed: \"test-seed-reuse-1\",\n        reuseSnapshotCount: 1,\n      });\n\n      const now = Date.now();\n\n      await setupQueue({\n        redis,\n        keyProducer,\n        parentQueue: \"parent-queue\",\n        score: now - 3000,\n        queueId: \"queue-1\",\n        orgId: \"org-1\",\n        envId: \"env-1\",\n      });\n\n      await setupQueue({\n        redis,\n        keyProducer,\n        parentQueue: \"parent-queue\",\n        score: now - 2000,\n        queueId: \"queue-2\",\n        orgId: \"org-2\",\n        envId: \"env-2\",\n      });\n\n      await setupQueue({\n        redis,\n        keyProducer,\n        parentQueue: \"parent-queue\",\n        score: now - 1000,\n        queueId: \"queue-3\",\n        orgId: \"org-3\",\n        envId: \"env-3\",\n      });\n\n      const startDistribute1 = performance.now();\n\n      const envResult = await strategy.distributeFairQueuesFromParentQueue(\n        \"parent-queue\",\n        \"consumer-1\"\n      );\n      const result = flattenResults(envResult);\n\n      const distribute1Duration = performance.now() - startDistribute1;\n\n      console.log(\"First distribution took\", distribute1Duration, \"ms\");\n\n      expect(result).toHaveLength(3);\n      // Should only get the two oldest queues\n      const queue1 = keyProducer.queueKey(\"org-1\", \"env-1\", \"queue-1\");\n      const queue2 = keyProducer.queueKey(\"org-2\", \"env-2\", \"queue-2\");\n      const queue3 = keyProducer.queueKey(\"org-3\", \"env-3\", \"queue-3\");\n      expect(result).toEqual([queue2, queue1, queue3]);\n\n      const startDistribute2 = performance.now();\n\n      const result2 = await strategy.distributeFairQueuesFromParentQueue(\n        \"parent-queue\",\n        \"consumer-1\"\n      );\n\n      const tolerance = 0.15;\n      const withTolerance = (value: number) => value * (1 + tolerance);\n\n      const distribute2Duration = performance.now() - startDistribute2;\n\n      console.log(\"Second distribution took\", distribute2Duration, \"ms\");\n\n      // Make sure the second call is more than 2 times faster than the first\n      expect(distribute2Duration).toBeLessThan(withTolerance(distribute1Duration / 2));\n\n      const startDistribute3 = performance.now();\n\n      const result3 = await strategy.distributeFairQueuesFromParentQueue(\n        \"parent-queue\",\n        \"consumer-1\"\n      );\n\n      const distribute3Duration = performance.now() - startDistribute3;\n\n      console.log(\"Third distribution took\", distribute3Duration, \"ms\");\n\n      // Make sure the third call is more than 4 times the second\n      expect(withTolerance(distribute3Duration)).toBeGreaterThan(distribute2Duration * 4);\n    }\n  );\n\n  redisTest(\n    \"should fairly distribute queues across environments over time\",\n    async ({ redisOptions }) => {\n      const redis = createRedisClient(redisOptions);\n\n      const keyProducer = createKeyProducer(\"test\");\n      const strategy = new FairDequeuingStrategy({\n        tracer,\n        redis,\n        keys: keyProducer,\n        defaultEnvConcurrency: 5,\n        parentQueueLimit: 100,\n        seed: \"test-seed-5\",\n      });\n\n      const now = Date.now();\n\n      // Test configuration\n      const orgs = [\"org-1\", \"org-2\", \"org-3\"];\n      const envsPerOrg = 3; // Each org has 3 environments\n      const queuesPerEnv = 5; // Each env has 5 queues\n      const iterations = 1000;\n\n      // Setup queues\n      for (const orgId of orgs) {\n        for (let envNum = 1; envNum <= envsPerOrg; envNum++) {\n          const envId = `env-${orgId}-${envNum}`;\n\n          for (let queueNum = 1; queueNum <= queuesPerEnv; queueNum++) {\n            await setupQueue({\n              redis,\n              keyProducer,\n              parentQueue: \"parent-queue\",\n              // Vary the ages slightly\n              score: now - Math.random() * 10000,\n              queueId: `queue-${orgId}-${envId}-${queueNum}`,\n              orgId,\n              envId,\n            });\n          }\n\n          // Setup reasonable concurrency limits\n          await setupConcurrency({\n            redis,\n            keyProducer,\n            env: { id: envId, currentConcurrency: 1, limit: 5 },\n          });\n        }\n      }\n\n      // Track distribution statistics\n      type PositionStats = {\n        firstPosition: number; // Count of times this env/org was first\n        positionSums: number; // Sum of positions (for averaging)\n        appearances: number; // Total number of appearances\n      };\n\n      const envStats: Record<string, PositionStats> = {};\n      const orgStats: Record<string, PositionStats> = {};\n\n      // Initialize stats objects\n      for (const orgId of orgs) {\n        orgStats[orgId] = { firstPosition: 0, positionSums: 0, appearances: 0 };\n        for (let envNum = 1; envNum <= envsPerOrg; envNum++) {\n          const envId = `env-${orgId}-${envNum}`;\n          envStats[envId] = { firstPosition: 0, positionSums: 0, appearances: 0 };\n        }\n      }\n\n      // Run multiple iterations\n      for (let i = 0; i < iterations; i++) {\n        const envResult = await strategy.distributeFairQueuesFromParentQueue(\n          \"parent-queue\",\n          `consumer-${i % 3}` // Simulate 3 different consumers\n        );\n        const result = flattenResults(envResult);\n\n        // Track positions of queues\n        result.forEach((queueId, position) => {\n          const orgId = keyProducer.orgIdFromQueue(queueId);\n          const envId = keyProducer.envIdFromQueue(queueId);\n\n          // Update org stats\n          orgStats[orgId].appearances++;\n          orgStats[orgId].positionSums += position;\n          if (position === 0) orgStats[orgId].firstPosition++;\n\n          // Update env stats\n          envStats[envId].appearances++;\n          envStats[envId].positionSums += position;\n          if (position === 0) envStats[envId].firstPosition++;\n        });\n      }\n\n      // Calculate and log statistics\n      console.log(\"\\nOrganization Statistics:\");\n      for (const [orgId, stats] of Object.entries(orgStats)) {\n        const avgPosition = stats.positionSums / stats.appearances;\n        const firstPositionPercentage = (stats.firstPosition / iterations) * 100;\n        console.log(`${orgId}:\n      First Position: ${firstPositionPercentage.toFixed(2)}%\n      Average Position: ${avgPosition.toFixed(2)}\n      Total Appearances: ${stats.appearances}`);\n      }\n\n      console.log(\"\\nEnvironment Statistics:\");\n      for (const [envId, stats] of Object.entries(envStats)) {\n        const avgPosition = stats.positionSums / stats.appearances;\n        const firstPositionPercentage = (stats.firstPosition / iterations) * 100;\n        console.log(`${envId}:\n      First Position: ${firstPositionPercentage.toFixed(2)}%\n      Average Position: ${avgPosition.toFixed(2)}\n      Total Appearances: ${stats.appearances}`);\n      }\n\n      // Verify fairness of first position distribution\n      const expectedFirstPositionPercentage = 100 / orgs.length;\n      const firstPositionStdDevOrgs = calculateStandardDeviation(\n        Object.values(orgStats).map((stats) => (stats.firstPosition / iterations) * 100)\n      );\n\n      const expectedEnvFirstPositionPercentage = 100 / (orgs.length * envsPerOrg);\n      const firstPositionStdDevEnvs = calculateStandardDeviation(\n        Object.values(envStats).map((stats) => (stats.firstPosition / iterations) * 100)\n      );\n\n      // Assert reasonable fairness for first position\n      expect(firstPositionStdDevOrgs).toBeLessThan(5); // Allow 5% standard deviation for orgs\n      expect(firstPositionStdDevEnvs).toBeLessThan(5); // Allow 5% standard deviation for envs\n\n      // Verify that each org and env gets a fair chance at first position\n      for (const [orgId, stats] of Object.entries(orgStats)) {\n        const firstPositionPercentage = (stats.firstPosition / iterations) * 100;\n        expect(firstPositionPercentage).to",
            "cost": 0.18147000000000002
        },
        "triggerdotdev__trigger.dev.d1c3bfb9/packages/core/src/v3/utils/ioSerialization.ts": {
            "output": "import { Attributes, Span } from \"@opentelemetry/api\";\nimport { z } from \"zod\";\nimport { ApiClient } from \"../apiClient/index.js\";\nimport { apiClientManager } from \"../apiClientManager-api.js\";\nimport {\n  OFFLOAD_IO_PACKET_LENGTH_LIMIT,\n  OTEL_SPAN_ATTRIBUTE_COUNT_LIMIT,\n  imposeAttributeLimits,\n} from \"../limits.js\";\nimport type { RetryOptions } from \"../schemas/index.js\";\nimport { SemanticInternalAttributes } from \"../semanticInternalAttributes.js\";\nimport { TriggerTracer } from \"../tracer.js\";\nimport { zodfetch } from \"../zodfetch.js\";\nimport { flattenAttributes } from \"./flattenAttributes.js\";\nimport { JSONHeroPath } from \"@jsonhero/path\";\n\nexport type IOPacket = {\n  data?: string | undefined;\n  dataType: string;\n};\n\nexport type ParsePacketOptions = {\n  filteredKeys?: string[];\n};\n\nexport async function parsePacket(value: IOPacket, options?: ParsePacketOptions): Promise<any> {\n  if (!value.data) {\n    return undefined;\n  }\n\n  switch (value.dataType) {\n    case \"application/json\":\n      return JSON.parse(value.data, makeSafeReviver(options));\n    case \"application/super+json\":\n      const { parse } = await loadSuperJSON();\n\n      return parse(value.data);\n    case \"text/plain\":\n      return value.data;\n    case \"application/store\":\n      throw new Error(\n        `Cannot parse an application/store packet (${value.data}). Needs to be imported first.`\n      );\n    default:\n      return value.data;\n  }\n}\n\nexport async function parsePacketAsJson(\n  value: IOPacket,\n  options?: ParsePacketOptions\n): Promise<any> {\n  if (!value.data) {\n    return undefined;\n  }\n\n  switch (value.dataType) {\n    case \"application/json\":\n      return JSON.parse(value.data, makeSafeReviver(options));\n    case \"application/super+json\":\n      const { parse, serialize } = await loadSuperJSON();\n\n      const superJsonResult = parse(value.data);\n\n      const { json } = serialize(superJsonResult);\n\n      return json;\n    case \"text/plain\":\n      return value.data;\n    case \"application/store\":\n      throw new Error(\n        `Cannot parse an application/store packet (${value.data}). Needs to be imported first.`\n      );\n    default:\n      return value.data;\n  }\n}\n\nexport async function conditionallyImportAndParsePacket(\n  value: IOPacket,\n  client?: ApiClient\n): Promise<any> {\n  const importedPacket = await conditionallyImportPacket(value, undefined, client);\n\n  return await parsePacket(importedPacket);\n}\n\nexport async function stringifyIO(value: any): Promise<IOPacket> {\n  if (value === undefined) {\n    return { dataType: \"application/json\" };\n  }\n\n  if (typeof value === \"string\") {\n    return { data: value, dataType: \"text/plain\" };\n  }\n\n  try {\n    const { stringify } = await loadSuperJSON();\n    const data = stringify(value);\n\n    return { data, dataType: \"application/super+json\" };\n  } catch {\n    return { data: value, dataType: \"application/json\" };\n  }\n}\n\nexport async function conditionallyExportPacket(\n  packet: IOPacket,\n  pathPrefix: string,\n  tracer?: TriggerTracer\n): Promise<IOPacket> {\n  if (apiClientManager.client) {\n    const { needsOffloading, size } = packetRequiresOffloading(packet);\n\n    if (needsOffloading) {\n      if (!tracer) {\n        return await exportPacket(packet, pathPrefix);\n      } else {\n        const result = await tracer.startActiveSpan(\n          \"store.uploadOutput\",\n          async (span) => {\n            return await exportPacket(packet, pathPrefix);\n          },\n          {\n            attributes: {\n              byteLength: size,\n              [SemanticInternalAttributes.STYLE_ICON]: \"cloud-upload\",\n            },\n          }\n        );\n\n        return result ?? packet;\n      }\n    }\n  }\n\n  return packet;\n}\n\nexport function packetRequiresOffloading(\n  packet: IOPacket,\n  lengthLimit?: number\n): {\n  needsOffloading: boolean;\n  size: number;\n} {\n  if (!packet.data) {\n    return {\n      needsOffloading: false,\n      size: 0,\n    };\n  }\n\n  const byteSize = Buffer.byteLength(packet.data, \"utf8\");\n\n  return {\n    needsOffloading: byteSize >= (lengthLimit ?? OFFLOAD_IO_PACKET_LENGTH_LIMIT),\n    size: byteSize,\n  };\n}\n\nconst ioRetryOptions = {\n  minTimeoutInMs: 500,\n  maxTimeoutInMs: 5000,\n  maxAttempts: 5,\n  factor: 2,\n  randomize: true,\n} satisfies RetryOptions;\n\nasync function exportPacket(packet: IOPacket, pathPrefix: string): Promise<IOPacket> {\n  // Offload the output\n  const filename = `${pathPrefix}.${getPacketExtension(packet.dataType)}`;\n\n  const presignedResponse = await apiClientManager.client!.createUploadPayloadUrl(filename);\n\n  const uploadResponse = await zodfetch(\n    z.any(),\n    presignedResponse.presignedUrl,\n    {\n      method: \"PUT\",\n      headers: {\n        \"Content-Type\": packet.dataType,\n      },\n      body: packet.data,\n    },\n    {\n      retry: ioRetryOptions,\n    }\n  ).asResponse();\n\n  if (!uploadResponse.ok) {\n    throw new Error(\n      `Failed to upload output to ${presignedResponse.presignedUrl}: ${uploadResponse.statusText}`\n    );\n  }\n\n  return {\n    data: filename,\n    dataType: \"application/store\",\n  };\n}\n\nexport async function conditionallyImportPacket(\n  packet: IOPacket,\n  tracer?: TriggerTracer,\n  client?: ApiClient\n): Promise<IOPacket> {\n  if (packet.dataType !== \"application/store\") {\n    return packet;\n  }\n\n  if (!tracer) {\n    return await importPacket(packet, undefined, client);\n  } else {\n    const result = await tracer.startActiveSpan(\n      \"store.downloadPayload\",\n      async (span) => {\n        return await importPacket(packet, span, client);\n      },\n      {\n        attributes: {\n          [SemanticInternalAttributes.STYLE_ICON]: \"cloud-download\",\n        },\n      }\n    );\n\n    return result ?? packet;\n  }\n}\n\nexport async function resolvePresignedPacketUrl(\n  url: string,\n  tracer?: TriggerTracer\n): Promise<any | undefined> {\n  try {\n    const response = await fetch(url);\n\n    if (!response.ok) {\n      return;\n    }\n\n    const data = await response.text();\n    const dataType = response.headers.get(\"content-type\") ?? \"application/json\";\n\n    const packet = {\n      data,\n      dataType,\n    };\n\n    return await parsePacket(packet);\n  } catch (error) {\n    return;\n  }\n}\n\nasync function importPacket(packet: IOPacket, span?: Span, client?: ApiClient): Promise<IOPacket> {\n  if (!packet.data) {\n    return packet;\n  }\n\n  const $client = client ?? apiClientManager.client;\n\n  if (!$client) {\n    return packet;\n  }\n\n  const presignedResponse = await $client.getPayloadUrl(packet.data);\n\n  const response = await zodfetch(z.any(), presignedResponse.presignedUrl, undefined, {\n    retry: ioRetryOptions,\n  }).asResponse();\n\n  if (!response.ok) {\n    throw new Error(\n      `Failed to import packet ${presignedResponse.presignedUrl}: ${response.statusText}`\n    );\n  }\n\n  const data = await response.text();\n\n  span?.setAttribute(\"size\", Buffer.byteLength(data, \"utf8\"));\n\n  return {\n    data,\n    dataType: response.headers.get(\"content-type\") ?? \"application/json\",\n  };\n}\n\nexport async function createPacketAttributes(\n  packet: IOPacket,\n  dataKey: string,\n  dataTypeKey: string\n): Promise<Attributes | undefined> {\n  if (!packet.data) {\n    return;\n  }\n\n  switch (packet.dataType) {\n    case \"application/json\":\n      return {\n        ...flattenAttributes(packet, dataKey),\n        [dataTypeKey]: packet.dataType,\n      };\n    case \"application/super+json\":\n      const { parse } = await loadSuperJSON();\n\n      if (typeof packet.data === \"undefined\" || packet.data === null) {\n        return;\n      }\n\n      try {\n        const parsed = parse(packet.data) as any;\n        const jsonified = JSON.parse(JSON.stringify(parsed, makeSafeReplacer()));\n\n        const result = {\n          ...flattenAttributes(jsonified, dataKey),\n          [dataTypeKey]: \"application/json\",\n        };\n\n        return result;\n      } catch (e) {\n        return;\n      }\n\n    case \"application/store\":\n      return {\n        [dataKey]: packet.data,\n        [dataTypeKey]: packet.dataType,\n      };\n    case \"text/plain\":\n      return {\n        [dataKey]: packet.data,\n        [dataTypeKey]: packet.dataType,\n      };\n    default:\n      return;\n  }\n}\n\nexport async function createPacketAttributesAsJson(\n  data: any,\n  dataType: string\n): Promise<Attributes> {\n  if (\n    typeof data === \"string\" ||\n    typeof data === \"number\" ||\n    typeof data === \"boolean\" ||\n    data === null ||\n    data === undefined\n  ) {\n    return data;\n  }\n\n  switch (dataType) {\n    case \"application/json\": {\n      return imposeAttributeLimits(\n        flattenAttributes(data, undefined, OTEL_SPAN_ATTRIBUTE_COUNT_LIMIT)\n      );\n    }\n    case \"application/super+json\": {\n      const { deserialize } = await loadSuperJSON();\n\n      const deserialized = deserialize(data) as any;\n      const jsonify = safeJsonParse(JSON.stringify(deserialized, makeSafeReplacer()));\n\n      return imposeAttributeLimits(\n        flattenAttributes(jsonify, undefined, OTEL_SPAN_ATTRIBUTE_COUNT_LIMIT)\n      );\n    }\n    case \"application/store\": {\n      return data;\n    }\n    default: {\n      return {};\n    }\n  }\n}\n\nexport async function prettyPrintPacket(\n  rawData: any,\n  dataType?: string,\n  options?: ReplacerOptions\n): Promise<string> {\n  if (rawData === undefined) {\n    return \"\";\n  }\n\n  if (dataType === \"application/super+json\") {\n    if (typeof rawData === \"string\") {\n      rawData = safeJsonParse(rawData);\n    }\n    const { deserialize } = await loadSuperJSON();\n\n    return await prettyPrintPacket(deserialize(rawData), \"application/json\");\n  }\n\n  if (dataType === \"application/json\") {\n    if (typeof rawData === \"string\") {\n      rawData = safeJsonParse(rawData);\n    }\n    return JSON.stringify(rawData, makeSafeReplacer(options), 2);\n  }\n\n  if (typeof rawData === \"string\") {\n    return rawData;\n  }\n\n  return JSON.stringify(rawData, makeSafeReplacer(options), 2);\n}\n\ninterface ReplacerOptions {\n  filteredKeys?: string[];\n}\n\nfunction makeSafeReplacer(options?: ReplacerOptions) {\n  const seen = new WeakSet<any>();\n\n  return function replacer(key: string, value: any) {\n    if (typeof value === \"object\" && value !== null) {\n      if (seen.has(value)) {\n        return \"[Circular]\";\n      }\n      seen.add(value);\n    }\n    // Check if the key should be filtered out\n    if (options?.filteredKeys?.includes(key)) {\n      return undefined;\n    }\n\n    // If it is a BigInt\n    if (typeof value === \"bigint\") {\n      return value.toString();\n    }\n\n    // if it is a Regex\n    if (value instanceof RegExp) {\n      return value.toString();\n    }\n\n    // if it is a Set\n    if (value instanceof Set) {\n      return Array.from(value);\n    }\n\n    // if it is a Map, convert it to an object\n    if (value instanceof Map) {\n      const obj: Record<string, any> = {};\n      value.forEach((v, k) => {\n        obj[k] = v;\n      });\n      return obj;\n    }\n\n    return value;\n  };\n}\n\nfunction makeSafeReviver(options?: ReplacerOptions) {\n  if (!options) {\n    return undefined;\n  }\n\n  return function reviver(key: string, value: any) {\n    // Check if the key should be filtered out\n    if (options?.filteredKeys?.includes(key)) {\n      return undefined;\n    }\n\n    return value;\n  };\n}\n\nfunction getPacketExtension(outputType: string): string {\n  switch (outputType) {\n    case \"application/json\":\n      return \"json\";\n    case \"application/super+json\":\n      return \"json\";\n    case \"text/plain\":\n      return \"txt\";\n    default:\n      return \"txt\";\n  }\n}\n\nasync function loadSuperJSON() {\n  const superjson = await import(\"superjson\");\n\n  superjson.registerCustom<Buffer, number[]>(\n    {\n      isApplicable: (v): v is Buffer => typeof Buffer === \"function\" && Buffer.isBuffer(v),\n      serialize: (v) => [...v],\n      deserialize: (v) => Buffer.from(v),\n    },\n    \"buffer\"\n  );\n\n  return superjson;\n}\n\nfunction safeJsonParse(value: string): any {\n  try {\n    return JSON.parse(value);\n  } catch {\n    return;\n  }\n}\n\n/**\n * Replaces the data in a SuperJSON-serialized string with new payload data while preserving\n * the original type metadata (Dates, BigInts, Sets, Maps, etc.).\n *\n * It is primarily useful for our run replay functionality where we want to preserve the original\n * type metadata for the new payload.\n *\n * Note that `undefined` type metadata is ignored when the corresponding field is overriden in the\n * new payload, i.e., fields which were previously undefined in the original payload are restored into\n * the primitive type they have in the new payload, instead of `undefined`.\n * This is a workaround for https://github.com/triggerdotdev/trigger.dev/issues/1968.\n *\n * @param original - A SuperJSON-serialized string containing the original data with type metadata\n * @param newPayload - A JSON string containing the new data to replace the original payload\n * @returns The deserialized object with new data but original type metadata preserved\n *\n * @throws {Error} If the newPayload is not valid JSON\n */\nexport async function replaceSuperJsonPayload(original: string, newPayload: string) {\n  const superjson = await loadSuperJSON();\n  const originalObject = superjson.parse(original);\n  const newPayloadObject = JSON.parse(newPayload);\n  const { meta } = superjson.serialize(originalObject);\n\n  if (meta?.values) {\n    const originalUndefinedKeys = Object.entries(meta.values)\n      .filter(([, value]) => Array.isArray(value) && value.at(0) === \"undefined\")\n      .map(([key]) => key);\n\n    const over",
            "output_extracted": "import { Attributes, Span } from \"@opentelemetry/api\";\nimport { z } from \"zod\";\nimport { ApiClient } from \"../apiClient/index.js\";\nimport { apiClientManager } from \"../apiClientManager-api.js\";\nimport {\n  OFFLOAD_IO_PACKET_LENGTH_LIMIT,\n  OTEL_SPAN_ATTRIBUTE_COUNT_LIMIT,\n  imposeAttributeLimits,\n} from \"../limits.js\";\nimport type { RetryOptions } from \"../schemas/index.js\";\nimport { SemanticInternalAttributes } from \"../semanticInternalAttributes.js\";\nimport { TriggerTracer } from \"../tracer.js\";\nimport { zodfetch } from \"../zodfetch.js\";\nimport { flattenAttributes } from \"./flattenAttributes.js\";\nimport { JSONHeroPath } from \"@jsonhero/path\";\n\nexport type IOPacket = {\n  data?: string | undefined;\n  dataType: string;\n};\n\nexport type ParsePacketOptions = {\n  filteredKeys?: string[];\n};\n\nexport async function parsePacket(value: IOPacket, options?: ParsePacketOptions): Promise<any> {\n  if (!value.data) {\n    return undefined;\n  }\n\n  switch (value.dataType) {\n    case \"application/json\":\n      return JSON.parse(value.data, makeSafeReviver(options));\n    case \"application/super+json\":\n      const { parse } = await loadSuperJSON();\n\n      return parse(value.data);\n    case \"text/plain\":\n      return value.data;\n    case \"application/store\":\n      throw new Error(\n        `Cannot parse an application/store packet (${value.data}). Needs to be imported first.`\n      );\n    default:\n      return value.data;\n  }\n}\n\nexport async function parsePacketAsJson(\n  value: IOPacket,\n  options?: ParsePacketOptions\n): Promise<any> {\n  if (!value.data) {\n    return undefined;\n  }\n\n  switch (value.dataType) {\n    case \"application/json\":\n      return JSON.parse(value.data, makeSafeReviver(options));\n    case \"application/super+json\":\n      const { parse, serialize } = await loadSuperJSON();\n\n      const superJsonResult = parse(value.data);\n\n      const { json } = serialize(superJsonResult);\n\n      return json;\n    case \"text/plain\":\n      return value.data;\n    case \"application/store\":\n      throw new Error(\n        `Cannot parse an application/store packet (${value.data}). Needs to be imported first.`\n      );\n    default:\n      return value.data;\n  }\n}\n\nexport async function conditionallyImportAndParsePacket(\n  value: IOPacket,\n  client?: ApiClient\n): Promise<any> {\n  const importedPacket = await conditionallyImportPacket(value, undefined, client);\n\n  return await parsePacket(importedPacket);\n}\n\nexport async function stringifyIO(value: any): Promise<IOPacket> {\n  if (value === undefined) {\n    return { dataType: \"application/json\" };\n  }\n\n  if (typeof value === \"string\") {\n    return { data: value, dataType: \"text/plain\" };\n  }\n\n  try {\n    const { stringify } = await loadSuperJSON();\n    const data = stringify(value);\n\n    return { data, dataType: \"application/super+json\" };\n  } catch {\n    return { data: value, dataType: \"application/json\" };\n  }\n}\n\nexport async function conditionallyExportPacket(\n  packet: IOPacket,\n  pathPrefix: string,\n  tracer?: TriggerTracer\n): Promise<IOPacket> {\n  if (apiClientManager.client) {\n    const { needsOffloading, size } = packetRequiresOffloading(packet);\n\n    if (needsOffloading) {\n      if (!tracer) {\n        return await exportPacket(packet, pathPrefix);\n      } else {\n        const result = await tracer.startActiveSpan(\n          \"store.uploadOutput\",\n          async (span) => {\n            return await exportPacket(packet, pathPrefix);\n          },\n          {\n            attributes: {\n              byteLength: size,\n              [SemanticInternalAttributes.STYLE_ICON]: \"cloud-upload\",\n            },\n          }\n        );\n\n        return result ?? packet;\n      }\n    }\n  }\n\n  return packet;\n}\n\nexport function packetRequiresOffloading(\n  packet: IOPacket,\n  lengthLimit?: number\n): {\n  needsOffloading: boolean;\n  size: number;\n} {\n  if (!packet.data) {\n    return {\n      needsOffloading: false,\n      size: 0,\n    };\n  }\n\n  const byteSize = Buffer.byteLength(packet.data, \"utf8\");\n\n  return {\n    needsOffloading: byteSize >= (lengthLimit ?? OFFLOAD_IO_PACKET_LENGTH_LIMIT),\n    size: byteSize,\n  };\n}\n\nconst ioRetryOptions = {\n  minTimeoutInMs: 500,\n  maxTimeoutInMs: 5000,\n  maxAttempts: 5,\n  factor: 2,\n  randomize: true,\n} satisfies RetryOptions;\n\nasync function exportPacket(packet: IOPacket, pathPrefix: string): Promise<IOPacket> {\n  // Offload the output\n  const filename = `${pathPrefix}.${getPacketExtension(packet.dataType)}`;\n\n  const presignedResponse = await apiClientManager.client!.createUploadPayloadUrl(filename);\n\n  const uploadResponse = await zodfetch(\n    z.any(),\n    presignedResponse.presignedUrl,\n    {\n      method: \"PUT\",\n      headers: {\n        \"Content-Type\": packet.dataType,\n      },\n      body: packet.data,\n    },\n    {\n      retry: ioRetryOptions,\n    }\n  ).asResponse();\n\n  if (!uploadResponse.ok) {\n    throw new Error(\n      `Failed to upload output to ${presignedResponse.presignedUrl}: ${uploadResponse.statusText}`\n    );\n  }\n\n  return {\n    data: filename,\n    dataType: \"application/store\",\n  };\n}\n\nexport async function conditionallyImportPacket(\n  packet: IOPacket,\n  tracer?: TriggerTracer,\n  client?: ApiClient\n): Promise<IOPacket> {\n  if (packet.dataType !== \"application/store\") {\n    return packet;\n  }\n\n  if (!tracer) {\n    return await importPacket(packet, undefined, client);\n  } else {\n    const result = await tracer.startActiveSpan(\n      \"store.downloadPayload\",\n      async (span) => {\n        return await importPacket(packet, span, client);\n      },\n      {\n        attributes: {\n          [SemanticInternalAttributes.STYLE_ICON]: \"cloud-download\",\n        },\n      }\n    );\n\n    return result ?? packet;\n  }\n}\n\nexport async function resolvePresignedPacketUrl(\n  url: string,\n  tracer?: TriggerTracer\n): Promise<any | undefined> {\n  try {\n    const response = await fetch(url);\n\n    if (!response.ok) {\n      return;\n    }\n\n    const data = await response.text();\n    const dataType = response.headers.get(\"content-type\") ?? \"application/json\";\n\n    const packet = {\n      data,\n      dataType,\n    };\n\n    return await parsePacket(packet);\n  } catch (error) {\n    return;\n  }\n}\n\nasync function importPacket(packet: IOPacket, span?: Span, client?: ApiClient): Promise<IOPacket> {\n  if (!packet.data) {\n    return packet;\n  }\n\n  const $client = client ?? apiClientManager.client;\n\n  if (!$client) {\n    return packet;\n  }\n\n  const presignedResponse = await $client.getPayloadUrl(packet.data);\n\n  const response = await zodfetch(z.any(), presignedResponse.presignedUrl, undefined, {\n    retry: ioRetryOptions,\n  }).asResponse();\n\n  if (!response.ok) {\n    throw new Error(\n      `Failed to import packet ${presignedResponse.presignedUrl}: ${response.statusText}`\n    );\n  }\n\n  const data = await response.text();\n\n  span?.setAttribute(\"size\", Buffer.byteLength(data, \"utf8\"));\n\n  return {\n    data,\n    dataType: response.headers.get(\"content-type\") ?? \"application/json\",\n  };\n}\n\nexport async function createPacketAttributes(\n  packet: IOPacket,\n  dataKey: string,\n  dataTypeKey: string\n): Promise<Attributes | undefined> {\n  if (!packet.data) {\n    return;\n  }\n\n  switch (packet.dataType) {\n    case \"application/json\":\n      return {\n        ...flattenAttributes(packet, dataKey),\n        [dataTypeKey]: packet.dataType,\n      };\n    case \"application/super+json\":\n      const { parse } = await loadSuperJSON();\n\n      if (typeof packet.data === \"undefined\" || packet.data === null) {\n        return;\n      }\n\n      try {\n        const parsed = parse(packet.data) as any;\n        const jsonified = JSON.parse(JSON.stringify(parsed, makeSafeReplacer()));\n\n        const result = {\n          ...flattenAttributes(jsonified, dataKey),\n          [dataTypeKey]: \"application/json\",\n        };\n\n        return result;\n      } catch (e) {\n        return;\n      }\n\n    case \"application/store\":\n      return {\n        [dataKey]: packet.data,\n        [dataTypeKey]: packet.dataType,\n      };\n    case \"text/plain\":\n      return {\n        [dataKey]: packet.data,\n        [dataTypeKey]: packet.dataType,\n      };\n    default:\n      return;\n  }\n}\n\nexport async function createPacketAttributesAsJson(\n  data: any,\n  dataType: string\n): Promise<Attributes> {\n  if (\n    typeof data === \"string\" ||\n    typeof data === \"number\" ||\n    typeof data === \"boolean\" ||\n    data === null ||\n    data === undefined\n  ) {\n    return data;\n  }\n\n  switch (dataType) {\n    case \"application/json\": {\n      return imposeAttributeLimits(\n        flattenAttributes(data, undefined, OTEL_SPAN_ATTRIBUTE_COUNT_LIMIT)\n      );\n    }\n    case \"application/super+json\": {\n      const { deserialize } = await loadSuperJSON();\n\n      const deserialized = deserialize(data) as any;\n      const jsonify = safeJsonParse(JSON.stringify(deserialized, makeSafeReplacer()));\n\n      return imposeAttributeLimits(\n        flattenAttributes(jsonify, undefined, OTEL_SPAN_ATTRIBUTE_COUNT_LIMIT)\n      );\n    }\n    case \"application/store\": {\n      return data;\n    }\n    default: {\n      return {};\n    }\n  }\n}\n\nexport async function prettyPrintPacket(\n  rawData: any,\n  dataType?: string,\n  options?: ReplacerOptions\n): Promise<string> {\n  if (rawData === undefined) {\n    return \"\";\n  }\n\n  if (dataType === \"application/super+json\") {\n    if (typeof rawData === \"string\") {\n      rawData = safeJsonParse(rawData);\n    }\n    const { deserialize } = await loadSuperJSON();\n\n    return await prettyPrintPacket(deserialize(rawData), \"application/json\");\n  }\n\n  if (dataType === \"application/json\") {\n    if (typeof rawData === \"string\") {\n      rawData = safeJsonParse(rawData);\n    }\n    return JSON.stringify(rawData, makeSafeReplacer(options), 2);\n  }\n\n  if (typeof rawData === \"string\") {\n    return rawData;\n  }\n\n  return JSON.stringify(rawData, makeSafeReplacer(options), 2);\n}\n\ninterface ReplacerOptions {\n  filteredKeys?: string[];\n}\n\nfunction makeSafeReplacer(options?: ReplacerOptions) {\n  const seen = new WeakSet<any>();\n\n  return function replacer(key: string, value: any) {\n    if (typeof value === \"object\" && value !== null) {\n      if (seen.has(value)) {\n        return \"[Circular]\";\n      }\n      seen.add(value);\n    }\n    // Check if the key should be filtered out\n    if (options?.filteredKeys?.includes(key)) {\n      return undefined;\n    }\n\n    // If it is a BigInt\n    if (typeof value === \"bigint\") {\n      return value.toString();\n    }\n\n    // if it is a Regex\n    if (value instanceof RegExp) {\n      return value.toString();\n    }\n\n    // if it is a Set\n    if (value instanceof Set) {\n      return Array.from(value);\n    }\n\n    // if it is a Map, convert it to an object\n    if (value instanceof Map) {\n      const obj: Record<string, any> = {};\n      value.forEach((v, k) => {\n        obj[k] = v;\n      });\n      return obj;\n    }\n\n    return value;\n  };\n}\n\nfunction makeSafeReviver(options?: ReplacerOptions) {\n  if (!options) {\n    return undefined;\n  }\n\n  return function reviver(key: string, value: any) {\n    // Check if the key should be filtered out\n    if (options?.filteredKeys?.includes(key)) {\n      return undefined;\n    }\n\n    return value;\n  };\n}\n\nfunction getPacketExtension(outputType: string): string {\n  switch (outputType) {\n    case \"application/json\":\n      return \"json\";\n    case \"application/super+json\":\n      return \"json\";\n    case \"text/plain\":\n      return \"txt\";\n    default:\n      return \"txt\";\n  }\n}\n\nasync function loadSuperJSON() {\n  const superjson = await import(\"superjson\");\n\n  superjson.registerCustom<Buffer, number[]>(\n    {\n      isApplicable: (v): v is Buffer => typeof Buffer === \"function\" && Buffer.isBuffer(v),\n      serialize: (v) => [...v],\n      deserialize: (v) => Buffer.from(v),\n    },\n    \"buffer\"\n  );\n\n  return superjson;\n}\n\nfunction safeJsonParse(value: string): any {\n  try {\n    return JSON.parse(value);\n  } catch {\n    return;\n  }\n}\n\n/**\n * Replaces the data in a SuperJSON-serialized string with new payload data while preserving\n * the original type metadata (Dates, BigInts, Sets, Maps, etc.).\n *\n * It is primarily useful for our run replay functionality where we want to preserve the original\n * type metadata for the new payload.\n *\n * Note that `undefined` type metadata is ignored when the corresponding field is overriden in the\n * new payload, i.e., fields which were previously undefined in the original payload are restored into\n * the primitive type they have in the new payload, instead of `undefined`.\n * This is a workaround for https://github.com/triggerdotdev/trigger.dev/issues/1968.\n *\n * @param original - A SuperJSON-serialized string containing the original data with type metadata\n * @param newPayload - A JSON string containing the new data to replace the original payload\n * @returns The deserialized object with new data but original type metadata preserved\n *\n * @throws {Error} If the newPayload is not valid JSON\n */\nexport async function replaceSuperJsonPayload(original: string, newPayload: string) {\n  const superjson = await loadSuperJSON();\n  const originalObject = superjson.parse(original);\n  const newPayloadObject = JSON.parse(newPayload);\n  const { meta } = superjson.serialize(originalObject);\n\n  if (meta?.values) {\n    const originalUndefinedKeys = Object.entries(meta.values)\n      .filter(([, value]) => Array.isArray(value) && value.at(0) === \"undefined\")\n      .map(([key]) => key);\n\n    const over",
            "cost": 0.142105
        },
        "triggerdotdev__trigger.dev.d1c3bfb9/packages/core/test/ioSerialization.test.ts": {
            "output": "import { replaceSuperJsonPayload } from \"../src/v3/utils/ioSerialization.js\";\n\ndescribe(\"ioSerialization\", () => {\n  describe(\"replaceSuperJsonPayload\", () => {\n    it(\"should replace simple JSON payload while preserving SuperJSON metadata\", async () => {\n      const originalData = {\n        name: \"John\",\n        age: 30,\n        date: new Date(\"2023-01-01\"),\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        name: \"Jane\",\n        surname: \"Doe\",\n        age: 25,\n        date: \"2023-02-01T00:00:00.000Z\",\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.name).toBe(\"Jane\");\n      expect(result.surname).toBe(\"Doe\");\n      expect(result.age).toBe(25);\n      expect(result.date).toBeInstanceOf(Date);\n      expect(result.date.toISOString()).toBe(\"2023-02-01T00:00:00.000Z\");\n    });\n\n    // related to issue https://github.com/triggerdotdev/trigger.dev/issues/1968\n    it(\"should ignore original undefined type metadata for overriden fields\", async () => {\n      const originalData = {\n        name: \"John\",\n        age: 30,\n        date: new Date(\"2023-01-01\"),\n        country: undefined,\n        settings: {\n          theme: undefined,\n        },\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        name: \"Jane\",\n        surname: \"Doe\",\n        age: 25,\n        date: \"2023-02-01T00:00:00.000Z\",\n        country: \"US\",\n        settings: {\n          theme: \"dark\",\n        },\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.name).toBe(\"Jane\");\n      expect(result.surname).toBe(\"Doe\");\n      expect(result.country).toBe(\"US\");\n      expect(result.settings.theme).toBe(\"dark\");\n      expect(result.age).toBe(25);\n      expect(result.date).toBeInstanceOf(Date);\n      expect(result.date.toISOString()).toBe(\"2023-02-01T00:00:00.000Z\");\n    });\n\n    it(\"should preserve BigInt type metadata\", async () => {\n      const originalData = {\n        id: BigInt(123456789),\n        count: 42,\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        id: \"987654321\",\n        count: 100,\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.id).toBe(BigInt(987654321));\n      expect(typeof result.id).toBe(\"bigint\");\n      expect(result.count).toBe(100);\n    });\n\n    it(\"should preserve nested type metadata\", async () => {\n      const originalData = {\n        user: {\n          id: BigInt(123),\n          createdAt: new Date(\"2023-01-01\"),\n          settings: {\n            theme: \"dark\",\n            updatedAt: new Date(\"2023-01-01\"),\n          },\n        },\n        metadata: {\n          version: 1,\n        },\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        user: {\n          id: \"456\",\n          createdAt: \"2023-06-01T00:00:00.000Z\",\n          settings: {\n            theme: \"light\",\n            updatedAt: \"2023-06-01T00:00:00.000Z\",\n          },\n        },\n        metadata: {\n          version: 2,\n        },\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.user.id).toBe(BigInt(456));\n      expect(result.user.createdAt).toBeInstanceOf(Date);\n      expect(result.user.createdAt.toISOString()).toBe(\"2023-06-01T00:00:00.000Z\");\n      expect(result.user.settings.theme).toBe(\"light\");\n      expect(result.user.settings.updatedAt).toBeInstanceOf(Date);\n      expect(result.user.settings.updatedAt.toISOString()).toBe(\"2023-06-01T00:00:00.000Z\");\n      expect(result.metadata.version).toBe(2);\n    });\n\n    it(\"should preserve Set type metadata\", async () => {\n      const originalData = {\n        tags: new Set([\"tag1\", \"tag2\"]),\n        name: \"test\",\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        tags: [\"tag3\", \"tag4\", \"tag5\"],\n        name: \"updated\",\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.tags).toBeInstanceOf(Set);\n      expect(Array.from(result.tags)).toEqual([\"tag3\", \"tag4\", \"tag5\"]);\n      expect(result.name).toBe(\"updated\");\n    });\n\n    it(\"should preserve Map type metadata\", async () => {\n      const originalData = {\n        mapping: new Map([\n          [\"key1\", \"value1\"],\n          [\"key2\", \"value2\"],\n        ]),\n        name: \"test\",\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        mapping: [\n          [\"key3\", \"value3\"],\n          [\"key4\", \"value4\"],\n        ],\n        name: \"updated\",\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.mapping).toBeInstanceOf(Map);\n      expect(result.mapping.get(\"key3\")).toBe(\"value3\");\n      expect(result.mapping.get(\"key4\")).toBe(\"value4\");\n      expect(result.name).toBe(\"updated\");\n    });\n\n    it(\"should throw error for invalid JSON payload\", async () => {\n      const originalData = { name: \"test\" };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n      const invalidPayload = \"{ invalid json }\";\n\n      await expect(replaceSuperJsonPayload(originalSerialized, invalidPayload)).rejects.toThrow();\n    });\n  });\n});",
            "output_extracted": "import { replaceSuperJsonPayload } from \"../src/v3/utils/ioSerialization.js\";\n\ndescribe(\"ioSerialization\", () => {\n  describe(\"replaceSuperJsonPayload\", () => {\n    it(\"should replace simple JSON payload while preserving SuperJSON metadata\", async () => {\n      const originalData = {\n        name: \"John\",\n        age: 30,\n        date: new Date(\"2023-01-01\"),\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        name: \"Jane\",\n        surname: \"Doe\",\n        age: 25,\n        date: \"2023-02-01T00:00:00.000Z\",\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.name).toBe(\"Jane\");\n      expect(result.surname).toBe(\"Doe\");\n      expect(result.age).toBe(25);\n      expect(result.date).toBeInstanceOf(Date);\n      expect(result.date.toISOString()).toBe(\"2023-02-01T00:00:00.000Z\");\n    });\n\n    // related to issue https://github.com/triggerdotdev/trigger.dev/issues/1968\n    it(\"should ignore original undefined type metadata for overriden fields\", async () => {\n      const originalData = {\n        name: \"John\",\n        age: 30,\n        date: new Date(\"2023-01-01\"),\n        country: undefined,\n        settings: {\n          theme: undefined,\n        },\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        name: \"Jane\",\n        surname: \"Doe\",\n        age: 25,\n        date: \"2023-02-01T00:00:00.000Z\",\n        country: \"US\",\n        settings: {\n          theme: \"dark\",\n        },\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.name).toBe(\"Jane\");\n      expect(result.surname).toBe(\"Doe\");\n      expect(result.country).toBe(\"US\");\n      expect(result.settings.theme).toBe(\"dark\");\n      expect(result.age).toBe(25);\n      expect(result.date).toBeInstanceOf(Date);\n      expect(result.date.toISOString()).toBe(\"2023-02-01T00:00:00.000Z\");\n    });\n\n    it(\"should preserve BigInt type metadata\", async () => {\n      const originalData = {\n        id: BigInt(123456789),\n        count: 42,\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        id: \"987654321\",\n        count: 100,\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.id).toBe(BigInt(987654321));\n      expect(typeof result.id).toBe(\"bigint\");\n      expect(result.count).toBe(100);\n    });\n\n    it(\"should preserve nested type metadata\", async () => {\n      const originalData = {\n        user: {\n          id: BigInt(123),\n          createdAt: new Date(\"2023-01-01\"),\n          settings: {\n            theme: \"dark\",\n            updatedAt: new Date(\"2023-01-01\"),\n          },\n        },\n        metadata: {\n          version: 1,\n        },\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        user: {\n          id: \"456\",\n          createdAt: \"2023-06-01T00:00:00.000Z\",\n          settings: {\n            theme: \"light\",\n            updatedAt: \"2023-06-01T00:00:00.000Z\",\n          },\n        },\n        metadata: {\n          version: 2,\n        },\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.user.id).toBe(BigInt(456));\n      expect(result.user.createdAt).toBeInstanceOf(Date);\n      expect(result.user.createdAt.toISOString()).toBe(\"2023-06-01T00:00:00.000Z\");\n      expect(result.user.settings.theme).toBe(\"light\");\n      expect(result.user.settings.updatedAt).toBeInstanceOf(Date);\n      expect(result.user.settings.updatedAt.toISOString()).toBe(\"2023-06-01T00:00:00.000Z\");\n      expect(result.metadata.version).toBe(2);\n    });\n\n    it(\"should preserve Set type metadata\", async () => {\n      const originalData = {\n        tags: new Set([\"tag1\", \"tag2\"]),\n        name: \"test\",\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        tags: [\"tag3\", \"tag4\", \"tag5\"],\n        name: \"updated\",\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.tags).toBeInstanceOf(Set);\n      expect(Array.from(result.tags)).toEqual([\"tag3\", \"tag4\", \"tag5\"]);\n      expect(result.name).toBe(\"updated\");\n    });\n\n    it(\"should preserve Map type metadata\", async () => {\n      const originalData = {\n        mapping: new Map([\n          [\"key1\", \"value1\"],\n          [\"key2\", \"value2\"],\n        ]),\n        name: \"test\",\n      };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n\n      const newPayloadJson = JSON.stringify({\n        mapping: [\n          [\"key3\", \"value3\"],\n          [\"key4\", \"value4\"],\n        ],\n        name: \"updated\",\n      });\n\n      const result = (await replaceSuperJsonPayload(originalSerialized, newPayloadJson)) as any;\n\n      expect(result.mapping).toBeInstanceOf(Map);\n      expect(result.mapping.get(\"key3\")).toBe(\"value3\");\n      expect(result.mapping.get(\"key4\")).toBe(\"value4\");\n      expect(result.name).toBe(\"updated\");\n    });\n\n    it(\"should throw error for invalid JSON payload\", async () => {\n      const originalData = { name: \"test\" };\n\n      const superjson = await import(\"superjson\");\n      const originalSerialized = superjson.stringify(originalData);\n      const invalidPayload = \"{ invalid json }\";\n\n      await expect(replaceSuperJsonPayload(originalSerialized, invalidPayload)).rejects.toThrow();\n    });\n  });\n});",
            "cost": 0.07867500000000001
        }
    },
    "recover_status": "success",
    "instance_ref": {
        "instance_id": "triggerdotdev__trigger.dev.pr_mirror.2508",
        "repo": "triggerdotdev/trigger.dev",
        "base_commit": "7d333e5b3c0c698f95b8c90896bff3176ccb482b",
        "head_commit": "ff7be341a80ca382becb0e8695bef27fc4abfffa",
        "title": "fix(core): prettyPrintingPacket will now do a structuredClone on non-circular references instead of outputting [Circular]",
        "merged_at": "2025-09-15T16:43:46Z",
        "html_url": "https://github.com/triggerdotdev/trigger.dev/pull/2508",
        "test_files": [
            "apps/webapp/test/fairDequeuingStrategy.test.ts",
            "packages/core/test/ioSerialization.test.ts"
        ],
        "code_files": [
            "apps/webapp/app/routes/resources.taskruns.$runParam.replay.ts",
            "packages/core/src/v3/utils/ioSerialization.ts",
            "references/hello-world/src/trigger/circularPayload.ts"
        ],
        "total_changes": 431,
        "num_files": 5,
        "pull_number": 2508,
        "patch": "diff --git a/apps/webapp/app/routes/resources.taskruns.$runParam.replay.ts b/apps/webapp/app/routes/resources.taskruns.$runParam.replay.ts\nindex 0e87a3d1bd..57de7632a2 100644\n--- a/apps/webapp/app/routes/resources.taskruns.$runParam.replay.ts\n+++ b/apps/webapp/app/routes/resources.taskruns.$runParam.replay.ts\n@@ -108,6 +108,8 @@ export async function loader({ request, params }: LoaderFunctionArgs) {\n   const disableVersionSelection = environment.type === \"DEVELOPMENT\";\n   const allowArbitraryQueues = backgroundWorkers.at(0)?.engine === \"V1\";\n \n+  const payload = await prettyPrintPacket(run.payload, run.payloadType);\n+\n   return typedjson({\n     concurrencyKey: run.concurrencyKey,\n     maxAttempts: run.maxAttempts,\n@@ -116,7 +118,7 @@ export async function loader({ request, params }: LoaderFunctionArgs) {\n     ttlSeconds: run.ttl ? parseDuration(run.ttl, \"s\") ?? undefined : undefined,\n     idempotencyKey: run.idempotencyKey,\n     runTags: run.runTags,\n-    payload: await prettyPrintPacket(run.payload, run.payloadType),\n+    payload,\n     payloadType: run.payloadType,\n     queue: run.queue,\n     metadata: run.seedMetadata\ndiff --git a/apps/webapp/test/fairDequeuingStrategy.test.ts b/apps/webapp/test/fairDequeuingStrategy.test.ts\nindex 3b4a6a375b..0d8b708161 100644\n--- a/apps/webapp/test/fairDequeuingStrategy.test.ts\n+++ b/apps/webapp/test/fairDequeuingStrategy.test.ts\n@@ -270,8 +270,8 @@ describe(\"FairDequeuingStrategy\", () => {\n \n       console.log(\"Second distribution took\", distribute2Duration, \"ms\");\n \n-      // Make sure the second call is more than 2 times faster than the first\n-      expect(distribute2Duration).toBeLessThan(withTolerance(distribute1Duration / 2));\n+      // Make sure the second call is faster than the first\n+      expect(distribute2Duration).toBeLessThan(distribute1Duration);\n \n       const startDistribute3 = performance.now();\n \n@@ -284,8 +284,8 @@ describe(\"FairDequeuingStrategy\", () => {\n \n       console.log(\"Third distribution took\", distribute3Duration, \"ms\");\n \n-      // Make sure the third call is more than 4 times the second\n-      expect(withTolerance(distribute3Duration)).toBeGreaterThan(distribute2Duration * 4);\n+      // Make sure the third call is faster than the second\n+      expect(withTolerance(distribute3Duration)).toBeGreaterThan(distribute2Duration);\n     }\n   );\n \ndiff --git a/packages/core/src/v3/utils/ioSerialization.ts b/packages/core/src/v3/utils/ioSerialization.ts\nindex 103260b85c..9bacc41422 100644\n--- a/packages/core/src/v3/utils/ioSerialization.ts\n+++ b/packages/core/src/v3/utils/ioSerialization.ts\n@@ -1,3 +1,4 @@\n+import { JSONHeroPath } from \"@jsonhero/path\";\n import { Attributes, Span } from \"@opentelemetry/api\";\n import { z } from \"zod\";\n import { ApiClient } from \"../apiClient/index.js\";\n@@ -12,7 +13,6 @@ import { SemanticInternalAttributes } from \"../semanticInternalAttributes.js\";\n import { TriggerTracer } from \"../tracer.js\";\n import { zodfetch } from \"../zodfetch.js\";\n import { flattenAttributes } from \"./flattenAttributes.js\";\n-import { JSONHeroPath } from \"@jsonhero/path\";\n \n export type IOPacket = {\n   data?: string | undefined;\n@@ -389,16 +389,40 @@ export async function prettyPrintPacket(\n     if (typeof rawData === \"string\") {\n       rawData = safeJsonParse(rawData);\n     }\n+\n     const { deserialize } = await loadSuperJSON();\n \n-    return await prettyPrintPacket(deserialize(rawData), \"application/json\");\n+    const hasCircularReferences = rawData && rawData.meta && hasCircularReference(rawData.meta);\n+\n+    if (hasCircularReferences) {\n+      return await prettyPrintPacket(deserialize(rawData), \"application/json\", {\n+        ...options,\n+        cloneReferences: false,\n+      });\n+    }\n+\n+    return await prettyPrintPacket(deserialize(rawData), \"application/json\", {\n+      ...options,\n+      cloneReferences: true,\n+    });\n   }\n \n   if (dataType === \"application/json\") {\n     if (typeof rawData === \"string\") {\n       rawData = safeJsonParse(rawData);\n     }\n-    return JSON.stringify(rawData, makeSafeReplacer(options), 2);\n+\n+    try {\n+      return JSON.stringify(rawData, makeSafeReplacer(options), 2);\n+    } catch (error) {\n+      // If cloneReferences is true, it's possible if our hasCircularReference logic is incorrect that stringifying the data will fail with a circular reference error\n+      // So we will try to stringify the data with cloneReferences set to false\n+      if (options?.cloneReferences) {\n+        return JSON.stringify(rawData, makeSafeReplacer({ ...options, cloneReferences: false }), 2);\n+      }\n+\n+      throw error;\n+    }\n   }\n \n   if (typeof rawData === \"string\") {\n@@ -410,6 +434,7 @@ export async function prettyPrintPacket(\n \n interface ReplacerOptions {\n   filteredKeys?: string[];\n+  cloneReferences?: boolean;\n }\n \n function makeSafeReplacer(options?: ReplacerOptions) {\n@@ -418,6 +443,10 @@ function makeSafeReplacer(options?: ReplacerOptions) {\n   return function replacer(key: string, value: any) {\n     if (typeof value === \"object\" && value !== null) {\n       if (seen.has(value)) {\n+        if (options?.cloneReferences) {\n+          return structuredClone(value);\n+        }\n+\n         return \"[Circular]\";\n       }\n       seen.add(value);\n@@ -557,3 +586,80 @@ function getKeyFromObject(object: unknown, key: string) {\n \n   return jsonHeroPath.first(object);\n }\n+\n+/**\n+ * Detects if a superjson serialization contains circular references\n+ * by analyzing the meta.referentialEqualities structure.\n+ *\n+ * Based on superjson's ReferentialEqualityAnnotations type:\n+ * Record<string, string[]> | [string[]] | [string[], Record<string, string[]>]\n+ *\n+ * Circular references are represented as:\n+ * - [string[]] where strings are paths that reference back to root or ancestors\n+ * - The first element in [string[], Record<string, string[]>] format\n+ */\n+function hasCircularReference(meta: any): boolean {\n+  if (!meta?.referentialEqualities) {\n+    return false;\n+  }\n+\n+  const re = meta.referentialEqualities;\n+\n+  // Case 1: [string[]] - array containing only circular references\n+  if (Array.isArray(re) && re.length === 1 && Array.isArray(re[0])) {\n+    return re[0].length > 0; // Has circular references\n+  }\n+\n+  // Case 2: [string[], Record<string, string[]>] - mixed format\n+  if (Array.isArray(re) && re.length === 2 && Array.isArray(re[0])) {\n+    return re[0].length > 0; // First element contains circular references\n+  }\n+\n+  // Case 3: Record<string, string[]> - check for circular patterns in shared references\n+  if (!Array.isArray(re) && typeof re === \"object\") {\n+    // Check if any reference path points to an ancestor path\n+    for (const [targetPath, referencePaths] of Object.entries(re)) {\n+      for (const refPath of referencePaths as string[]) {\n+        if (isCircularPattern(targetPath, refPath)) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+\n+  return false;\n+}\n+\n+/**\n+ * Checks if a reference pattern represents a circular reference\n+ * by analyzing if the reference path points back to an ancestor of the target path\n+ */\n+function isCircularPattern(targetPath: string, referencePath: string): boolean {\n+  const targetParts = targetPath.split(\".\");\n+  const refParts = referencePath.split(\".\");\n+\n+  // For circular references, the reference path often contains the target path as a prefix\n+  // Example: targetPath=\"user\", referencePath=\"user.details.user\"\n+  // This means user.details.user points back to user (circular)\n+\n+  // Check if reference path starts with target path + additional segments that loop back\n+  if (refParts.length > targetParts.length) {\n+    // Check if reference path starts with target path\n+    let isPrefix = true;\n+    for (let i = 0; i < targetParts.length; i++) {\n+      if (targetParts[i] !== refParts[i]) {\n+        isPrefix = false;\n+        break;\n+      }\n+    }\n+\n+    // If reference path starts with target path and ends with target path,\n+    // it's likely a circular reference (e.g., \"user\" -> \"user.details.user\")\n+    if (isPrefix && refParts[refParts.length - 1] === targetParts[targetParts.length - 1]) {\n+      return true;\n+    }\n+  }\n+\n+  return false;\n+}\ndiff --git a/packages/core/test/ioSerialization.test.ts b/packages/core/test/ioSerialization.test.ts\nindex ffb9b30753..d7bd90add8 100644\n--- a/packages/core/test/ioSerialization.test.ts\n+++ b/packages/core/test/ioSerialization.test.ts\n@@ -1,4 +1,4 @@\n-import { replaceSuperJsonPayload } from \"../src/v3/utils/ioSerialization.js\";\n+import { replaceSuperJsonPayload, prettyPrintPacket } from \"../src/v3/utils/ioSerialization.js\";\n \n describe(\"ioSerialization\", () => {\n   describe(\"replaceSuperJsonPayload\", () => {\n@@ -188,4 +188,160 @@ describe(\"ioSerialization\", () => {\n       await expect(replaceSuperJsonPayload(originalSerialized, invalidPayload)).rejects.toThrow();\n     });\n   });\n+\n+  describe(\"prettyPrintPacket\", () => {\n+    it(\"should return empty string for undefined data\", async () => {\n+      const result = await prettyPrintPacket(undefined);\n+      expect(result).toBe(\"\");\n+    });\n+\n+    it(\"should return string data as-is\", async () => {\n+      const result = await prettyPrintPacket(\"Hello, World!\");\n+      expect(result).toBe(\"Hello, World!\");\n+    });\n+\n+    it(\"should pretty print JSON data with default options\", async () => {\n+      const data = { name: \"John\", age: 30, nested: { value: true } };\n+      const result = await prettyPrintPacket(data, \"application/json\");\n+\n+      expect(result).toBe(JSON.stringify(data, null, 2));\n+    });\n+\n+    it(\"should handle JSON data as string\", async () => {\n+      const data = { name: \"John\", age: 30 };\n+      const jsonString = JSON.stringify(data);\n+      const result = await prettyPrintPacket(jsonString, \"application/json\");\n+\n+      expect(result).toBe(JSON.stringify(data, null, 2));\n+    });\n+\n+    it(\"should pretty print SuperJSON data\", async () => {\n+      const data = {\n+        name: \"John\",\n+        date: new Date(\"2023-01-01\"),\n+        bigInt: BigInt(123),\n+        set: new Set([\"a\", \"b\"]),\n+        map: new Map([[\"key\", \"value\"]]),\n+      };\n+\n+      const superjson = await import(\"superjson\");\n+      const serialized = superjson.stringify(data);\n+\n+      const result = await prettyPrintPacket(serialized, \"application/super+json\");\n+\n+      // Should deserialize and pretty print the data\n+      expect(result).toContain('\"name\": \"John\"');\n+      expect(result).toContain('\"date\": \"2023-01-01T00:00:00.000Z\"');\n+      expect(result).toContain('\"bigInt\": \"123\"');\n+      expect(result).toContain('\"set\": [\\n    \"a\",\\n    \"b\"\\n  ]');\n+      expect(result).toContain('\"map\": {\\n    \"key\": \"value\"\\n  }');\n+    });\n+\n+    it(\"should handle circular references\", async () => {\n+      const data: any = { name: \"John\" };\n+      data.self = data; // Create circular reference\n+\n+      // Create a SuperJSON serialized version to test the circular reference detection\n+      const superjson = await import(\"superjson\");\n+      const serialized = superjson.stringify(data);\n+\n+      const result = await prettyPrintPacket(serialized, \"application/super+json\");\n+\n+      expect(result).toContain('\"name\": \"John\"');\n+      expect(result).toContain('\"self\": \"[Circular]\"');\n+    });\n+\n+    it(\"should handle regular non-circular references\", async () => {\n+      const person = { name: \"John\" };\n+\n+      const data: any = { person1: person, person2: person };\n+\n+      // Create a SuperJSON serialized version to test the circular reference detection\n+      const superjson = await import(\"superjson\");\n+      const serialized = superjson.stringify(data);\n+\n+      const result = await prettyPrintPacket(serialized, \"application/super+json\");\n+\n+      expect(result).toContain('\"person1\": {');\n+      expect(result).toContain('\"person2\": {');\n+    });\n+\n+    it(\"should filter out specified keys\", async () => {\n+      const data = { name: \"John\", password: \"secret\", age: 30 };\n+      const result = await prettyPrintPacket(data, \"application/json\", {\n+        filteredKeys: [\"password\"],\n+      });\n+\n+      expect(result).toContain('\"name\": \"John\"');\n+      expect(result).toContain('\"age\": 30');\n+      expect(result).not.toContain('\"password\"');\n+    });\n+\n+    it(\"should handle BigInt values\", async () => {\n+      const data = { id: BigInt(123456789), name: \"John\" };\n+      const result = await prettyPrintPacket(data, \"application/json\");\n+\n+      expect(result).toContain('\"id\": \"123456789\"');\n+      expect(result).toContain('\"name\": \"John\"');\n+    });\n+\n+    it(\"should handle RegExp values\", async () => {\n+      const data = { pattern: /test/gi, name: \"John\" };\n+      const result = await prettyPrintPacket(data, \"application/json\");\n+\n+      expect(result).toContain('\"pattern\": \"/test/gi\"');\n+      expect(result).toContain('\"name\": \"John\"');\n+    });\n+\n+    it(\"should handle Set values\", async () => {\n+      const data = { tags: new Set([\"tag1\", \"tag2\"]), name: \"John\" };\n+      const result = await prettyPrintPacket(data, \"application/json\");\n+\n+      expect(result).toContain('\"tags\": [\\n    \"tag1\",\\n    \"tag2\"\\n  ]');\n+      expect(result).toContain('\"name\": \"John\"');\n+    });\n+\n+    it(\"should handle Map values\", async () => {\n+      const data = { mapping: new Map([[\"key1\", \"value1\"]]), name: \"John\" };\n+      const result = await prettyPrintPacket(data, \"application/json\");\n+\n+      expect(result).toContain('\"mapping\": {\\n    \"key1\": \"value1\"\\n  }');\n+      expect(result).toContain('\"name\": \"John\"');\n+    });\n+\n+    it(\"should handle complex nested data\", async () => {\n+      const data = {\n+        user: {\n+          id: BigInt(123),\n+          createdAt: new Date(\"2023-01-01\"),\n+          settings: {\n+            theme: \"dark\",\n+            tags: new Set([\"admin\", \"user\"]),\n+            config: new Map([[\"timeout\", \"30s\"]]),\n+          },\n+        },\n+        metadata: {\n+          version: 1,\n+          pattern: /^test$/,\n+        },\n+      };\n+\n+      const result = await prettyPrintPacket(data, \"application/json\");\n+\n+      expect(result).toContain('\"id\": \"123\"');\n+      expect(result).toContain('\"createdAt\": \"2023-01-01T00:00:00.000Z\"');\n+      expect(result).toContain('\"theme\": \"dark\"');\n+      expect(result).toContain('\"tags\": [\\n        \"admin\",\\n        \"user\"\\n      ]');\n+      expect(result).toContain('\"config\": {\\n        \"timeout\": \"30s\"\\n      }');\n+      expect(result).toContain('\"version\": 1');\n+      expect(result).toContain('\"pattern\": \"/^test$/\"');\n+    });\n+\n+    it(\"should handle data without dataType parameter\", async () => {\n+      const data = { name: \"John\", age: 30 };\n+      const result = await prettyPrintPacket(data);\n+\n+      expect(result).toBe(JSON.stringify(data, null, 2));\n+    });\n+  });\n });\ndiff --git a/references/hello-world/src/trigger/circularPayload.ts b/references/hello-world/src/trigger/circularPayload.ts\nnew file mode 100644\nindex 0000000000..3e9d0a9545\n--- /dev/null\n+++ b/references/hello-world/src/trigger/circularPayload.ts\n@@ -0,0 +1,149 @@\n+import { logger, schemaTask, task, tasks } from \"@trigger.dev/sdk\";\n+import { z } from \"zod/v3\";\n+\n+export const referentialPayloadParentTask = task({\n+  id: \"referential-payload-parent\",\n+  run: async (payload: any) => {\n+    // Shared objects\n+    const workflowData = {\n+      id: \"workflow-123\",\n+      formName: \"Contact Form\",\n+    };\n+\n+    const response = [\n+      {\n+        id: \"q1_name\",\n+        answer: \"John Doe\",\n+      },\n+      {\n+        id: \"q2_consent\",\n+        answer: \"yes\",\n+        leadAttribute: undefined, // Will be marked in meta\n+      },\n+    ];\n+\n+    const personAttributes = {\n+      ip: \"192.168.1.1\",\n+      visitedForm: 1,\n+    };\n+\n+    // Main object with shared references\n+    const originalObject = {\n+      workflowData: workflowData, // Root reference\n+      workflowContext: {\n+        leadId: undefined, // Will be marked in meta\n+        workflowJob: {\n+          workflowData: workflowData, // Same reference as root\n+          createdAt: new Date(\"2025-08-19T12:13:42.260Z\"), // Date object\n+        },\n+        responseData: {\n+          personAttributes: personAttributes, // Same reference as root\n+        },\n+        response: response, // Same reference as root\n+      },\n+      personAttributes: personAttributes, // Root reference\n+      response: response, // Root reference\n+      jobArgs: {\n+        response: response, // Same reference as root\n+        args: workflowData, // Same reference as root\n+      },\n+    };\n+\n+    await tasks.triggerAndWait<typeof referentialPayloadChildTask>(\n+      \"referential-payload-child\",\n+      originalObject\n+    );\n+\n+    return {\n+      message: \"Hello, world!\",\n+    };\n+  },\n+});\n+\n+// Define the circular schema using z.lazy() for the recursive reference\n+const WorkflowDataSchema = z.object({\n+  id: z.string(),\n+  formName: z.string(),\n+});\n+\n+const ResponseItemSchema = z.object({\n+  id: z.string(),\n+  answer: z.string(),\n+  leadAttribute: z.undefined().optional(),\n+});\n+\n+const PersonAttributesSchema = z.object({\n+  ip: z.string(),\n+  visitedForm: z.number(),\n+});\n+\n+const OriginalObjectSchema = z.object({\n+  workflowData: WorkflowDataSchema,\n+  workflowContext: z.object({\n+    leadId: z.undefined(),\n+    workflowJob: z.object({\n+      workflowData: WorkflowDataSchema, // Same reference\n+      createdAt: z.date(),\n+    }),\n+    responseData: z.object({\n+      personAttributes: PersonAttributesSchema, // Same reference\n+    }),\n+    response: z.array(ResponseItemSchema), // Same reference\n+  }),\n+  personAttributes: PersonAttributesSchema, // Root reference\n+  response: z.array(ResponseItemSchema), // Root reference\n+  jobArgs: z.object({\n+    response: z.array(ResponseItemSchema), // Same reference\n+    args: WorkflowDataSchema, // Same reference\n+  }),\n+});\n+\n+export const referentialPayloadChildTask = schemaTask({\n+  id: \"referential-payload-child\",\n+  schema: OriginalObjectSchema,\n+  run: async (payload) => {\n+    logger.info(\"Received circular payload\", { payload });\n+\n+    return {\n+      message: \"Hello, world!\",\n+    };\n+  },\n+});\n+\n+export const circularReferenceParentTask = task({\n+  id: \"circular-reference-parent\",\n+  run: async (payload: any) => {\n+    const user = {\n+      name: \"Alice\",\n+      details: {\n+        age: 30,\n+        email: \"alice@example.com\",\n+      },\n+    };\n+    // @ts-expect-error - This is a circular reference\n+    user.details.user = user;\n+\n+    await tasks.triggerAndWait<typeof circularReferenceChildTask>(\"circular-reference-child\", {\n+      // @ts-expect-error - This is a circular reference\n+      user,\n+    });\n+  },\n+});\n+\n+type CircularReferencePayload = {\n+  user: {\n+    name: string;\n+    details: {\n+      age: number;\n+      email: string;\n+      user: CircularReferencePayload;\n+    };\n+  };\n+};\n+\n+export const circularReferenceChildTask = task({\n+  id: \"circular-reference-child\",\n+  run: async (payload: CircularReferencePayload) => {\n+    logger.info(\"Received circular payload\", { payload });\n+  },\n+});\n",
        "test_patch": "",
        "problem_statement": "",
        "hints_text": "",
        "pr_mirror": "triggerdotdev__trigger.dev.d1c3bfb9"
    }
}