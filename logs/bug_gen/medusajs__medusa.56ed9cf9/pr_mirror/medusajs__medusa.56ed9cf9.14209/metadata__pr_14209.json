{
    "cost": 0.15246500000000002,
    "rewrites": {
        "medusajs__medusa.56ed9cf9/packages/modules/providers/file-s3/integration-tests/__tests__/services.spec.ts": {
            "output": "import fs from \"fs/promises\"\nimport axios from \"axios\"\nimport { S3FileService } from \"../../src/services/s3-file\"\njest.setTimeout(100000)\n\n// Note: This test hits the S3 service, and it is mainly meant to be run manually after setting all the envvars below.\n// We can also set up some test buckets in our pipeline to run this test, but it is not really that important to do so for now.\ndescribe.skip(\"S3 File Plugin\", () => {\n  let s3Service: S3FileService\n  let fixtureImagePath: string\n  beforeAll(() => {\n    fixtureImagePath =\n      process.cwd() + \"/integration-tests/__fixtures__/catphoto.jpg\"\n\n    s3Service = new S3FileService(\n      {\n        logger: console as any,\n      },\n      {\n        endpoint: process.env.S3_TEST_ENDPOINT ?? \"\",\n        file_url: process.env.S3_TEST_FILE_URL ?? \"\",\n        access_key_id: process.env.S3_TEST_ACCESS_KEY_ID ?? \"\",\n        secret_access_key: process.env.S3_TEST_SECRET_ACCESS_KEY ?? \"\",\n        region: process.env.S3_TEST_REGION ?? \"\",\n        bucket: process.env.S3_TEST_BUCKET ?? \"\",\n        prefix: \"tests/\",\n        additional_client_config: process.env.S3_TEST_ENDPOINT?.includes(\n          \"localhost\"\n        )\n          ? {\n              sslEnabled: false,\n              s3ForcePathStyle: true,\n            }\n          : {},\n      }\n    )\n  })\n  ;([\"public\", \"private\"] as const).forEach((access) => {\n    it(\"uploads, reads, and then deletes a file successfully\", async () => {\n      const fileContent = await fs.readFile(fixtureImagePath)\n      const fixtureAsBinary = fileContent.toString(\"binary\")\n\n      const resp = await s3Service.upload({\n        filename: \"catphoto.jpg\",\n        mimeType: \"image/jpeg\",\n        content: fixtureAsBinary,\n        access,\n      })\n\n      expect(resp).toEqual({\n        key: expect.stringMatching(/tests\\/catphoto.*\\.jpg/),\n        url: expect.stringMatching(/https:\\/\\/.*\\.jpg/),\n      })\n\n      const urlResp = await axios.get(resp.url).catch((e) => e.response)\n      expect(urlResp.status).toEqual(access === \"public\" ? 200 : 403)\n\n      const signedUrl = await s3Service.getPresignedDownloadUrl({\n        fileKey: resp.key,\n      })\n\n      const signedUrlFile = Buffer.from(\n        await axios\n          .get(signedUrl, { responseType: \"arraybuffer\" })\n          .then((r) => r.data)\n      )\n\n      expect(signedUrlFile.toString(\"binary\")).toEqual(fixtureAsBinary)\n\n      await s3Service.delete({ fileKey: resp.key })\n\n      // TODO: Currently the presignedURL will be returned even if the file doesn't exist. Should we check for existence first?\n      const deletedFileUrl = await s3Service.getPresignedDownloadUrl({\n        fileKey: resp.key,\n      })\n\n      const { response } = await axios\n        .get(deletedFileUrl, { responseType: \"arraybuffer\" })\n        .catch((e) => e)\n\n      expect(response.status).toEqual(404)\n    })\n  })\n\n  it(\"uploads a file with non-ascii characters in the name\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"base64\")\n\n    const resp = await s3Service.upload({\n      filename: \"catphoto-\u304b.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n      access: \"private\",\n    })\n\n    expect(resp).toEqual({\n      key: expect.stringMatching(/tests\\/catphoto-\u304b.*\\.jpg/),\n      url: expect.stringMatching(/https:\\/\\/.*\\/catphoto-%E3%81%8B.*\\.jpg/),\n    })\n  })\n\n  it(\"uploads a file with special URL characters in the name\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"base64\")\n\n    const resp = await s3Service.upload({\n      filename: \"cat?photo.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n      access: \"private\",\n    })\n\n    expect(resp).toEqual({\n      key: expect.stringMatching(/tests\\/catphoto.*\\.jpg/),\n      url: expect.stringMatching(/https:\\/\\/.*\\/cat%3Fphoto.*\\.jpg/),\n    })\n  })\n\n  it(\"gets a presigned upload URL and uploads a file successfully\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"binary\")\n\n    const resp = await s3Service.getPresignedUploadUrl({\n      filename: \"catphoto.jpg\",\n      mimeType: \"image/jpeg\",\n      access: \"private\",\n    })\n\n    expect(resp).toEqual({\n      key: expect.stringMatching(/tests\\/catphoto.*\\.jpg/),\n      url: expect.stringMatching(/https:\\/\\/.*catphoto\\.jpg/),\n    })\n\n    const uploadResp = await axios.put(resp.url, fileContent, {\n      headers: {\n        // On Digitalocean, among others, despite the ACL set on the upload URL, the caller can set the acl to anything they want.\n        // On AWS passing the ACL in the upload will fail since it's set on the signed URL.\n        // \"x-amz-acl\": \"private\",\n        \"Content-Type\": \"image/jpeg\",\n      },\n    })\n\n    expect(uploadResp.status).toEqual(200)\n\n    const signedUrl = await s3Service.getPresignedDownloadUrl({\n      fileKey: resp.key,\n    })\n\n    const signedUrlFile = Buffer.from(\n      await axios\n        .get(signedUrl, { responseType: \"arraybuffer\" })\n        .then((r) => r.data)\n    )\n\n    expect(signedUrlFile.toString(\"binary\")).toEqual(fixtureAsBinary)\n\n    await s3Service.delete({ fileKey: resp.key })\n  })\n\n  it(\"gets a presigned upload URL for a nested filename structure and uploads a file successfully\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"binary\")\n\n    const resp = await s3Service.getPresignedUploadUrl({\n      filename: \"testfolder/catphoto.jpg\",\n      mimeType: \"image/jpeg\",\n      access: \"private\",\n    })\n\n    expect(resp).toEqual({\n      key: expect.stringMatching(/tests\\/testfolder\\/catphoto.*\\.jpg/),\n      url: expect.stringMatching(/https:\\/\\/.*testfolder\\/catphoto\\.jpg/),\n    })\n\n    const uploadResp = await axios.put(resp.url, fileContent, {\n      headers: {\n        \"Content-Type\": \"image/jpeg\",\n      },\n    })\n\n    expect(uploadResp.status).toEqual(200)\n\n    const signedUrl = await s3Service.getPresignedDownloadUrl({\n      fileKey: resp.key,\n    })\n\n    const signedUrlFile = Buffer.from(\n      await axios\n        .get(signedUrl, { responseType: \"arraybuffer\" })\n        .then((r) => r.data)\n    )\n\n    expect(signedUrlFile.toString(\"binary\")).toEqual(fixtureAsBinary)\n\n    await s3Service.delete({ fileKey: resp.key })\n  })\n\n  it(\"deletes multiple files in bulk\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"binary\")\n\n    const cat = await s3Service.upload({\n      filename: \"catphoto.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n    })\n    const cat1 = await s3Service.upload({\n      filename: \"catphoto-1.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n    })\n    const cat2 = await s3Service.upload({\n      filename: \"catphoto-2.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n    })\n\n    await s3Service.delete([\n      { fileKey: cat.key },\n      { fileKey: cat1.key },\n      { fileKey: cat2.key },\n    ])\n  })\n})",
            "output_extracted": "import fs from \"fs/promises\"\nimport axios from \"axios\"\nimport { S3FileService } from \"../../src/services/s3-file\"\njest.setTimeout(100000)\n\n// Note: This test hits the S3 service, and it is mainly meant to be run manually after setting all the envvars below.\n// We can also set up some test buckets in our pipeline to run this test, but it is not really that important to do so for now.\ndescribe.skip(\"S3 File Plugin\", () => {\n  let s3Service: S3FileService\n  let fixtureImagePath: string\n  beforeAll(() => {\n    fixtureImagePath =\n      process.cwd() + \"/integration-tests/__fixtures__/catphoto.jpg\"\n\n    s3Service = new S3FileService(\n      {\n        logger: console as any,\n      },\n      {\n        endpoint: process.env.S3_TEST_ENDPOINT ?? \"\",\n        file_url: process.env.S3_TEST_FILE_URL ?? \"\",\n        access_key_id: process.env.S3_TEST_ACCESS_KEY_ID ?? \"\",\n        secret_access_key: process.env.S3_TEST_SECRET_ACCESS_KEY ?? \"\",\n        region: process.env.S3_TEST_REGION ?? \"\",\n        bucket: process.env.S3_TEST_BUCKET ?? \"\",\n        prefix: \"tests/\",\n        additional_client_config: process.env.S3_TEST_ENDPOINT?.includes(\n          \"localhost\"\n        )\n          ? {\n              sslEnabled: false,\n              s3ForcePathStyle: true,\n            }\n          : {},\n      }\n    )\n  })\n  ;([\"public\", \"private\"] as const).forEach((access) => {\n    it(\"uploads, reads, and then deletes a file successfully\", async () => {\n      const fileContent = await fs.readFile(fixtureImagePath)\n      const fixtureAsBinary = fileContent.toString(\"binary\")\n\n      const resp = await s3Service.upload({\n        filename: \"catphoto.jpg\",\n        mimeType: \"image/jpeg\",\n        content: fixtureAsBinary,\n        access,\n      })\n\n      expect(resp).toEqual({\n        key: expect.stringMatching(/tests\\/catphoto.*\\.jpg/),\n        url: expect.stringMatching(/https:\\/\\/.*\\.jpg/),\n      })\n\n      const urlResp = await axios.get(resp.url).catch((e) => e.response)\n      expect(urlResp.status).toEqual(access === \"public\" ? 200 : 403)\n\n      const signedUrl = await s3Service.getPresignedDownloadUrl({\n        fileKey: resp.key,\n      })\n\n      const signedUrlFile = Buffer.from(\n        await axios\n          .get(signedUrl, { responseType: \"arraybuffer\" })\n          .then((r) => r.data)\n      )\n\n      expect(signedUrlFile.toString(\"binary\")).toEqual(fixtureAsBinary)\n\n      await s3Service.delete({ fileKey: resp.key })\n\n      // TODO: Currently the presignedURL will be returned even if the file doesn't exist. Should we check for existence first?\n      const deletedFileUrl = await s3Service.getPresignedDownloadUrl({\n        fileKey: resp.key,\n      })\n\n      const { response } = await axios\n        .get(deletedFileUrl, { responseType: \"arraybuffer\" })\n        .catch((e) => e)\n\n      expect(response.status).toEqual(404)\n    })\n  })\n\n  it(\"uploads a file with non-ascii characters in the name\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"base64\")\n\n    const resp = await s3Service.upload({\n      filename: \"catphoto-\u304b.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n      access: \"private\",\n    })\n\n    expect(resp).toEqual({\n      key: expect.stringMatching(/tests\\/catphoto-\u304b.*\\.jpg/),\n      url: expect.stringMatching(/https:\\/\\/.*\\/catphoto-%E3%81%8B.*\\.jpg/),\n    })\n  })\n\n  it(\"uploads a file with special URL characters in the name\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"base64\")\n\n    const resp = await s3Service.upload({\n      filename: \"cat?photo.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n      access: \"private\",\n    })\n\n    expect(resp).toEqual({\n      key: expect.stringMatching(/tests\\/catphoto.*\\.jpg/),\n      url: expect.stringMatching(/https:\\/\\/.*\\/cat%3Fphoto.*\\.jpg/),\n    })\n  })\n\n  it(\"gets a presigned upload URL and uploads a file successfully\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"binary\")\n\n    const resp = await s3Service.getPresignedUploadUrl({\n      filename: \"catphoto.jpg\",\n      mimeType: \"image/jpeg\",\n      access: \"private\",\n    })\n\n    expect(resp).toEqual({\n      key: expect.stringMatching(/tests\\/catphoto.*\\.jpg/),\n      url: expect.stringMatching(/https:\\/\\/.*catphoto\\.jpg/),\n    })\n\n    const uploadResp = await axios.put(resp.url, fileContent, {\n      headers: {\n        // On Digitalocean, among others, despite the ACL set on the upload URL, the caller can set the acl to anything they want.\n        // On AWS passing the ACL in the upload will fail since it's set on the signed URL.\n        // \"x-amz-acl\": \"private\",\n        \"Content-Type\": \"image/jpeg\",\n      },\n    })\n\n    expect(uploadResp.status).toEqual(200)\n\n    const signedUrl = await s3Service.getPresignedDownloadUrl({\n      fileKey: resp.key,\n    })\n\n    const signedUrlFile = Buffer.from(\n      await axios\n        .get(signedUrl, { responseType: \"arraybuffer\" })\n        .then((r) => r.data)\n    )\n\n    expect(signedUrlFile.toString(\"binary\")).toEqual(fixtureAsBinary)\n\n    await s3Service.delete({ fileKey: resp.key })\n  })\n\n  it(\"gets a presigned upload URL for a nested filename structure and uploads a file successfully\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"binary\")\n\n    const resp = await s3Service.getPresignedUploadUrl({\n      filename: \"testfolder/catphoto.jpg\",\n      mimeType: \"image/jpeg\",\n      access: \"private\",\n    })\n\n    expect(resp).toEqual({\n      key: expect.stringMatching(/tests\\/testfolder\\/catphoto.*\\.jpg/),\n      url: expect.stringMatching(/https:\\/\\/.*testfolder\\/catphoto\\.jpg/),\n    })\n\n    const uploadResp = await axios.put(resp.url, fileContent, {\n      headers: {\n        \"Content-Type\": \"image/jpeg\",\n      },\n    })\n\n    expect(uploadResp.status).toEqual(200)\n\n    const signedUrl = await s3Service.getPresignedDownloadUrl({\n      fileKey: resp.key,\n    })\n\n    const signedUrlFile = Buffer.from(\n      await axios\n        .get(signedUrl, { responseType: \"arraybuffer\" })\n        .then((r) => r.data)\n    )\n\n    expect(signedUrlFile.toString(\"binary\")).toEqual(fixtureAsBinary)\n\n    await s3Service.delete({ fileKey: resp.key })\n  })\n\n  it(\"deletes multiple files in bulk\", async () => {\n    const fileContent = await fs.readFile(fixtureImagePath)\n    const fixtureAsBinary = fileContent.toString(\"binary\")\n\n    const cat = await s3Service.upload({\n      filename: \"catphoto.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n    })\n    const cat1 = await s3Service.upload({\n      filename: \"catphoto-1.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n    })\n    const cat2 = await s3Service.upload({\n      filename: \"catphoto-2.jpg\",\n      mimeType: \"image/jpeg\",\n      content: fixtureAsBinary,\n    })\n\n    await s3Service.delete([\n      { fileKey: cat.key },\n      { fileKey: cat1.key },\n      { fileKey: cat2.key },\n    ])\n  })\n})",
            "cost": 0.07364000000000001
        },
        "medusajs__medusa.56ed9cf9/packages/modules/providers/file-s3/src/services/s3-file.ts": {
            "output": "import {\n  DeleteObjectCommand,\n  DeleteObjectsCommand,\n  GetObjectCommand,\n  ObjectCannedACL,\n  PutObjectCommand,\n  S3Client,\n  S3ClientConfigType,\n} from \"@aws-sdk/client-s3\"\nimport { getSignedUrl } from \"@aws-sdk/s3-request-presigner\"\nimport {\n  FileTypes,\n  Logger,\n  S3FileServiceOptions,\n} from \"@medusajs/framework/types\"\nimport {\n  AbstractFileProviderService,\n  MedusaError,\n} from \"@medusajs/framework/utils\"\nimport path from \"path\"\nimport { Readable } from \"stream\"\nimport { ulid } from \"ulid\"\n\ntype InjectedDependencies = {\n  logger: Logger\n}\n\ninterface S3FileServiceConfig {\n  fileUrl: string\n  accessKeyId?: string\n  secretAccessKey?: string\n  authenticationMethod?: \"access-key\" | \"s3-iam-role\"\n  region: string\n  bucket: string\n  prefix?: string\n  endpoint?: string\n  cacheControl?: string\n  downloadFileDuration?: number\n  additionalClientConfig?: Record<string, any>\n}\n\nconst DEFAULT_UPLOAD_EXPIRATION_DURATION_SECONDS = 60 * 60\n\nexport class S3FileService extends AbstractFileProviderService {\n  static identifier = \"s3\"\n  protected config_: S3FileServiceConfig\n  protected logger_: Logger\n  protected client_: S3Client\n\n  constructor({ logger }: InjectedDependencies, options: S3FileServiceOptions) {\n    super()\n\n    const authenticationMethod = options.authentication_method ?? \"access-key\"\n\n    if (\n      authenticationMethod === \"access-key\" &&\n      (!options.access_key_id || !options.secret_access_key)\n    ) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `Access key ID and secret access key are required when using access key authentication`\n      )\n    }\n\n    this.config_ = {\n      fileUrl: options.file_url,\n      accessKeyId: options.access_key_id,\n      secretAccessKey: options.secret_access_key,\n      authenticationMethod: authenticationMethod,\n      region: options.region,\n      bucket: options.bucket,\n      prefix: options.prefix ?? \"\",\n      endpoint: options.endpoint,\n      cacheControl: options.cache_control ?? \"public, max-age=31536000\",\n      downloadFileDuration: options.download_file_duration ?? 60 * 60,\n      additionalClientConfig: options.additional_client_config ?? {},\n    }\n    this.logger_ = logger\n    this.client_ = this.getClient()\n  }\n\n  protected getClient() {\n    // If none is provided, the SDK will use the default credentials provider chain, see https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-envvars.html\n    const credentials =\n      this.config_.authenticationMethod === \"access-key\"\n        ? {\n            accessKeyId: this.config_.accessKeyId!,\n            secretAccessKey: this.config_.secretAccessKey!,\n          }\n        : undefined\n\n    const config: S3ClientConfigType = {\n      credentials,\n      region: this.config_.region,\n      endpoint: this.config_.endpoint,\n      ...this.config_.additionalClientConfig,\n    }\n\n    return new S3Client(config)\n  }\n\n  async upload(\n    file: FileTypes.ProviderUploadFileDTO\n  ): Promise<FileTypes.ProviderFileResultDTO> {\n    if (!file) {\n      throw new MedusaError(MedusaError.Types.INVALID_DATA, `No file provided`)\n    }\n\n    if (!file.filename) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `No filename provided`\n      )\n    }\n\n    const parsedFilename = path.parse(file.filename)\n\n    // TODO: Allow passing a full path for storage per request, not as a global config.\n    const fileKey = `${this.config_.prefix}${parsedFilename.name}-${ulid()}${\n      parsedFilename.ext\n    }`\n\n    let content: Buffer\n    try {\n      const decoded = Buffer.from(file.content, \"base64\")\n      if (decoded.toString(\"base64\") === file.content) {\n        content = decoded\n      } else {\n        content = Buffer.from(file.content, \"utf8\")\n      }\n    } catch {\n      // Last-resort fallback: binary\n      content = Buffer.from(file.content, \"binary\")\n    }\n\n    const command = new PutObjectCommand({\n      // We probably also want to support a separate bucket altogether for private files\n      // protected private_bucket_: string\n      // protected private_access_key_id_: string\n      // protected private_secret_access_key_: string\n\n      ACL: file.access === \"public\" ? \"public-read\" : \"private\",\n      Bucket: this.config_.bucket,\n      Body: content,\n      Key: fileKey,\n      ContentType: file.mimeType,\n      CacheControl: this.config_.cacheControl,\n      // Note: We could potentially set the content disposition when uploading,\n      // but storing the original filename as metadata should suffice.\n      Metadata: {\n        \"x-amz-meta-original-filename\": file.filename,\n      },\n    })\n\n    try {\n      await this.client_.send(command)\n    } catch (e) {\n      this.logger_.error(e)\n      throw e\n    }\n\n    return {\n      url: `${this.config_.fileUrl}/${fileKey}`,\n      key: fileKey,\n    }\n  }\n\n  async delete(\n    files: FileTypes.ProviderDeleteFileDTO | FileTypes.ProviderDeleteFileDTO[]\n  ): Promise<void> {\n    try {\n      /**\n       * Bulk delete files\n       */\n      if (Array.isArray(files)) {\n        await this.client_.send(\n          new DeleteObjectsCommand({\n            Bucket: this.config_.bucket,\n            Delete: {\n              Objects: files.map((file) => ({\n                Key: file.fileKey,\n              })),\n              Quiet: true,\n            },\n          })\n        )\n      } else {\n        await this.client_.send(\n          new DeleteObjectCommand({\n            Bucket: this.config_.bucket,\n            Key: files.fileKey,\n          })\n        )\n      }\n    } catch (e) {\n      // TODO: Rethrow depending on the error (eg. a file not found error is fine, but a failed request should be rethrown)\n      this.logger_.error(e)\n    }\n  }\n\n  async getPresignedDownloadUrl(\n    fileData: FileTypes.ProviderGetFileDTO\n  ): Promise<string> {\n    // TODO: Allow passing content disposition when getting a presigned URL\n    const command = new GetObjectCommand({\n      Bucket: this.config_.bucket,\n      Key: `${fileData.fileKey}`,\n    })\n\n    return await getSignedUrl(this.client_, command, {\n      expiresIn: this.config_.downloadFileDuration,\n    })\n  }\n\n  // Note: Some providers (eg. AWS S3) allows IAM policies to further restrict what can be uploaded.\n  async getPresignedUploadUrl(\n    fileData: FileTypes.ProviderGetPresignedUploadUrlDTO\n  ): Promise<FileTypes.ProviderFileResultDTO> {\n    if (!fileData?.filename) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `No filename provided`\n      )\n    }\n\n    const fileKey = `${this.config_.prefix}${fileData.filename}`\n\n    let acl: ObjectCannedACL | undefined\n    if (fileData.access) {\n      acl = fileData.access === \"public\" ? \"public-read\" : \"private\"\n    }\n\n    // Using content-type, acl, etc. doesn't work with all providers, and some simply ignore it.\n    const command = new PutObjectCommand({\n      Bucket: this.config_.bucket,\n      ContentType: fileData.mimeType,\n      ACL: acl,\n      Key: fileKey,\n    })\n\n    const signedUrl = await getSignedUrl(this.client_, command, {\n      expiresIn:\n        fileData.expiresIn ?? DEFAULT_UPLOAD_EXPIRATION_DURATION_SECONDS,\n    })\n\n    return {\n      url: signedUrl,\n      key: fileKey,\n    }\n  }\n\n  async getDownloadStream(\n    file: FileTypes.ProviderGetFileDTO\n  ): Promise<Readable> {\n    if (!file?.fileKey) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `No fileKey provided`\n      )\n    }\n\n    const fileKey = file.fileKey\n    const response = await this.client_.send(\n      new GetObjectCommand({\n        Key: fileKey,\n        Bucket: this.config_.bucket,\n      })\n    )\n\n    return response.Body! as Readable\n  }\n\n  async getAsBuffer(file: FileTypes.ProviderGetFileDTO): Promise<Buffer> {\n    if (!file?.fileKey) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `No fileKey provided`\n      )\n    }\n\n    const fileKey = file.fileKey\n    const response = await this.client_.send(\n      new GetObjectCommand({\n        Key: fileKey,\n        Bucket: this.config_.bucket,\n      })\n    )\n\n    return Buffer.from(await response.Body!.transformToByteArray())\n  }\n}",
            "output_extracted": "import {\n  DeleteObjectCommand,\n  DeleteObjectsCommand,\n  GetObjectCommand,\n  ObjectCannedACL,\n  PutObjectCommand,\n  S3Client,\n  S3ClientConfigType,\n} from \"@aws-sdk/client-s3\"\nimport { getSignedUrl } from \"@aws-sdk/s3-request-presigner\"\nimport {\n  FileTypes,\n  Logger,\n  S3FileServiceOptions,\n} from \"@medusajs/framework/types\"\nimport {\n  AbstractFileProviderService,\n  MedusaError,\n} from \"@medusajs/framework/utils\"\nimport path from \"path\"\nimport { Readable } from \"stream\"\nimport { ulid } from \"ulid\"\n\ntype InjectedDependencies = {\n  logger: Logger\n}\n\ninterface S3FileServiceConfig {\n  fileUrl: string\n  accessKeyId?: string\n  secretAccessKey?: string\n  authenticationMethod?: \"access-key\" | \"s3-iam-role\"\n  region: string\n  bucket: string\n  prefix?: string\n  endpoint?: string\n  cacheControl?: string\n  downloadFileDuration?: number\n  additionalClientConfig?: Record<string, any>\n}\n\nconst DEFAULT_UPLOAD_EXPIRATION_DURATION_SECONDS = 60 * 60\n\nexport class S3FileService extends AbstractFileProviderService {\n  static identifier = \"s3\"\n  protected config_: S3FileServiceConfig\n  protected logger_: Logger\n  protected client_: S3Client\n\n  constructor({ logger }: InjectedDependencies, options: S3FileServiceOptions) {\n    super()\n\n    const authenticationMethod = options.authentication_method ?? \"access-key\"\n\n    if (\n      authenticationMethod === \"access-key\" &&\n      (!options.access_key_id || !options.secret_access_key)\n    ) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `Access key ID and secret access key are required when using access key authentication`\n      )\n    }\n\n    this.config_ = {\n      fileUrl: options.file_url,\n      accessKeyId: options.access_key_id,\n      secretAccessKey: options.secret_access_key,\n      authenticationMethod: authenticationMethod,\n      region: options.region,\n      bucket: options.bucket,\n      prefix: options.prefix ?? \"\",\n      endpoint: options.endpoint,\n      cacheControl: options.cache_control ?? \"public, max-age=31536000\",\n      downloadFileDuration: options.download_file_duration ?? 60 * 60,\n      additionalClientConfig: options.additional_client_config ?? {},\n    }\n    this.logger_ = logger\n    this.client_ = this.getClient()\n  }\n\n  protected getClient() {\n    // If none is provided, the SDK will use the default credentials provider chain, see https://docs.aws.amazon.com/cli/v1/userguide/cli-configure-envvars.html\n    const credentials =\n      this.config_.authenticationMethod === \"access-key\"\n        ? {\n            accessKeyId: this.config_.accessKeyId!,\n            secretAccessKey: this.config_.secretAccessKey!,\n          }\n        : undefined\n\n    const config: S3ClientConfigType = {\n      credentials,\n      region: this.config_.region,\n      endpoint: this.config_.endpoint,\n      ...this.config_.additionalClientConfig,\n    }\n\n    return new S3Client(config)\n  }\n\n  async upload(\n    file: FileTypes.ProviderUploadFileDTO\n  ): Promise<FileTypes.ProviderFileResultDTO> {\n    if (!file) {\n      throw new MedusaError(MedusaError.Types.INVALID_DATA, `No file provided`)\n    }\n\n    if (!file.filename) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `No filename provided`\n      )\n    }\n\n    const parsedFilename = path.parse(file.filename)\n\n    // TODO: Allow passing a full path for storage per request, not as a global config.\n    const fileKey = `${this.config_.prefix}${parsedFilename.name}-${ulid()}${\n      parsedFilename.ext\n    }`\n\n    let content: Buffer\n    try {\n      const decoded = Buffer.from(file.content, \"base64\")\n      if (decoded.toString(\"base64\") === file.content) {\n        content = decoded\n      } else {\n        content = Buffer.from(file.content, \"utf8\")\n      }\n    } catch {\n      // Last-resort fallback: binary\n      content = Buffer.from(file.content, \"binary\")\n    }\n\n    const command = new PutObjectCommand({\n      // We probably also want to support a separate bucket altogether for private files\n      // protected private_bucket_: string\n      // protected private_access_key_id_: string\n      // protected private_secret_access_key_: string\n\n      ACL: file.access === \"public\" ? \"public-read\" : \"private\",\n      Bucket: this.config_.bucket,\n      Body: content,\n      Key: fileKey,\n      ContentType: file.mimeType,\n      CacheControl: this.config_.cacheControl,\n      // Note: We could potentially set the content disposition when uploading,\n      // but storing the original filename as metadata should suffice.\n      Metadata: {\n        \"x-amz-meta-original-filename\": file.filename,\n      },\n    })\n\n    try {\n      await this.client_.send(command)\n    } catch (e) {\n      this.logger_.error(e)\n      throw e\n    }\n\n    return {\n      url: `${this.config_.fileUrl}/${fileKey}`,\n      key: fileKey,\n    }\n  }\n\n  async delete(\n    files: FileTypes.ProviderDeleteFileDTO | FileTypes.ProviderDeleteFileDTO[]\n  ): Promise<void> {\n    try {\n      /**\n       * Bulk delete files\n       */\n      if (Array.isArray(files)) {\n        await this.client_.send(\n          new DeleteObjectsCommand({\n            Bucket: this.config_.bucket,\n            Delete: {\n              Objects: files.map((file) => ({\n                Key: file.fileKey,\n              })),\n              Quiet: true,\n            },\n          })\n        )\n      } else {\n        await this.client_.send(\n          new DeleteObjectCommand({\n            Bucket: this.config_.bucket,\n            Key: files.fileKey,\n          })\n        )\n      }\n    } catch (e) {\n      // TODO: Rethrow depending on the error (eg. a file not found error is fine, but a failed request should be rethrown)\n      this.logger_.error(e)\n    }\n  }\n\n  async getPresignedDownloadUrl(\n    fileData: FileTypes.ProviderGetFileDTO\n  ): Promise<string> {\n    // TODO: Allow passing content disposition when getting a presigned URL\n    const command = new GetObjectCommand({\n      Bucket: this.config_.bucket,\n      Key: `${fileData.fileKey}`,\n    })\n\n    return await getSignedUrl(this.client_, command, {\n      expiresIn: this.config_.downloadFileDuration,\n    })\n  }\n\n  // Note: Some providers (eg. AWS S3) allows IAM policies to further restrict what can be uploaded.\n  async getPresignedUploadUrl(\n    fileData: FileTypes.ProviderGetPresignedUploadUrlDTO\n  ): Promise<FileTypes.ProviderFileResultDTO> {\n    if (!fileData?.filename) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `No filename provided`\n      )\n    }\n\n    const fileKey = `${this.config_.prefix}${fileData.filename}`\n\n    let acl: ObjectCannedACL | undefined\n    if (fileData.access) {\n      acl = fileData.access === \"public\" ? \"public-read\" : \"private\"\n    }\n\n    // Using content-type, acl, etc. doesn't work with all providers, and some simply ignore it.\n    const command = new PutObjectCommand({\n      Bucket: this.config_.bucket,\n      ContentType: fileData.mimeType,\n      ACL: acl,\n      Key: fileKey,\n    })\n\n    const signedUrl = await getSignedUrl(this.client_, command, {\n      expiresIn:\n        fileData.expiresIn ?? DEFAULT_UPLOAD_EXPIRATION_DURATION_SECONDS,\n    })\n\n    return {\n      url: signedUrl,\n      key: fileKey,\n    }\n  }\n\n  async getDownloadStream(\n    file: FileTypes.ProviderGetFileDTO\n  ): Promise<Readable> {\n    if (!file?.fileKey) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `No fileKey provided`\n      )\n    }\n\n    const fileKey = file.fileKey\n    const response = await this.client_.send(\n      new GetObjectCommand({\n        Key: fileKey,\n        Bucket: this.config_.bucket,\n      })\n    )\n\n    return response.Body! as Readable\n  }\n\n  async getAsBuffer(file: FileTypes.ProviderGetFileDTO): Promise<Buffer> {\n    if (!file?.fileKey) {\n      throw new MedusaError(\n        MedusaError.Types.INVALID_DATA,\n        `No fileKey provided`\n      )\n    }\n\n    const fileKey = file.fileKey\n    const response = await this.client_.send(\n      new GetObjectCommand({\n        Key: fileKey,\n        Bucket: this.config_.bucket,\n      })\n    )\n\n    return Buffer.from(await response.Body!.transformToByteArray())\n  }\n}",
            "cost": 0.078825
        }
    },
    "recover_status": "success",
    "instance_ref": {
        "instance_id": "medusajs__medusa.56ed9cf9.14209",
        "repo": "medusajs/medusa",
        "base_commit": "765232948900b7be98fb2cef1a9c8caf108a2d1e",
        "head_commit": "6e2b4fb880d497cda9ede2201870989363a67e46",
        "title": "escape non-ascii characters in filenames in s3 file provider",
        "merged_at": "2025-12-04T17:37:56Z",
        "html_url": "https://github.com/medusajs/medusa/pull/14209",
        "test_files": [
            "packages/modules/providers/file-s3/integration-tests/__tests__/services.spec.ts"
        ],
        "code_files": [
            "packages/modules/providers/file-s3/src/services/s3-file.ts"
        ],
        "total_changes": 28,
        "num_files": 3,
        "pull_number": 14209,
        "patch": "diff --git a/.changeset/shiny-hounds-learn.md b/.changeset/shiny-hounds-learn.md\nnew file mode 100644\nindex 0000000000000..df370ca61eba5\n--- /dev/null\n+++ b/.changeset/shiny-hounds-learn.md\n@@ -0,0 +1,5 @@\n+---\n+\"@medusajs/file-s3\": patch\n+---\n+\n+URL-encode S3 object metadata and returned URL\ndiff --git a/packages/modules/providers/file-s3/integration-tests/__tests__/services.spec.ts b/packages/modules/providers/file-s3/integration-tests/__tests__/services.spec.ts\nindex d874e88c3a21f..50fa4ca54bded 100644\n--- a/packages/modules/providers/file-s3/integration-tests/__tests__/services.spec.ts\n+++ b/packages/modules/providers/file-s3/integration-tests/__tests__/services.spec.ts\n@@ -1,5 +1,5 @@\n-import fs from \"fs/promises\"\n import axios from \"axios\"\n+import fs from \"fs/promises\"\n import { S3FileService } from \"../../src/services/s3-file\"\n jest.setTimeout(100000)\n \n@@ -82,6 +82,23 @@ describe.skip(\"S3 File Plugin\", () => {\n     })\n   })\n \n+  it(\"uploads a file with non-ascii characters in the name\", async () => {\n+    const fileContent = await fs.readFile(fixtureImagePath)\n+    const fixtureAsBinary = fileContent.toString(\"base64\")\n+\n+    const resp = await s3Service.upload({\n+      filename: \"catphoto-\u304b.jpg\",\n+      mimeType: \"image/jpeg\",\n+      content: fixtureAsBinary,\n+      access: \"private\",\n+    })\n+\n+    expect(resp).toEqual({\n+      key: expect.stringMatching(/tests\\/catphoto-\u304b.*\\.jpg/),\n+      url: expect.stringMatching(/https:\\/\\/.*\\/catphoto-%E3%81%8B.*\\.jpg/),\n+    })\n+  })\n+\n   it(\"gets a presigned upload URL and uploads a file successfully\", async () => {\n     const fileContent = await fs.readFile(fixtureImagePath)\n     const fixtureAsBinary = fileContent.toString(\"binary\")\ndiff --git a/packages/modules/providers/file-s3/src/services/s3-file.ts b/packages/modules/providers/file-s3/src/services/s3-file.ts\nindex 4caedd5587468..ef3b8be1352f2 100644\n--- a/packages/modules/providers/file-s3/src/services/s3-file.ts\n+++ b/packages/modules/providers/file-s3/src/services/s3-file.ts\n@@ -148,7 +148,7 @@ export class S3FileService extends AbstractFileProviderService {\n       // Note: We could potentially set the content disposition when uploading,\n       // but storing the original filename as metadata should suffice.\n       Metadata: {\n-        \"x-amz-meta-original-filename\": file.filename,\n+        \"original-filename\": encodeURIComponent(file.filename),\n       },\n     })\n \n@@ -160,7 +160,7 @@ export class S3FileService extends AbstractFileProviderService {\n     }\n \n     return {\n-      url: `${this.config_.fileUrl}/${fileKey}`,\n+      url: `${this.config_.fileUrl}/${encodeURI(fileKey)}`,\n       key: fileKey,\n     }\n   }\n",
        "pr_mirror": "medusajs__medusa.56ed9cf9"
    }
}