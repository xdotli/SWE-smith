diff --git a/plugin-server/src/ingestion/ingestion-e2e.test.ts b/plugin-server/src/ingestion/ingestion-e2e.test.ts
index 453bd35..4561285 100644
--- a/plugin-server/src/ingestion/ingestion-e2e.test.ts
+++ b/plugin-server/src/ingestion/ingestion-e2e.test.ts
@@ -401,2173 +401,4 @@ describe('Event Pipeline E2E tests', () => {
 
         expect(hub.groupRepository.fetchGroup).toHaveBeenCalledTimes(1)
         expect(hub.groupRepository.insertGroup).toHaveBeenCalledTimes(1)
-        expect(hub.groupRepository.updateGroup).toHaveBeenCalledTimes(0)
-        expect(hub.groupRepository.updateGroupOptimistically).toHaveBeenCalledTimes(1)
-
-        await waitForExpect(async () => {
-            const group = await hub.groupRepository.fetchGroup(team.id, 0, groupKey)
-            expect(group).toEqual(
-                expect.objectContaining({
-                    team_id: team.id,
-                    group_type_index: 0,
-                    group_properties: { k1: 'v1', k2: 'v3', k3: 'v2', k4: 'v3' },
-                    group_key: groupKey,
-                    // Just one write after the creation of the group
-                    version: 2,
-                })
-            )
-        })
-    })
-
-    testWithTeamIngester('can handle $groupidentify with no properties', {}, async (ingester, hub, team) => {
-        const events = [new EventBuilder(team).withEvent('$groupidentify').withProperties({}).build()]
-
-        await ingester.handleKafkaBatch(createKafkaMessages(events))
-
-        await waitForKafkaMessages(hub)
-
-        await waitForExpect(async () => {
-            const events = await fetchEvents(hub, team.id)
-            expect(events.length).toEqual(1)
-            expect(events[0].event).toEqual('$groupidentify')
-            expect(events[0].properties).toEqual({})
-        })
-    })
-
-    testWithTeamIngester(
-        'can handle multiple $groupidentify for different distinct ids',
-        {},
-        async (ingester, hub, team) => {
-            const n = 50
-            const distinctIds = []
-            for (let i = 0; i < n; i++) {
-                distinctIds.push(new UUIDT().toString())
-            }
-
-            const events = []
-            for (const distinctId of distinctIds) {
-                events.push(
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$groupidentify')
-                        .withGroupProperties('organization', distinctId, { foo: 'bar' })
-                        .build()
-                )
-                events.push(
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$groupidentify')
-                        .withGroupProperties('organization', distinctId, { update: 'new' })
-                        .build()
-                )
-            }
-
-            // handle 100 events in one batch
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(n * 2)
-            })
-
-            for (const distinctId of distinctIds) {
-                await waitForExpect(async () => {
-                    const group = await hub.groupRepository.fetchGroup(team.id, 0, distinctId)
-                    expect(group).toEqual(
-                        expect.objectContaining({
-                            team_id: team.id,
-                            group_type_index: 0,
-                            group_properties: { foo: 'bar', update: 'new' },
-                            version: 2,
-                        })
-                    )
-                })
-            }
-        }
-    )
-
-    testWithTeamIngester(
-        'can handle multiple $groupidentify for different distinct ids',
-        {},
-        async (ingester, hub, team) => {
-            const n = 50
-            const distinctIds = []
-            for (let i = 0; i < n; i++) {
-                distinctIds.push(new UUIDT().toString())
-            }
-
-            const events = []
-            for (const distinctId of distinctIds) {
-                events.push(
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$groupidentify')
-                        .withGroupProperties('organization', distinctId, { foo: 'bar' })
-                        .build()
-                )
-                events.push(
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$groupidentify')
-                        .withGroupProperties('organization', distinctId, { update: 'new' })
-                        .build()
-                )
-            }
-
-            // handle 100 events in one batch
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(n * 2)
-            })
-
-            for (const distinctId of distinctIds) {
-                await waitForExpect(async () => {
-                    const group = await hub.groupRepository.fetchGroup(team.id, 0, distinctId)
-                    expect(group).toEqual(
-                        expect.objectContaining({
-                            team_id: team.id,
-                            group_type_index: 0,
-                            group_properties: { foo: 'bar', update: 'new' },
-                            version: 2,
-                        })
-                    )
-                })
-            }
-        }
-    )
-
-    testWithTeamIngester(
-        'can $set and update person properties when reading event',
-        {},
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            const timestamp = DateTime.now().toMillis()
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$identify')
-                        .withProperties({
-                            $set: { prop: 'value' },
-                        })
-                        .withTimestamp(timestamp)
-                        .build(),
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$identify')
-                        .withProperties({
-                            $set: { prop: 'updated value' },
-                        })
-                        .withTimestamp(timestamp + 1)
-                        .build(),
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$identify')
-                        .withProperties({
-                            $set: { value: 'new value' },
-                        })
-                        .withTimestamp(timestamp + 2)
-                        .build(),
-                ])
-            )
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(3)
-                expect(events[0].event).toEqual('$identify')
-                expect(events[0].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-                expect(events[1].event).toEqual('$identify')
-                expect(events[1].person_properties).toEqual(expect.objectContaining({ prop: 'updated value' }))
-                expect(events[2].event).toEqual('$identify')
-                expect(events[2].person_properties).toEqual(
-                    expect.objectContaining({ prop: 'updated value', value: 'new value' })
-                )
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        '$identify and $set events force person property updates even for filtered properties',
-        {},
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            const timestamp = DateTime.now().toMillis()
-
-            // Chain of events: normal event with filtered props, then $identify with filtered props, then another normal event
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    // Event 1: Normal pageview with filtered properties (should be ignored on its own)
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$pageview')
-                        .withProperties({
-                            $set: { $browser: 'Chrome', utm_source: 'google' },
-                        })
-                        .withTimestamp(timestamp)
-                        .build(),
-                    // Event 2: $identify with ONLY filtered properties (should force update)
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$identify')
-                        .withProperties({
-                            $set: { $browser: 'Safari', $geoip_city_name: 'San Francisco' },
-                        })
-                        .withTimestamp(timestamp + 1)
-                        .build(),
-                    // Event 3: Another normal event with filtered properties
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$pageview')
-                        .withProperties({
-                            $set: { utm_source: 'facebook' },
-                        })
-                        .withTimestamp(timestamp + 2)
-                        .build(),
-                ])
-            )
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(3)
-
-                // Event 0 (first pageview): Should have properties from first event only
-                expect(events[0].person_properties).toEqual({
-                    $browser: 'Chrome',
-                    $creator_event_uuid: events[0].uuid,
-                    utm_source: 'google',
-                })
-
-                // Event 1 ($identify): Should have accumulated properties from first two events
-                expect(events[1].person_properties).toEqual({
-                    $browser: 'Safari', // Updated by $identify
-                    $creator_event_uuid: events[0].uuid,
-                    utm_source: 'google', // Still from first event
-                    $geoip_city_name: 'San Francisco', // Added by $identify
-                })
-
-                // Event 2 (last pageview): Should have all accumulated properties
-                expect(events[2].person_properties).toEqual({
-                    $browser: 'Safari', // From $identify
-                    $creator_event_uuid: events[0].uuid,
-                    utm_source: 'facebook', // Updated by this event
-                    $geoip_city_name: 'San Francisco', // From $identify
-                })
-
-                // Verify the final state of the person in the database
-                const person = await hub.personRepository.fetchPerson(team.id, distinctId)
-                expect(person).toBeDefined()
-                expect(person!.properties).toEqual({
-                    $browser: 'Safari',
-                    $creator_event_uuid: events[0].uuid,
-                    utm_source: 'facebook',
-                    $geoip_city_name: 'San Francisco',
-                })
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        '$set events force person property updates even for filtered properties',
-        {},
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            const timestamp = DateTime.now().toMillis()
-
-            // Similar test but with $set event instead of $identify
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    // Event 1: Normal event with filtered properties
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$pageview')
-                        .withProperties({
-                            $set: { $browser: 'Chrome' },
-                        })
-                        .withTimestamp(timestamp)
-                        .build(),
-                    // Event 2: $set event with filtered properties (should force update)
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$set')
-                        .withProperties({
-                            $set: { utm_source: 'twitter', $geoip_country_code: 'US' },
-                        })
-                        .withTimestamp(timestamp + 1)
-                        .build(),
-                ])
-            )
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(2)
-
-                // Event 0 (first pageview): Should have properties from first event only
-                expect(events[0].person_properties).toEqual({
-                    $browser: 'Chrome',
-                    $creator_event_uuid: events[0].uuid,
-                })
-
-                // Event 1 ($set): Should have accumulated properties from both events
-                expect(events[1].person_properties).toEqual({
-                    $browser: 'Chrome', // From first event
-                    $creator_event_uuid: events[0].uuid,
-                    utm_source: 'twitter', // From $set event
-                    $geoip_country_code: 'US', // From $set event
-                })
-
-                // Verify the final state of the person in the database
-                const person = await hub.personRepository.fetchPerson(team.id, distinctId)
-                expect(person).toBeDefined()
-                expect(person!.properties).toEqual({
-                    $browser: 'Chrome',
-                    $creator_event_uuid: events[0].uuid,
-                    utm_source: 'twitter',
-                    $geoip_country_code: 'US',
-                })
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'PERSON_PROPERTIES_UPDATE_ALL flag enables updates for normally filtered properties',
-        { pluginServerConfig: { PERSON_PROPERTIES_UPDATE_ALL: true } },
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            const timestamp = DateTime.now().toMillis()
-
-            // With PERSON_PROPERTIES_UPDATE_ALL=true, even normal pageview events should update
-            // filtered properties like $browser, $geoip_*, etc.
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    // Event 1: Normal pageview that creates the person with initial properties
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$pageview')
-                        .withProperties({
-                            $set: { $browser: 'Chrome', $geoip_city_name: 'New York' },
-                        })
-                        .withTimestamp(timestamp)
-                        .build(),
-                    // Event 2: Another normal pageview with updated filtered properties
-                    // With the flag enabled, these should trigger a person update
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$pageview')
-                        .withProperties({
-                            $set: { $browser: 'Safari', $geoip_city_name: 'San Francisco' },
-                        })
-                        .withTimestamp(timestamp + 1)
-                        .build(),
-                ])
-            )
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(2)
-
-                // Event 0 (first pageview): Should have initial properties
-                expect(events[0].person_properties).toEqual({
-                    $browser: 'Chrome',
-                    $creator_event_uuid: events[0].uuid,
-                    $geoip_city_name: 'New York',
-                })
-
-                // Event 1 (second pageview): Should have UPDATED properties
-                // This is the key difference - without the flag, these would still show the old values
-                expect(events[1].person_properties).toEqual({
-                    $browser: 'Safari',
-                    $creator_event_uuid: events[0].uuid,
-                    $geoip_city_name: 'San Francisco',
-                })
-
-                // Verify the final state of the person in the database reflects the updates
-                const person = await hub.personRepository.fetchPerson(team.id, distinctId)
-                expect(person).toBeDefined()
-                expect(person!.properties).toEqual({
-                    $browser: 'Safari',
-                    $creator_event_uuid: events[0].uuid,
-                    $geoip_city_name: 'San Francisco',
-                })
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'allowed geoip properties ($geoip_country_name, $geoip_city_name) trigger person updates alongside blocked geoip properties',
-        {},
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            const timestamp = DateTime.now().toMillis()
-
-            // When $geoip_country_name or $geoip_city_name changes, all geoip properties in the batch
-            // should be updated, even the normally-blocked ones like $geoip_latitude
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    // Event 1: Create person with initial geoip properties
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$pageview')
-                        .withProperties({
-                            $set: {
-                                $geoip_country_name: 'Canada',
-                                $geoip_city_name: 'Toronto',
-                                $geoip_latitude: 43.6532,
-                                $geoip_longitude: -79.3832,
-                            },
-                        })
-                        .withTimestamp(timestamp)
-                        .build(),
-                    // Event 2: Update geoip properties including allowed ones (country/city)
-                    // Since $geoip_country_name changes, all geoip properties should be updated
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$pageview')
-                        .withProperties({
-                            $set: {
-                                $geoip_country_name: 'United States',
-                                $geoip_city_name: 'San Francisco',
-                                $geoip_latitude: 37.7749,
-                                $geoip_longitude: -122.4194,
-                            },
-                        })
-                        .withTimestamp(timestamp + 1)
-                        .build(),
-                ])
-            )
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(2)
-
-                // Event 0 (first pageview): Should have initial geoip properties
-                expect(events[0].person_properties).toEqual({
-                    $creator_event_uuid: events[0].uuid,
-                    $geoip_country_name: 'Canada',
-                    $geoip_city_name: 'Toronto',
-                    $geoip_latitude: 43.6532,
-                    $geoip_longitude: -79.3832,
-                })
-
-                // Event 1 (second pageview): Should have UPDATED geoip properties
-                // Because $geoip_country_name is an allowed property, all geoip properties get updated
-                expect(events[1].person_properties).toEqual({
-                    $creator_event_uuid: events[0].uuid,
-                    $geoip_country_name: 'United States',
-                    $geoip_city_name: 'San Francisco',
-                    $geoip_latitude: 37.7749,
-                    $geoip_longitude: -122.4194,
-                })
-
-                // Verify the final state of the person in the database reflects the updates
-                const person = await hub.personRepository.fetchPerson(team.id, distinctId)
-                expect(person).toBeDefined()
-                expect(person!.properties).toEqual({
-                    $creator_event_uuid: events[0].uuid,
-                    $geoip_country_name: 'United States',
-                    $geoip_city_name: 'San Francisco',
-                    $geoip_latitude: 37.7749,
-                    $geoip_longitude: -122.4194,
-                })
-            })
-        }
-    )
-
-    testWithTeamIngester('can handle events with $process_person_profile=false', {}, async (ingester, hub, team) => {
-        const distinctId = new UUIDT().toString()
-        const timestamp = DateTime.now().toMillis()
-        await ingester.handleKafkaBatch(
-            createKafkaMessages([
-                new EventBuilder(team, distinctId)
-                    .withEvent('$identify')
-                    .withProperties({
-                        distinct_id: distinctId,
-                        $set: { prop: 'value' },
-                    })
-                    .withTimestamp(timestamp)
-                    .build(),
-                new EventBuilder(team, distinctId)
-                    .withEvent('custom event')
-                    .withProperties({
-                        distinctId: distinctId,
-                        $process_person_profile: false,
-                        $group_0: 'group_key',
-                        $set: {
-                            c: 3,
-                        },
-                        $set_once: {
-                            d: 4,
-                        },
-                        $unset: ['prop'],
-                    })
-                    .withOverrides({
-                        $set: {
-                            a: 1,
-                        },
-                        $set_once: {
-                            b: 2,
-                        },
-                    })
-                    .withTimestamp(timestamp + 1)
-                    .build(),
-                new EventBuilder(team, distinctId)
-                    .withEvent('custom event')
-                    .withProperties({})
-                    .withTimestamp(timestamp + 2)
-                    .build(),
-            ])
-        )
-
-        await waitForExpect(async () => {
-            const events = await fetchEvents(hub, team.id)
-            expect(events.length).toEqual(3)
-            expect(events[0].event).toEqual('$identify')
-            expect(events[0].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-            expect(events[1].event).toEqual('custom event')
-            expect(events[1].person_properties).toEqual({})
-            expect(events[2].event).toEqual('custom event')
-            expect(events[2].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-        })
-    })
-
-    testWithTeamIngester(
-        'force_upgrade triggers when personless event sent after person creation',
-        {},
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            const timestamp = DateTime.now().toMillis()
-
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$identify')
-                        .withProperties({
-                            distinct_id: distinctId,
-                            $set: { prop: 'value' },
-                        })
-                        .withTimestamp(timestamp)
-                        .build(),
-                    new EventBuilder(team, distinctId)
-                        .withEvent('custom event')
-                        .withProperties({
-                            $process_person_profile: false,
-                        })
-                        .withTimestamp(timestamp + 120000) // 2 minutes later (beyond 1-minute grace period)
-                        .build(),
-                ])
-            )
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(2)
-                expect(events[0].event).toEqual('$identify')
-                expect(events[0].person_mode).toEqual('full')
-                expect(events[1].event).toEqual('custom event')
-                expect(events[1].person_mode).toEqual('force_upgrade')
-                expect(events[1].person_properties).toEqual({})
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'can $set and update person properties with top level $set',
-        {},
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    new EventBuilder(team, distinctId)
-                        .withEvent('$identify')
-                        .withProperties({
-                            distinct_id: distinctId,
-                        })
-                        .withOverrides({
-                            $set: { prop: 'value' },
-                        })
-                        .build(),
-                ])
-            )
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(1)
-                expect(events[0].event).toEqual('$identify')
-                expect(events[0].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'should guarantee that person properties are set in the order of the events',
-        {},
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            const timestamp = DateTime.now().toMillis()
-
-            const events = [
-                new EventBuilder(team, distinctId)
-                    .withEvent('$identify')
-                    .withProperties({ $set: { prop: 'value' } })
-                    .withTimestamp(timestamp)
-                    .build(),
-
-                new EventBuilder(team, distinctId)
-                    .withEvent('custom event')
-                    .withProperties({})
-                    .withTimestamp(timestamp + 1)
-                    .build(),
-
-                new EventBuilder(team, distinctId)
-                    .withEvent('custom event')
-                    .withProperties({
-                        $set: {
-                            prop: 'updated value',
-                            new_prop: 'new value',
-                        },
-                    })
-                    .withTimestamp(timestamp + 2)
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(3)
-                expect(events[0].event).toEqual('$identify')
-                expect(events[0].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-                expect(events[1].event).toEqual('custom event')
-                expect(events[1].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-                expect(events[2].event).toEqual('custom event')
-                expect(events[2].person_properties).toEqual(
-                    expect.objectContaining({ prop: 'updated value', new_prop: 'new value' })
-                )
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'should be able to $set_once person properties but not update',
-        {},
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            const events = [
-                new EventBuilder(team, distinctId)
-                    .withEvent('$identify')
-                    .withProperties({ $set_once: { prop: 'value' } })
-                    .build(),
-                new EventBuilder(team, distinctId)
-                    .withEvent('$identify')
-                    .withProperties({ $set_once: { prop: 'updated value' } })
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(2)
-                expect(events[0].event).toEqual('$identify')
-                expect(events[0].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-                expect(events[1].event).toEqual('$identify')
-                expect(events[1].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'should be able to $set_once person properties but not update, at the top level',
-        {},
-        async (ingester, hub, team) => {
-            const distinctId = new UUIDT().toString()
-            const events = [
-                new EventBuilder(team, distinctId)
-                    .withEvent('$identify')
-                    .withProperties({})
-                    .withOverrides({ $set_once: { prop: 'value' } })
-                    .build(),
-                new EventBuilder(team, distinctId)
-                    .withEvent('$identify')
-                    .withProperties({})
-                    .withOverrides({ $set_once: { prop: 'updated value' } })
-                    .build(),
-                new EventBuilder(team, distinctId).withEvent('custom event').withProperties({}).build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toEqual(3)
-                expect(events[0].event).toEqual('$identify')
-                expect(events[0].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-                expect(events[1].event).toEqual('$identify')
-                expect(events[1].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-                expect(events[2].event).toEqual('custom event')
-                expect(events[2].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-            })
-        }
-    )
-
-    testWithTeamIngester('should identify previous events with $anon_distinct_id', {}, async (ingester, hub, team) => {
-        const initialDistinctId = new UUIDT().toString()
-        const personIdentifier = 'test@posthog.com'
-
-        const events = [
-            new EventBuilder(team, initialDistinctId).withEvent('custom event').withProperties({}).build(),
-            new EventBuilder(team, personIdentifier)
-                .withEvent('$identify')
-                .withProperties({
-                    distinct_id: personIdentifier,
-                    $anon_distinct_id: initialDistinctId,
-                })
-                .build(),
-        ]
-
-        await ingester.handleKafkaBatch(createKafkaMessages(events))
-
-        await waitForExpect(async () => {
-            const events = await fetchEvents(hub, team.id)
-            expect(events.length).toEqual(2)
-            expect(events[0].person_id).toEqual(events[1].person_id)
-        })
-    })
-
-    testWithTeamIngester('should perserve all events if merge fails', {}, async (ingester, hub, team) => {
-        const illegalDistinctId = '0'
-        const distinctId = new UUIDT().toString()
-
-        const events = [
-            new EventBuilder(team, illegalDistinctId).withEvent('custom event').withProperties({}).build(),
-            new EventBuilder(team, distinctId).withEvent('custom event 2').withProperties({}).build(),
-        ]
-
-        await ingester.handleKafkaBatch(createKafkaMessages(events))
-
-        await waitForKafkaMessages(hub)
-
-        await waitForExpect(async () => {
-            const persons = await fetchPersons(hub, team.id)
-            expect(persons.length).toEqual(2)
-        })
-
-        const mergeEvents = [
-            new EventBuilder(team, distinctId)
-                .withEvent('$merge_dangerously')
-                .withProperties({
-                    distinct_id: distinctId,
-                    alias: illegalDistinctId,
-                    $set: { prop: 'value' },
-                })
-                .build(),
-        ]
-
-        await ingester.handleKafkaBatch(createKafkaMessages(mergeEvents))
-
-        await waitForExpect(async () => {
-            const events = await fetchEvents(hub, team.id)
-            expect(events.length).toEqual(3)
-            // Assert that there are 2 different persons in person_id column
-            const personIds = new Set(events.map((event) => event.person_id))
-            expect(personIds.size).toEqual(2)
-        })
-    })
-
-    testWithTeamIngester('should preserve properties if merge fails', {}, async (ingester, hub, team) => {
-        const illegalDistinctId = '0'
-        const distinctId = new UUIDT().toString()
-        await ingester.handleKafkaBatch(
-            createKafkaMessages([
-                new EventBuilder(team, distinctId)
-                    .withEvent('$merge_dangerously')
-                    .withProperties({
-                        distinct_id: distinctId,
-                        alias: illegalDistinctId,
-                        $set: { prop: 'value' },
-                    })
-                    .build(),
-                new EventBuilder(team, distinctId).withEvent('custom event').withProperties({}).build(),
-            ])
-        )
-
-        await waitForExpect(async () => {
-            const events = await fetchEvents(hub, team.id)
-            expect(events.length).toEqual(2)
-            expect(events[1].person_properties).toEqual(expect.objectContaining({ prop: 'value' }))
-        })
-    })
-
-    testWithTeamIngester('should merge all events into same person id', {}, async (ingester, hub, team) => {
-        const initialDistinctId = 'id1'
-        const secondDistinctId = 'id2'
-        const personIdentifier = 'person_id'
-
-        const event1 = new EventBuilder(team, initialDistinctId).withEvent('custom event').withProperties({}).build()
-        const event2 = new EventBuilder(team, secondDistinctId).withEvent('custom event 2').withProperties({}).build()
-
-        await ingester.handleKafkaBatch(createKafkaMessages([event1, event2]))
-
-        await waitForKafkaMessages(hub)
-
-        await waitForExpect(async () => {
-            const persons = await fetchPersons(hub, team.id)
-            expect(persons).toEqual(
-                expect.arrayContaining([
-                    expect.objectContaining({
-                        properties: expect.objectContaining({
-                            $creator_event_uuid: event1.uuid,
-                        }),
-                    }),
-                    expect.objectContaining({
-                        properties: expect.objectContaining({
-                            $creator_event_uuid: event2.uuid,
-                        }),
-                    }),
-                ])
-            )
-        })
-
-        await ingester.handleKafkaBatch(
-            createKafkaMessages([
-                new EventBuilder(team, personIdentifier)
-                    .withEvent('$identify')
-                    .withProperties({
-                        distinct_id: personIdentifier,
-                        $anon_distinct_id: initialDistinctId,
-                    })
-                    .build(),
-                new EventBuilder(team, personIdentifier)
-                    .withEvent('$identify')
-                    .withProperties({
-                        distinct_id: personIdentifier,
-                        $anon_distinct_id: secondDistinctId,
-                    })
-                    .build(),
-            ])
-        )
-
-        await waitForKafkaMessages(hub)
-
-        await waitForExpect(async () => {
-            const events = await fetchEvents(hub, team.id)
-            expect(events.length).toEqual(4)
-            // assert all events have the same person_id
-            const personIds = new Set(events.map((event) => event.person_id))
-            expect(personIds.size).toEqual(1)
-        })
-    })
-
-    testWithTeamIngester('should resolve to same person id chained merges', {}, async (ingester, hub, team) => {
-        const initialDistinctId = 'initialId'
-        const secondDistinctId = 'secondId'
-        const thirdDistinctId = 'thirdId'
-
-        await ingester.handleKafkaBatch(
-            createKafkaMessages([
-                new EventBuilder(team, initialDistinctId).withEvent('custom event').withProperties({}).build(),
-                new EventBuilder(team, secondDistinctId).withEvent('custom event 2').withProperties({}).build(),
-                new EventBuilder(team, thirdDistinctId).withEvent('custom event 3').withProperties({}).build(),
-            ])
-        )
-
-        await waitForKafkaMessages(hub)
-
-        await waitForExpect(async () => {
-            const events = await fetchEvents(hub, team.id)
-            expect(events.length).toEqual(3)
-            expect(new Set(events.map((event) => event.person_id)).size).toEqual(3)
-        })
-
-        await ingester.handleKafkaBatch(
-            createKafkaMessages([
-                new EventBuilder(team, initialDistinctId)
-                    .withEvent('$identify')
-                    .withProperties({
-                        distinct_id: initialDistinctId,
-                        $anon_distinct_id: secondDistinctId,
-                    })
-                    .build(),
-                new EventBuilder(team, initialDistinctId)
-                    .withEvent('$identify')
-                    .withProperties({
-                        distinct_id: initialDistinctId,
-                        $anon_distinct_id: thirdDistinctId,
-                    })
-                    .build(),
-            ])
-        )
-
-        await waitForKafkaMessages(hub)
-
-        await waitForExpect(async () => {
-            const events = await fetchEvents(hub, team.id)
-            expect(events.length).toEqual(5)
-            expect(new Set(events.map((event) => event.person_id)).size).toEqual(1)
-        })
-    })
-
-    testWithTeamIngester(
-        'should resolve to same person id even with complex chained merges',
-        {},
-        async (ingester, hub, team) => {
-            const initialDistinctId = new UUIDT().toString()
-            const secondDistinctId = new UUIDT().toString()
-            const thirdDistinctId = new UUIDT().toString()
-            const forthDistinctId = new UUIDT().toString()
-
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    new EventBuilder(team, initialDistinctId).withEvent('custom event').withProperties({}).build(),
-                    new EventBuilder(team, secondDistinctId).withEvent('custom event 2').withProperties({}).build(),
-                    new EventBuilder(team, thirdDistinctId).withEvent('custom event 3').withProperties({}).build(),
-                    new EventBuilder(team, forthDistinctId).withEvent('custom event 4').withProperties({}).build(),
-                ])
-            )
-
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const persons = await fetchPersons(hub, team.id)
-                expect(persons.length).toBe(4)
-            })
-
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    new EventBuilder(team, initialDistinctId)
-                        .withEvent('$identify')
-                        .withProperties({
-                            distinct_id: initialDistinctId,
-                            $anon_distinct_id: secondDistinctId,
-                        })
-                        .build(),
-                    new EventBuilder(team, thirdDistinctId)
-                        .withEvent('$identify')
-                        .withProperties({
-                            distinct_id: thirdDistinctId,
-                            $anon_distinct_id: forthDistinctId,
-                        })
-                        .build(),
-                ])
-            )
-
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toBe(6)
-            })
-
-            await ingester.handleKafkaBatch(
-                createKafkaMessages([
-                    new EventBuilder(team, secondDistinctId)
-                        .withEvent('$merge_dangerously')
-                        .withProperties({
-                            distinct_id: secondDistinctId,
-                            alias: thirdDistinctId,
-                        })
-                        .build(),
-                ])
-            )
-
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toBe(7)
-                expect(new Set(events.map((event) => event.person_id)).size).toBe(1)
-            })
-        }
-    )
-
-    testWithTeamIngester('should produce ingestion warnings for messages over 1MB', {}, async (ingester, hub, team) => {
-        // For this we basically want the plugin-server to try and produce a new
-        // message larger than 1MB. We do this by creating a person with a lot of
-        // properties. We will end up denormalizing the person properties onto the
-        // event, which already has the properties as $set therefore resulting in a
-        // message that's larger than 1MB. There may also be other attributes that
-        // are added to the event which pushes it over the limit.
-        //
-        // We verify that this is handled by checking that there is a message in the
-        // appropriate topic.
-        const distinctId = new UUIDT().toString()
-
-        const personProperties = {
-            distinct_id: distinctId,
-            $set: {} as Record<string, string>,
-        }
-
-        for (let i = 0; i < 10000; i++) {
-            personProperties.$set[new UUIDT().toString()] = new UUIDT().toString()
-        }
-
-        const events = [
-            new EventBuilder(team, distinctId).withEvent('$identify').withProperties(personProperties).build(),
-        ]
-
-        await ingester.handleKafkaBatch(createKafkaMessages(events))
-
-        await waitForKafkaMessages(hub)
-
-        await waitForExpect(async () => {
-            const ingestionWarnings = await fetchIngestionWarnings(hub, team.id)
-            expect(ingestionWarnings.length).toBe(1)
-            expect(ingestionWarnings[0].details.eventUuid).toBe(events[0].uuid)
-        })
-    })
-
-    const fetchPersons = async (hub: Hub, teamId: number) => {
-        const persons = await clickhouse.fetchPersons(teamId)
-        return persons.map((person) => ({
-            ...person,
-            properties: parseJSON(person.properties),
-        }))
-    }
-
-    const fetchEvents = async (hub: Hub, teamId: number) => {
-        // Force ClickHouse to merge parts to ensure FINAL consistency with retry logic
-        await retryClickHouseOperation(
-            () => clickhouse.exec(`OPTIMIZE TABLE person_distinct_id_overrides FINAL`),
-            'OPTIMIZE TABLE person_distinct_id_overrides FINAL',
-            3, // max retries
-            false // non-fatal operation
-        )
-
-        // Query events with retry logic for connection stability
-        const queryResult = (await retryClickHouseOperation(
-            () =>
-                clickhouse.query(`
-                SELECT *,
-                       if(notEmpty(overrides.person_id), overrides.person_id, e.person_id) as person_id
-                FROM events e
-                FINAL
-                LEFT OUTER JOIN (
-                    SELECT
-                        distinct_id,
-                        argMax(person_id, version) as person_id
-                      FROM person_distinct_id_overrides
-                      FINAL
-                      WHERE team_id = ${teamId}
-                      GROUP BY distinct_id
-                ) AS overrides USING distinct_id
-                WHERE team_id = ${teamId}
-                ORDER BY timestamp ASC
-            `),
-            'fetchEvents query',
-            3, // max retries
-            true // fatal operation
-        )) as unknown as RawClickHouseEvent[]
-
-        return queryResult.map(parseRawClickHouseEvent)
-    }
-
-    // Utility function for retrying ClickHouse operations with exponential backoff
-    const retryClickHouseOperation = async <T>(
-        operation: () => Promise<T>,
-        operationName: string,
-        maxRetries: number = 3,
-        throwOnFailure: boolean = true
-    ): Promise<T | null> => {
-        let lastError: Error | null = null
-
-        for (let attempt = 1; attempt <= maxRetries; attempt++) {
-            try {
-                return await operation()
-            } catch (error: any) {
-                lastError = error
-
-                const isSocketError =
-                    error?.message?.includes('socket hang up') ||
-                    error?.message?.includes('ECONNRESET') ||
-                    error?.message?.includes('ETIMEDOUT')
-
-                if (isSocketError && attempt < maxRetries) {
-                    const backoffMs = Math.min(1000 * Math.pow(2, attempt - 1), 10000) // Exponential backoff, max 10s
-                    console.warn(
-                        `[DEBUG] ClickHouse ${operationName} failed (attempt ${attempt}/${maxRetries}), retrying in ${backoffMs}ms:`,
-                        error?.message
-                    )
-                    await new Promise((resolve) => setTimeout(resolve, backoffMs))
-                    continue
-                }
-
-                console.warn(
-                    `[DEBUG] ClickHouse ${operationName} failed (attempt ${attempt}/${maxRetries}):`,
-                    error?.message
-                )
-                break
-            }
-        }
-
-        if (throwOnFailure && lastError) {
-            throw lastError
-        } else if (lastError) {
-            console.warn(
-                `[DEBUG] ClickHouse ${operationName} failed after all retries (non-fatal):`,
-                lastError?.message
-            )
-            return null
-        }
-
-        return null
-    }
-
-    const fetchIngestionWarnings = async (hub: Hub, teamId: number) => {
-        const queryResult = (await retryClickHouseOperation(
-            () =>
-                clickhouse.query(`
-                SELECT *
-                FROM ingestion_warnings
-                WHERE team_id = ${teamId}
-            `),
-            'fetchIngestionWarnings query',
-            3, // max retries
-            true // fatal operation
-        )) as any[]
-
-        return queryResult.map((warning: any) => ({ ...warning, details: parseJSON(warning.details) }))
-    }
-
-    // TODO: Re-enable after re-adding FK constraints to posthog_persondistinctid
-    // testWithTeamIngester('alias events ordering scenario 1: original order', {}, async (ingester, hub, team) => {
-    //     const testName = DateTime.now().toFormat('yyyy-MM-dd-HH-mm-ss')
-    //     const user1DistinctId = 'user1-distinct-id'
-    //     const user2DistinctId = 'user2-distinct-id'
-    //     const user3DistinctId = 'user3-distinct-id'
-
-    //     const events = [
-    //         // User 1 creation
-    //         new EventBuilder(team, user1DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     name: 'User 1',
-    //                     email: `user1-${user1DistinctId}@example.com`,
-    //                     age: 30,
-    //                     test_name: testName,
-    //                 },
-    //             })
-    //             .build(),
-    //         new EventBuilder(team, user1DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     new_name: 'User 1 - Updated',
-    //                 },
-    //             })
-    //             .build(),
-    //         // User 2 creation
-    //         new EventBuilder(team, user2DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     name: 'User 2',
-    //                     email: `user2-${user2DistinctId}@example.com`,
-    //                     age: 30,
-    //                     test_name: testName,
-    //                 },
-    //             })
-    //             .build(),
-    //         new EventBuilder(team, user2DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     new_name: 'User 2 - Updated',
-    //                 },
-    //             })
-    //             .build(),
-    //         // Merge users: alias user1 -> user2
-    //         new EventBuilder(team, user1DistinctId)
-    //             .withEvent('$create_alias')
-    //             .withProperties({
-    //                 distinct_id: user1DistinctId,
-    //                 alias: user2DistinctId,
-    //             })
-    //             .build(),
-
-    //         // Create alias for user2 -> user3
-    //         new EventBuilder(team, user2DistinctId)
-    //             .withEvent('$create_alias')
-    //             .withProperties({
-    //                 distinct_id: user2DistinctId,
-    //                 alias: user3DistinctId,
-    //             })
-    //             .build(),
-    //     ]
-
-    //     await ingester.handleKafkaBatch(createKafkaMessages(events))
-    //     await waitForKafkaMessages(hub)
-
-    //     await waitForExpect(async () => {
-    //         const events = await fetchEvents(hub, team.id)
-    //         expect(events.length).toBe(6)
-
-    //         // TODO: Add specific assertions based on expected behavior
-    //         // All events should be processed without errors
-    //         expect(events).toBeDefined()
-    //     })
-
-    //     // fetch the person properties
-    //     await waitForExpect(async () => {
-    //         const persons = await fetchPostgresPersons(hub.db, team.id)
-    //         expect(persons.length).toBe(1)
-    //         const personsClickhouse = await fetchPersons(hub, team.id)
-    //         expect(personsClickhouse.length).toBe(1)
-    //         expect(persons[0].properties).toMatchObject(
-    //             expect.objectContaining({
-    //                 name: 'User 1',
-    //                 new_name: 'User 1 - Updated',
-    //                 email: `user1-${user1DistinctId}@example.com`,
-    //                 age: 30,
-    //                 test_name: testName,
-    //             })
-    //         )
-    //         expect(personsClickhouse[0].properties).toMatchObject(
-    //             expect.objectContaining({
-    //                 name: 'User 1',
-    //                 new_name: 'User 1 - Updated',
-    //                 email: `user1-${user1DistinctId}@example.com`,
-    //                 age: 30,
-    //                 test_name: testName,
-    //             })
-    //         )
-    //         const distinctIdsPersons = await fetchDistinctIds(hub.db.postgres, {
-    //             id: persons[0].id,
-    //             team_id: team.id,
-    //         } as InternalPerson)
-    //         expect(distinctIdsPersons.length).toBe(3)
-    //         // Except distinctids to match the ids, in any order
-    //         expect(distinctIdsPersons.map((distinctId) => distinctId.distinct_id)).toEqual(
-    //             expect.arrayContaining([user1DistinctId, user2DistinctId, user3DistinctId])
-    //         )
-    //     })
-    // })
-
-    // TODO: Re-enable after re-adding FK constraints to posthog_persondistinctid
-    // testWithTeamIngester('alias events ordering scenario 2: alias first', {}, async (ingester, hub, team) => {
-    //     const testName = DateTime.now().toFormat('yyyy-MM-dd-HH-mm-ss')
-    //     const user1DistinctId = 'user1-distinct-id'
-    //     const user2DistinctId = 'user2-distinct-id'
-    //     const user3DistinctId = 'user3-distinct-id'
-
-    //     const events = [
-    //         // User 1 creation
-    //         new EventBuilder(team, user1DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     name: 'User 1',
-    //                     email: `user1-${user1DistinctId}@example.com`,
-    //                     age: 30,
-    //                     test_name: testName,
-    //                 },
-    //             })
-    //             .build(),
-    //         new EventBuilder(team, user1DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     new_name: 'User 1 - Updated',
-    //                 },
-    //             })
-    //             .build(),
-    //         // User 2 creation
-    //         new EventBuilder(team, user2DistinctId)
-    //             .withProperties({
-    //                 anon_distinct_id: user2DistinctId,
-    //                 $set: {
-    //                     name: 'User 2',
-    //                     email: `user2-${user2DistinctId}@example.com`,
-    //                     age: 30,
-    //                     test_name: testName,
-    //                 },
-    //             })
-    //             .build(),
-    //         new EventBuilder(team, user2DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     new_name: 'User 2 - Updated',
-    //                 },
-    //             })
-    //             .build(),
-
-    //         // Create alias for user2 -> user3
-    //         new EventBuilder(team, user2DistinctId)
-    //             .withEvent('$create_alias')
-    //             .withProperties({
-    //                 distinct_id: user2DistinctId,
-    //                 alias: user3DistinctId,
-    //             })
-    //             .build(),
-
-    //         // Merge users: alias user1 -> user2
-    //         new EventBuilder(team, user1DistinctId)
-    //             .withEvent('$create_alias')
-    //             .withProperties({
-    //                 distinct_id: user1DistinctId,
-    //                 alias: user2DistinctId,
-    //             })
-    //             .build(),
-    //     ]
-
-    //     await ingester.handleKafkaBatch(createKafkaMessages(events))
-    //     await waitForKafkaMessages(hub)
-
-    //     await waitForExpect(async () => {
-    //         const events = await fetchEvents(hub, team.id)
-    //         expect(events.length).toBe(6)
-
-    //         // TODO: Add specific assertions based on expected behavior
-    //         // All events should be processed without errors
-    //         expect(events).toBeDefined()
-    //     })
-
-    //     // fetch the person properties
-    //     await waitForExpect(async () => {
-    //         const persons = await fetchPostgresPersons(hub.db, team.id)
-    //         expect(persons.length).toBe(1)
-    //         const personsClickhouse = await fetchPersons(hub, team.id)
-    //         expect(personsClickhouse.length).toBe(1)
-    //         expect(persons[0].properties).toMatchObject(
-    //             expect.objectContaining({
-    //                 name: 'User 1',
-    //                 new_name: 'User 1 - Updated',
-    //                 email: `user1-${user1DistinctId}@example.com`,
-    //                 age: 30,
-    //                 test_name: testName,
-    //             })
-    //         )
-    //         expect(personsClickhouse[0].properties).toMatchObject(
-    //             expect.objectContaining({
-    //                 name: 'User 1',
-    //                 new_name: 'User 1 - Updated',
-    //                 email: `user1-${user1DistinctId}@example.com`,
-    //                 age: 30,
-    //                 test_name: testName,
-    //             })
-    //         )
-    //         const distinctIdsPersons = await fetchDistinctIds(hub.db.postgres, {
-    //             id: persons[0].id,
-    //             team_id: team.id,
-    //         } as InternalPerson)
-    //         expect(distinctIdsPersons.length).toBe(3)
-    //         // Except distinctids to match the ids, in any order
-    //         expect(distinctIdsPersons.map((distinctId) => distinctId.distinct_id)).toEqual(
-    //             expect.arrayContaining([user1DistinctId, user2DistinctId, user3DistinctId])
-    //         )
-    //     })
-    // })
-
-    // TODO: Re-enable after re-adding FK constraints to posthog_persondistinctid
-    // testWithTeamIngester('alias events ordering scenario 2: user 2 first', {}, async (ingester, hub, team) => {
-    //     const testName = DateTime.now().toFormat('yyyy-MM-dd-HH-mm-ss')
-    //     const user1DistinctId = 'user1-distinct-id'
-    //     const user2DistinctId = 'user2-distinct-id'
-    //     const user3DistinctId = 'user3-distinct-id'
-
-    //     const events = [
-    //         // User 2 creation
-    //         new EventBuilder(team, user2DistinctId)
-    //             .withProperties({
-    //                 anon_distinct_id: user2DistinctId,
-    //                 $set: {
-    //                     name: 'User 2',
-    //                     email: `user2-${user2DistinctId}@example.com`,
-    //                     age: 30,
-    //                     test_name: testName,
-    //                 },
-    //             })
-    //             .build(),
-    //         new EventBuilder(team, user2DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     new_name: 'User 2 - Updated',
-    //                 },
-    //             })
-    //             .build(),
-
-    //         // Create alias for user2 -> user3
-    //         new EventBuilder(team, user2DistinctId)
-    //             .withEvent('$create_alias')
-    //             .withProperties({
-    //                 distinct_id: user2DistinctId,
-    //                 alias: user3DistinctId,
-    //             })
-    //             .build(),
-
-    //         // User 1 creation
-    //         new EventBuilder(team, user1DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     name: 'User 1',
-    //                     email: `user1-${user1DistinctId}@example.com`,
-    //                     age: 30,
-    //                     test_name: testName,
-    //                 },
-    //             })
-    //             .build(),
-    //         new EventBuilder(team, user1DistinctId)
-    //             .withEvent('$identify')
-    //             .withProperties({
-    //                 $set: {
-    //                     new_name: 'User 1 - Updated',
-    //                 },
-    //             })
-    //             .build(),
-
-    //         // Merge users: alias user1 -> user2
-    //         new EventBuilder(team, user1DistinctId)
-    //             .withEvent('$create_alias')
-    //             .withProperties({
-    //                 distinct_id: user1DistinctId,
-    //                 alias: user2DistinctId,
-    //             })
-    //             .build(),
-    //     ]
-
-    //     await ingester.handleKafkaBatch(createKafkaMessages(events))
-    //     await waitForKafkaMessages(hub)
-
-    //     await waitForExpect(async () => {
-    //         const events = await fetchEvents(hub, team.id)
-    //         expect(events.length).toBe(6)
-
-    //         // TODO: Add specific assertions based on expected behavior
-    //         // All events should be processed without errors
-    //         expect(events).toBeDefined()
-    //     })
-
-    //     // fetch the person properties
-    //     await waitForExpect(async () => {
-    //         const persons = await fetchPostgresPersons(hub.db, team.id)
-    //         expect(persons.length).toBe(1)
-    //         const personsClickhouse = await fetchPersons(hub, team.id)
-    //         expect(personsClickhouse.length).toBe(1)
-    //         expect(persons[0].properties).toMatchObject(
-    //             expect.objectContaining({
-    //                 name: 'User 1',
-    //                 new_name: 'User 1 - Updated',
-    //                 email: `user1-${user1DistinctId}@example.com`,
-    //                 age: 30,
-    //                 test_name: testName,
-    //             })
-    //         )
-    //         expect(personsClickhouse[0].properties).toMatchObject(
-    //             expect.objectContaining({
-    //                 name: 'User 1',
-    //                 new_name: 'User 1 - Updated',
-    //                 email: `user1-${user1DistinctId}@example.com`,
-    //                 age: 30,
-    //                 test_name: testName,
-    //             })
-    //         )
-    //         const distinctIdsPersons = await fetchDistinctIds(hub.db.postgres, {
-    //             id: persons[0].id,
-    //             team_id: team.id,
-    //         } as InternalPerson)
-    //         expect(distinctIdsPersons.length).toBe(3)
-    //         // Except distinctids to match the ids, in any order
-    //         expect(distinctIdsPersons.map((distinctId) => distinctId.distinct_id)).toEqual(
-    //             expect.arrayContaining([user1DistinctId, user2DistinctId, user3DistinctId])
-    //         )
-    //     })
-    // })
-
-    testWithTeamIngester(
-        'alias events ordering scenario 2: user 2 first, separate batch',
-        {},
-        async (ingester, hub, team) => {
-            const testName = DateTime.now().toFormat('yyyy-MM-dd-HH-mm-ss')
-            const user1DistinctId = 'user1-distinct-id'
-            const user2DistinctId = 'user2-distinct-id'
-            const user3DistinctId = 'user3-distinct-id'
-
-            const events = [
-                // User 2 creation
-                new EventBuilder(team, user2DistinctId)
-                    .withProperties({
-                        anon_distinct_id: user2DistinctId,
-                        $set: {
-                            name: 'User 2',
-                            email: `user2-${user2DistinctId}@example.com`,
-                            age: 30,
-                            test_name: testName,
-                        },
-                    })
-                    .build(),
-                new EventBuilder(team, user2DistinctId)
-                    .withEvent('$identify')
-                    .withProperties({
-                        $set: {
-                            new_name: 'User 2 - Updated',
-                        },
-                    })
-                    .build(),
-
-                // Create alias for user2 -> user3
-                new EventBuilder(team, user2DistinctId)
-                    .withEvent('$create_alias')
-                    .withProperties({
-                        distinct_id: user2DistinctId,
-                        alias: user3DistinctId,
-                    })
-                    .build(),
-            ]
-
-            const events2 = [
-                // User 1 creation
-                new EventBuilder(team, user1DistinctId)
-                    .withEvent('$identify')
-                    .withProperties({
-                        $set: {
-                            name: 'User 1',
-                            email: `user1-${user1DistinctId}@example.com`,
-                            age: 30,
-                            test_name: testName,
-                        },
-                    })
-                    .build(),
-                new EventBuilder(team, user1DistinctId)
-                    .withEvent('$identify')
-                    .withProperties({
-                        $set: {
-                            new_name: 'User 1 - Updated',
-                        },
-                    })
-                    .build(),
-
-                // Merge users: alias user1 -> user2
-                new EventBuilder(team, user1DistinctId)
-                    .withEvent('$create_alias')
-                    .withProperties({
-                        distinct_id: user1DistinctId,
-                        alias: user2DistinctId,
-                    })
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-            await waitForKafkaMessages(hub)
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events2))
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                expect(events.length).toBe(6)
-
-                // TODO: Add specific assertions based on expected behavior
-                // All events should be processed without errors
-                expect(events).toBeDefined()
-            })
-
-            // fetch the person properties
-            await waitForExpect(async () => {
-                const persons = await fetchPostgresPersons(hub.db, team.id)
-                expect(persons.length).toBe(2)
-                const personsClickhouse = await fetchPersons(hub, team.id)
-                expect(personsClickhouse.length).toBe(2)
-                expect(persons.map((person) => person.properties)).toEqual(
-                    expect.arrayContaining([
-                        expect.objectContaining({
-                            name: 'User 1',
-                            new_name: 'User 1 - Updated',
-                            email: `user1-${user1DistinctId}@example.com`,
-                            age: 30,
-                            test_name: testName,
-                        }),
-                        expect.objectContaining({
-                            name: 'User 2',
-                            new_name: 'User 2 - Updated',
-                            email: `user2-${user2DistinctId}@example.com`,
-                            age: 30,
-                            test_name: testName,
-                        }),
-                    ])
-                )
-                expect(personsClickhouse.map((person) => person.properties)).toEqual(
-                    expect.arrayContaining([
-                        expect.objectContaining({
-                            name: 'User 1',
-                            new_name: 'User 1 - Updated',
-                            email: `user1-${user1DistinctId}@example.com`,
-                            age: 30,
-                            test_name: testName,
-                        }),
-                        expect.objectContaining({
-                            name: 'User 2',
-                            new_name: 'User 2 - Updated',
-                            email: `user2-${user2DistinctId}@example.com`,
-                            age: 30,
-                            test_name: testName,
-                        }),
-                    ])
-                )
-                const person1 = persons.find((person) => person.properties.name === 'User 1')!
-                const person2 = persons.find((person) => person.properties.name === 'User 2')!
-                const distinctIdsPersons1 = await fetchDistinctIds(hub.db.postgres, {
-                    id: person1.id,
-                    team_id: team.id,
-                } as InternalPerson)
-                expect(distinctIdsPersons1.length).toBe(1)
-                // Except distinctids to match the ids, in any order
-                expect(distinctIdsPersons1.map((distinctId) => distinctId.distinct_id)).toEqual(
-                    expect.arrayContaining([user1DistinctId])
-                )
-                const distinctIdsPersons2 = await fetchDistinctIds(hub.db.postgres, {
-                    id: person2.id,
-                    team_id: team.id,
-                } as InternalPerson)
-                expect(distinctIdsPersons2.length).toBe(2)
-                // Except distinctids to match the ids, in any order
-                expect(distinctIdsPersons2.map((distinctId) => distinctId.distinct_id)).toEqual(
-                    expect.arrayContaining([user2DistinctId, user3DistinctId])
-                )
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'Should set and $unset person properties, different batches',
-        {},
-        async (ingester, hub, team) => {
-            const user1DistinctId = 'user1-distinct-id'
-
-            const events = [
-                new EventBuilder(team, user1DistinctId)
-                    .withEvent('$identify')
-                    .withProperties({
-                        $set: {
-                            name: 'User 1',
-                            property_to_unset: 'value',
-                        },
-                    })
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const persons = await fetchPostgresPersons(hub.db, team.id)
-                expect(persons.length).toBe(1)
-                const personsClickhouse = await fetchPersons(hub, team.id)
-                expect(personsClickhouse.length).toBe(1)
-                expect(persons[0].properties).toMatchObject(
-                    expect.objectContaining({
-                        name: 'User 1',
-                        property_to_unset: 'value',
-                    })
-                )
-                expect(personsClickhouse[0].properties).toMatchObject(
-                    expect.objectContaining({
-                        name: 'User 1',
-                        property_to_unset: 'value',
-                    })
-                )
-            })
-
-            const events2 = [
-                new EventBuilder(team, user1DistinctId)
-                    .withEvent('$identify')
-                    .withProperties({
-                        $unset: ['property_to_unset'],
-                    })
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events2))
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const persons = await fetchPostgresPersons(hub.db, team.id)
-                expect(persons.length).toBe(1)
-                const personsClickhouse = await fetchPersons(hub, team.id)
-                expect(personsClickhouse.length).toBe(1)
-                expect(persons[0].properties).toMatchObject(
-                    expect.objectContaining({
-                        name: 'User 1',
-                    })
-                )
-                expect(persons[0].properties).not.toHaveProperty('property_to_unset')
-                expect(personsClickhouse[0].properties).toMatchObject(
-                    expect.objectContaining({
-                        name: 'User 1',
-                    })
-                )
-                expect(personsClickhouse[0].properties).not.toHaveProperty('property_to_unset')
-            })
-        }
-    )
-
-    testWithTeamIngester('Should set and $unset person properties, same batch', {}, async (ingester, hub, team) => {
-        const user1DistinctId = 'user1-distinct-id'
-
-        const events = [
-            new EventBuilder(team, user1DistinctId)
-                .withEvent('$identify')
-                .withProperties({
-                    $set: {
-                        name: 'User 1',
-                        property_to_unset: 'value',
-                    },
-                })
-                .build(),
-            new EventBuilder(team, user1DistinctId)
-                .withEvent('$identify')
-                .withProperties({
-                    $unset: ['property_to_unset'],
-                })
-                .build(),
-        ]
-
-        await ingester.handleKafkaBatch(createKafkaMessages(events))
-        await waitForKafkaMessages(hub)
-
-        await waitForExpect(async () => {
-            const persons = await fetchPostgresPersons(hub.db, team.id)
-            expect(persons.length).toBe(1)
-            const personsClickhouse = await fetchPersons(hub, team.id)
-            expect(personsClickhouse.length).toBe(1)
-            expect(persons[0].properties).toMatchObject(
-                expect.objectContaining({
-                    name: 'User 1',
-                })
-            )
-            expect(persons[0].properties).not.toHaveProperty('property_to_unset')
-            expect(personsClickhouse[0].properties).toMatchObject(
-                expect.objectContaining({
-                    name: 'User 1',
-                })
-            )
-            expect(personsClickhouse[0].properties).not.toHaveProperty('property_to_unset')
-        })
-    })
-
-    testWithTeamIngester(
-        'should process events with various timestamps when dropping is disabled',
-        { teamOverrides: { drop_events_older_than_seconds: null } },
-        async (ingester, hub, team) => {
-            const currentTime = DateTime.now().toMillis()
-            const events = [
-                // Current time event
-                new EventBuilder(team)
-                    .withEvent('current_time_event')
-                    .withTimestamp(currentTime)
-                    .withNow(currentTime)
-                    .build(),
-                // Recent event (30 minutes ago)
-                new EventBuilder(team)
-                    .withEvent('recent_event')
-                    .withTimestamp(currentTime - 30 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Old event (2 hours ago)
-                new EventBuilder(team)
-                    .withEvent('old_event')
-                    .withTimestamp(currentTime - 2 * 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Very old event (1 day ago)
-                new EventBuilder(team)
-                    .withEvent('very_old_event')
-                    .withTimestamp(currentTime - 24 * 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Extremely old event (1 month ago)
-                new EventBuilder(team)
-                    .withEvent('extremely_old_event')
-                    .withTimestamp(currentTime - 30 * 24 * 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Future event (1 hour from now)
-                new EventBuilder(team)
-                    .withEvent('future_event')
-                    .withTimestamp(currentTime + 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                // All events should be processed when dropping is disabled
-                expect(events.length).toBe(6)
-
-                // Verify all events are present
-                const eventTypes = events.map((e) => e.event)
-                expect(eventTypes).toContain('current_time_event')
-                expect(eventTypes).toContain('recent_event')
-                expect(eventTypes).toContain('old_event')
-                expect(eventTypes).toContain('very_old_event')
-                expect(eventTypes).toContain('extremely_old_event')
-                expect(eventTypes).toContain('future_event')
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'should drop old events when dropping is enabled',
-        { teamOverrides: { drop_events_older_than_seconds: 3600 } },
-        async (ingester, hub, team) => {
-            const currentTime = DateTime.now().toMillis()
-            const events = [
-                // Current time event - should pass
-                new EventBuilder(team)
-                    .withEvent('current_time_event')
-                    .withTimestamp(currentTime)
-                    .withNow(currentTime)
-                    .build(),
-                // Recent event (30 minutes ago) - should pass
-                new EventBuilder(team)
-                    .withEvent('recent_event')
-                    .withTimestamp(currentTime - 30 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Just under threshold (59 minutes ago) - should pass
-                new EventBuilder(team)
-                    .withEvent('just_under_threshold')
-                    .withTimestamp(currentTime - 59 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Just over threshold (61 minutes ago) - should be dropped
-                new EventBuilder(team)
-                    .withEvent('just_over_threshold')
-                    .withTimestamp(currentTime - 61 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Old event (2 hours ago) - should be dropped
-                new EventBuilder(team)
-                    .withEvent('old_event')
-                    .withTimestamp(currentTime - 2 * 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Very old event (1 day ago) - should be dropped
-                new EventBuilder(team)
-                    .withEvent('very_old_event')
-                    .withTimestamp(currentTime - 24 * 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Future event (1 hour from now) - should pass
-                new EventBuilder(team)
-                    .withEvent('future_event')
-                    .withTimestamp(currentTime + 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                // Only events under 1 hour old should be processed
-                expect(events.length).toBe(4)
-
-                // Verify only expected events are present
-                const eventTypes = events.map((e) => e.event)
-                expect(eventTypes).toContain('current_time_event')
-                expect(eventTypes).toContain('recent_event')
-                expect(eventTypes).toContain('just_under_threshold')
-                expect(eventTypes).toContain('future_event')
-
-                // Verify dropped events are not present
-                expect(eventTypes).not.toContain('just_over_threshold')
-                expect(eventTypes).not.toContain('old_event')
-                expect(eventTypes).not.toContain('very_old_event')
-            })
-
-            // Check that ingestion warnings were created for dropped events
-            await waitForExpect(async () => {
-                const warnings = await fetchIngestionWarnings(hub, team.id)
-                expect(warnings.length).toBe(3)
-
-                const warningTypes = warnings.map((w) => w.type)
-                expect(warningTypes).toEqual(
-                    expect.arrayContaining(['event_dropped_too_old', 'event_dropped_too_old', 'event_dropped_too_old'])
-                )
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'should process all events when drop threshold is set to 0',
-        { teamOverrides: { drop_events_older_than_seconds: 0 } },
-        async (ingester, hub, team) => {
-            const currentTime = DateTime.now().toMillis()
-            const events = [
-                // Current time event - should pass
-                new EventBuilder(team)
-                    .withEvent('current_time_event')
-                    .withTimestamp(currentTime)
-                    .withNow(currentTime)
-                    .build(),
-                // Recent event (30 minutes ago) - should pass
-                new EventBuilder(team)
-                    .withEvent('recent_event')
-                    .withTimestamp(currentTime - 30 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Old event (2 hours ago) - should pass (0 threshold means no dropping)
-                new EventBuilder(team)
-                    .withEvent('old_event')
-                    .withTimestamp(currentTime - 2 * 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Very old event (1 day ago) - should pass (0 threshold means no dropping)
-                new EventBuilder(team)
-                    .withEvent('very_old_event')
-                    .withTimestamp(currentTime - 24 * 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Extremely old event (1 month ago) - should pass (0 threshold means no dropping)
-                new EventBuilder(team)
-                    .withEvent('extremely_old_event')
-                    .withTimestamp(currentTime - 30 * 24 * 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-                // Future event (1 hour from now) - should pass
-                new EventBuilder(team)
-                    .withEvent('future_event')
-                    .withTimestamp(currentTime + 60 * 60 * 1000)
-                    .withNow(currentTime)
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const events = await fetchEvents(hub, team.id)
-                // All events should be processed when threshold is 0 (no dropping)
-                expect(events.length).toBe(6)
-
-                // Verify all events are present
-                const eventTypes = events.map((e) => e.event)
-                expect(eventTypes).toContain('current_time_event')
-                expect(eventTypes).toContain('recent_event')
-                expect(eventTypes).toContain('old_event')
-                expect(eventTypes).toContain('very_old_event')
-                expect(eventTypes).toContain('extremely_old_event')
-                expect(eventTypes).toContain('future_event')
-            })
-
-            // Check that no ingestion warnings were created (no events should be dropped)
-            await waitForExpect(async () => {
-                const warnings = await fetchIngestionWarnings(hub, team.id)
-                expect(warnings.length).toBe(0)
-            })
-        }
-    )
-
-    // testWithTeamIngester(
-    //     'we do not alias users if distinct id changes but we are already identified',
-    //     {},
-    //     async (ingester, hub, team) => {
-    //         // This test is in reference to
-    //         // https://github.com/PostHog/posthog/issues/5527 , where we were
-    //         // correctly identifying that an anonymous user before login should be
-    //         // aliased to the user they subsequently login as, but incorrectly
-    //         // aliasing on subsequent $identify events. The anonymous case is
-    //         // special as we want to alias to a known user, but otherwise we
-    //         // shouldn't be doing so.
-
-    //         const anonymousId = 'anonymous_id'
-    //         const initialDistinctId = 'initial_distinct_id'
-    //         const p2DistinctId = 'p2_distinct_id'
-    //         const p2NewDistinctId = 'new_distinct_id'
-
-    //         // Play out a sequence of events that should result in two users being
-    //         // identified, with the first to events associated with one user, and
-    //         // the third with another.
-    //         const events = [
-    //             new EventBuilder(team, anonymousId).withEvent('event 1').build(),
-    //             new EventBuilder(team, initialDistinctId)
-    //                 .withEvent('$identify')
-    //                 .withProperties({ $anon_distinct_id: anonymousId })
-    //                 .build(),
-    //             new EventBuilder(team, initialDistinctId).withEvent('event 2').build(),
-    //             new EventBuilder(team, p2DistinctId).withEvent('event 3').build(),
-    //             new EventBuilder(team, p2NewDistinctId)
-    //                 .withEvent('$identify')
-    //                 .withProperties({ $anon_distinct_id: p2DistinctId })
-    //                 .build(),
-    //             new EventBuilder(team, p2NewDistinctId).withEvent('event 4').build(),
-    //             // Let's also make sure that we do not alias when switching back to initialDistictId
-    //             new EventBuilder(team, initialDistinctId)
-    //                 .withEvent('$identify')
-    //                 .withProperties({ $anon_distinct_id: p2NewDistinctId })
-    //                 .build(),
-    //         ]
-
-    //         await ingester.handleKafkaBatch(createKafkaMessages(events))
-    //         await waitForKafkaMessages(hub)
-
-    //         await waitForExpect(async () => {
-    //             const persons = await fetchPostgresPersons(hub.db, team.id)
-    //             expect(persons.length).toBe(2)
-
-    //             // Both persons should be identified
-    //             expect(persons.map((person) => person.is_identified)).toEqual([true, true])
-
-    //             // Check that events are grouped correctly by person
-    //             const events = await fetchEvents(hub, team.id)
-    //             const eventsByPersonId = new Map<string, string[]>()
-
-    //             for (const event of events) {
-    //                 const personId = event.person_id!
-    //                 if (!eventsByPersonId.has(personId)) {
-    //                     eventsByPersonId.set(personId, [])
-    //                 }
-    //                 eventsByPersonId.get(personId)!.push(event.event)
-    //             }
-
-    //             expect(eventsByPersonId.size).toBe(2)
-    //             const eventGroups = Array.from(eventsByPersonId.values()).sort((a, b) => a.length - b.length)
-    //             expect(eventGroups).toEqual(
-    //                 expect.arrayContaining([
-    //                     expect.arrayContaining(['event 3', '$identify', 'event 4']),
-    //                     expect.arrayContaining(['event 1', '$identify', 'event 2', '$identify']),
-    //                 ])
-    //             )
-    //         })
-    //     }
-    // )
-
-    testWithTeamIngester(
-        'we do not alias users if distinct id changes but we are already identified, with no anonymous event',
-        {},
-        async (ingester, hub, team) => {
-            // This test is similar to the previous one, except it does not include an initial anonymous event.
-
-            const anonymousId = 'anonymous_id'
-            const initialDistinctId = 'initial_distinct_id'
-            const p2DistinctId = 'p2_distinct_id'
-            const p2NewDistinctId = 'new_distinct_id'
-
-            // Play out a sequence of events that should result in two users being
-            // identified, with the first to events associated with one user, and
-            // the third with another.
-            const events = [
-                new EventBuilder(team, initialDistinctId)
-                    .withEvent('$identify')
-                    .withProperties({ $anon_distinct_id: anonymousId })
-                    .build(),
-                new EventBuilder(team, initialDistinctId).withEvent('event 2').build(),
-                new EventBuilder(team, p2DistinctId).withEvent('event 3').build(),
-                new EventBuilder(team, p2NewDistinctId)
-                    .withEvent('$identify')
-                    .withProperties({ $anon_distinct_id: p2DistinctId })
-                    .build(),
-                new EventBuilder(team, p2NewDistinctId).withEvent('event 4').build(),
-                // Let's also make sure that we do not alias when switching back to initialDistictId
-                new EventBuilder(team, initialDistinctId)
-                    .withEvent('$identify')
-                    .withProperties({ $anon_distinct_id: p2NewDistinctId })
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const persons = await fetchPostgresPersons(hub.db, team.id)
-                expect(persons.length).toBe(2)
-
-                // Both persons should be identified
-                expect(persons.map((person) => person.is_identified)).toEqual([true, true])
-
-                // Check that events are grouped correctly by person
-                const events = await fetchEvents(hub, team.id)
-                const eventsByPersonId = new Map<string, string[]>()
-
-                for (const event of events) {
-                    const personId = event.person_id!
-                    if (!eventsByPersonId.has(personId)) {
-                        eventsByPersonId.set(personId, [])
-                    }
-                    eventsByPersonId.get(personId)!.push(event.event)
-                }
-
-                expect(eventsByPersonId.size).toBe(2)
-                const eventGroups = Array.from(eventsByPersonId.values()).sort((a, b) => a.length - b.length)
-                expect(eventGroups).toEqual(
-                    expect.arrayContaining([
-                        expect.arrayContaining(['event 3', '$identify', 'event 4']),
-                        expect.arrayContaining(['$identify', 'event 2', '$identify']),
-                    ])
-                )
-            })
-        }
-    )
-
-    testWithTeamIngester(
-        'we do not leave things in inconsistent state if $identify is run concurrently',
-        {},
-        async (ingester, hub, team) => {
-            // There are a few places where we have the pattern of:
-            //
-            //  1. fetch from postgres
-            //  2. check rows match condition
-            //  3. perform update
-            //
-            // This test is designed to check the specific case where, in
-            // handling we are creating an unidentified user, then updating this
-            // user to have is_identified = true. Since we are using the
-            // is_identified to decide on if we will merge persons, we want to
-            // make sure we guard against this race condition.
-
-            const anonymousId = 'anonymous_id'
-            const initialDistinctId = 'initial-distinct-id'
-            const newDistinctId = 'new-distinct-id'
-
-            const events = [
-                new EventBuilder(team, newDistinctId).withEvent('some event').build(),
-                new EventBuilder(team, initialDistinctId)
-                    .withEvent('$identify')
-                    .withProperties({ $anon_distinct_id: anonymousId })
-                    .build(),
-                new EventBuilder(team, newDistinctId)
-                    .withEvent('$identify')
-                    .withProperties({ $anon_distinct_id: anonymousId })
-                    .build(),
-            ]
-
-            await ingester.handleKafkaBatch(createKafkaMessages(events))
-            await waitForKafkaMessages(hub)
-
-            await waitForExpect(async () => {
-                const persons = await fetchPostgresPersons(hub.db, team.id)
-                expect(persons.length).toBe(2)
-                expect(persons.map((person) => person.is_identified)).toEqual([true, true])
-            })
-        }
-    )
-
-    testWithTeamIngester('we can alias an anonymous person to an anonymous person', {}, async (ingester, hub, team) => {
-        const anonymous1 = 'anonymous-1'
-        const anonymous2 = 'anonymous-2'
-
-        const events = [
-            new EventBuilder(team, anonymous1).withEvent('anonymous event 1').build(),
-            new EventBuilder(team, anonymous2).withEvent('anonymous event 2').build(),
-            // Then try to alias them
-            new EventBuilder(team, anonymous2).withEvent('$create_alias').withProperties({ alias: anonymous1 }).build(),
-        ]
-
-        await ingester.handleKafkaBatch(createKafkaMessages(events))
-        await waitForKafkaMessages(hub)
-
-        await waitForExpect(async () => {
-            const persons = await fetchPostgresPersons(hub.db, team.id)
-            expect(persons.length).toBe(1)
-
-            // Make sure there is one identified person
-            expect(persons.map((person) => person.is_identified)).toEqual([true])
-
-            // Check that events are grouped correctly by person
-            const events = await fetchEvents(hub, team.id)
-            expect(events.length).toBe(3)
-
-            // All events should belong to the same person
-            const personIds = new Set(events.map((e) => e.person_id))
-            expect(personIds.size).toBe(1)
-
-            const eventNames = events.map((e) => e.event).sort()
-            expect(eventNames).toEqual(['$create_alias', 'anonymous event 1', 'anonymous event 2'])
-        })
-    })
-})
+        expect(hub.groupRepository.updateGroup).toHaveBeenCalledTimes(0)
\ No newline at end of file
diff --git a/plugin-server/src/worker/ingestion/persons/batch-writing-person-store.test.ts b/plugin-server/src/worker/ingestion/persons/batch-writing-person-store.test.ts
index 8d7f96b..61a7f07 100644
--- a/plugin-server/src/worker/ingestion/persons/batch-writing-person-store.test.ts
+++ b/plugin-server/src/worker/ingestion/persons/batch-writing-person-store.test.ts
@@ -350,1818 +350,4 @@ describe('BatchWritingPersonStore', () => {
         const personStoreForBatch = getBatchStoreForBatch()
 
         // First update
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(person, { prop1: 'value1' }, [], {}, 'test')
-
-        // Second update to same person
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            person,
-            { test: 'value2', prop2: 'value2' },
-            [],
-            {},
-            'test'
-        )
-
-        // Check cache contains merged updates
-        const cache = personStoreForBatch.getUpdateCache()
-        const cachedUpdate = cache.get(`${teamId}:${person.id}`)!
-        expect(cachedUpdate.properties).toEqual({ test: 'test' }) // Original properties from database
-        expect(cachedUpdate.properties_to_set).toEqual({ prop1: 'value1', test: 'value2', prop2: 'value2' }) // Merged properties to set
-        expect(cachedUpdate.properties_to_unset).toEqual([]) // No properties to unset
-        expect(cachedUpdate.needs_write).toBe(true)
-    })
-
-    describe('fetchForUpdate vs fetchForChecking', () => {
-        it('should use separate caches for update and checking', async () => {
-            const personStoreForBatch = getBatchStoreForBatch()
-
-            // Fetch for checking should cache in check cache
-            const personFromCheck = await personStoreForBatch.fetchForChecking(teamId, 'test-distinct')
-            expect(personFromCheck).toEqual(person)
-
-            const checkCache = (personStoreForBatch as any)['personCheckCache']
-            expect(checkCache.get('1:test-distinct')).toEqual(person)
-
-            // Fetch for update should cache in update cache and return PersonUpdate converted to InternalPerson
-            const personFromUpdate = await personStoreForBatch.fetchForUpdate(teamId, 'test-distinct2')
-            expect(personFromUpdate).toBeDefined()
-            expect(personFromUpdate!.id).toBe(person.id)
-            expect(personFromUpdate!.team_id).toBe(person.team_id)
-            expect(personFromUpdate!.id).toBe(person.id)
-
-            const updateCache = personStoreForBatch.getUpdateCache()
-            const cachedPersonUpdate = updateCache.get(`${teamId}:${person.id}`)
-            expect(cachedPersonUpdate).toBeDefined()
-            expect(cachedPersonUpdate!.distinct_id).toBe('test-distinct2')
-        })
-
-        it('should handle cache hits for both checking and updating', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch()
-
-            // First fetch should hit the database
-            await personStoreForBatch.fetchForChecking(teamId, 'test-distinct')
-            expect(mockRepo.fetchPerson).toHaveBeenCalledTimes(1)
-
-            // Second fetch should hit the cache
-            await personStoreForBatch.fetchForChecking(teamId, 'test-distinct')
-            expect(mockRepo.fetchPerson).toHaveBeenCalledTimes(1) // No additional call
-
-            // Similar for update cache
-            await personStoreForBatch.fetchForUpdate(teamId, 'test-distinct2')
-            expect(mockRepo.fetchPerson).toHaveBeenCalledTimes(2)
-
-            await personStoreForBatch.fetchForUpdate(teamId, 'test-distinct2')
-            expect(mockRepo.fetchPerson).toHaveBeenCalledTimes(2) // No additional call
-        })
-
-        it('should prefer update cache over check cache in fetchForChecking', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch()
-
-            // First populate update cache
-            await personStoreForBatch.fetchForUpdate(teamId, 'test-distinct')
-
-            // Reset the mock to track new calls
-            jest.clearAllMocks()
-
-            // fetchForChecking should use the cached PersonUpdate instead of hitting DB
-            const result = await personStoreForBatch.fetchForChecking(teamId, 'test-distinct')
-            expect(result).toBeDefined()
-            expect(mockRepo.fetchPerson).not.toHaveBeenCalled()
-        })
-
-        it('should handle null results from database', async () => {
-            const mockRepo = createMockRepository()
-            mockRepo.fetchPerson = jest.fn().mockResolvedValue(undefined)
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch()
-
-            const checkResult = await personStoreForBatch.fetchForChecking(teamId, 'nonexistent')
-            expect(checkResult).toBeNull()
-
-            const updateResult = await personStoreForBatch.fetchForUpdate(teamId, 'nonexistent')
-            expect(updateResult).toBeNull()
-        })
-    })
-
-    it('should retry optimistic updates with exponential backoff', async () => {
-        // Use ASSERT_VERSION mode for this test since it tests optimistic behavior
-        const testMockRepo = createMockRepository()
-        const assertVersionStore = new BatchWritingPersonsStore(testMockRepo, db.kafkaProducer, {
-            dbWriteMode: 'ASSERT_VERSION',
-        })
-        const personStoreForBatch = assertVersionStore.forBatch()
-        let callCount = 0
-
-        // Mock to fail first few times, then succeed
-        testMockRepo.updatePersonAssertVersion = jest.fn().mockImplementation(() => {
-            callCount++
-            if (callCount < 3) {
-                return Promise.resolve([undefined, []]) // version mismatch
-            }
-            return Promise.resolve([5, []]) // success on 3rd try
-        })
-
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            person,
-            { new_value: 'new_value' },
-            [],
-            {},
-            'test'
-        )
-        await personStoreForBatch.flush()
-
-        expect(testMockRepo.updatePersonAssertVersion).toHaveBeenCalledTimes(3)
-        expect(testMockRepo.fetchPerson).toHaveBeenCalledTimes(2) // Called for each conflict
-        expect(testMockRepo.updatePerson).not.toHaveBeenCalled() // Shouldn't fallback if retries succeed
-    })
-
-    it('should fallback to direct update after max retries', async () => {
-        // Use ASSERT_VERSION mode for this test since it tests optimistic behavior
-        const assertVersionStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer, {
-            dbWriteMode: 'ASSERT_VERSION',
-        })
-        const personStoreForBatch = assertVersionStore.forBatch()
-
-        // Mock to always fail optimistic updates
-        mockRepo.updatePersonAssertVersion = jest.fn().mockResolvedValue([undefined, []])
-
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            person,
-            { new_value: 'new_value' },
-            [],
-            {},
-            'test'
-        )
-        await personStoreForBatch.flush()
-
-        // Should try optimistic update multiple times based on config (1 initial + 5 retries = 6 total)
-        expect(mockRepo.updatePersonAssertVersion).toHaveBeenCalledTimes(6) // default max retries
-        expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1) // fallback
-    })
-
-    it('should merge properties during conflict resolution', async () => {
-        // Use ASSERT_VERSION mode for this test since it tests optimistic behavior
-        const assertVersionStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer, {
-            dbWriteMode: 'ASSERT_VERSION',
-        })
-        const personStoreForBatch = assertVersionStore.forBatch()
-        const latestPerson = {
-            ...person,
-            version: 3,
-            properties: { existing_prop: 'existing_value', shared_prop: 'old_value' },
-        }
-
-        mockRepo.updatePersonAssertVersion = jest.fn().mockResolvedValue([undefined, []]) // Always fail, but we don't care about the version
-        mockRepo.fetchPerson = jest.fn().mockResolvedValue(latestPerson)
-
-        // Update with new properties
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            person,
-            { new_prop: 'new_value', shared_prop: 'new_value' },
-            [],
-            {},
-            'test'
-        )
-
-        await personStoreForBatch.flush()
-
-        // Verify the direct update was called with merged properties
-        expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-            expect.objectContaining({
-                version: 3, // Should use latest version
-            }),
-            expect.objectContaining({
-                properties: {
-                    existing_prop: 'existing_value',
-                    new_prop: 'new_value',
-                    shared_prop: 'new_value',
-                },
-            }),
-            'updatePersonNoAssert'
-        )
-    })
-
-    it('should handle database errors gracefully during flush', async () => {
-        const personStoreForBatch = personStore.forBatch()
-
-        mockRepo.updatePerson = jest.fn().mockRejectedValue(new Error('Database connection failed'))
-
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            person,
-            { new_value: 'new_value' },
-            [],
-            {},
-            'test'
-        )
-
-        await expect(personStoreForBatch.flush()).rejects.toThrow('Database connection failed')
-    })
-
-    it('should handle partial failures in batch flush', async () => {
-        const personStoreForBatch = personStore.forBatch()
-
-        // Set up multiple updates
-        const person2 = { ...person, id: '2', uuid: '2' }
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(person, { test: 'value1' }, [], {}, 'test1')
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(person2, { test: 'value2' }, [], {}, 'test2')
-
-        // Mock first update to succeed, second to fail
-        let callCount = 0
-        mockRepo.updatePerson = jest.fn().mockImplementation(() => {
-            callCount++
-            if (callCount === 1) {
-                return Promise.resolve([person, []]) // success for first person
-            }
-            throw new Error('Database error') // fail for second person
-        })
-
-        await expect(personStoreForBatch.flush()).rejects.toThrow('Database error')
-    })
-
-    it('should handle clearing cache for different team IDs', async () => {
-        const mockRepo = createMockRepository()
-        const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-        const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-        const person2 = { ...person, id: 'person2-id', uuid: 'person2-uuid', team_id: 2 }
-
-        // Add to both caches for different teams
-        const updateCache = personStoreForBatch.getUpdateCache()
-        const checkCache = personStoreForBatch.getCheckCache()
-
-        updateCache.set(`${person.team_id}:${person.id}`, fromInternalPerson(person, 'test'))
-        updateCache.set(`${person2.team_id}:${person2.id}`, fromInternalPerson(person2, 'test'))
-        checkCache.set(`${person.team_id}:test`, person)
-        checkCache.set(`${person2.team_id}:test`, person2)
-        personStoreForBatch.setDistinctIdToPersonId(person.team_id, 'test', person.id)
-        personStoreForBatch.setDistinctIdToPersonId(person2.team_id, 'test', person2.id)
-
-        // Delete person from team 1
-        await personStoreForBatch.deletePerson(person, 'test')
-        expect(mockRepo.deletePerson).toHaveBeenCalledWith(
-            expect.objectContaining({
-                ...person,
-                properties: { test: 'test' },
-            })
-        )
-
-        // Only team 1 entries should be removed
-        expect(updateCache.has(`${person.team_id}:${person.id}`)).toBe(false)
-        expect(updateCache.has(`${person2.team_id}:${person2.id}`)).toBe(true)
-        expect(checkCache.has(`${person.team_id}:test`)).toBe(false)
-        expect(checkCache.has(`${person2.team_id}:test`)).toBe(true)
-    })
-
-    it('should handle empty properties updates', async () => {
-        const personStoreForBatch = getBatchStoreForBatch()
-
-        const result = await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(person, {}, [], {}, 'test')
-        expect(result[0]).toEqual(person) // Should return original person unchanged
-
-        const cache = personStoreForBatch.getUpdateCache()
-        const cachedUpdate = cache.get(`${teamId}:${person.id}`)!
-        expect(cachedUpdate.needs_write).toBe(true) // Still marked for write
-    })
-
-    it('should handle null and undefined property values', async () => {
-        const personStoreForBatch = getBatchStoreForBatch()
-
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            person,
-            { null_prop: null, undefined_prop: undefined },
-            [],
-            {},
-            'test'
-        )
-
-        const cache = personStoreForBatch.getUpdateCache()
-        const cachedUpdate = cache.get(`${teamId}:${person.id}`)!
-        expect(cachedUpdate.properties_to_set.null_prop).toBeNull()
-        expect(cachedUpdate.properties_to_set.undefined_prop).toBeUndefined()
-
-        await personStoreForBatch.flush()
-
-        expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-            expect.objectContaining({
-                properties: { null_prop: null, undefined_prop: undefined, test: 'test' },
-            }),
-            expect.anything(),
-            'updatePersonNoAssert'
-        )
-    })
-
-    it('should handle MessageSizeTooLarge errors and capture warning', async () => {
-        const personStoreForBatch = personStore.forBatch()
-
-        // Mock NO_ASSERT update to fail with MessageSizeTooLarge
-        mockRepo.updatePerson = jest.fn().mockRejectedValue(new MessageSizeTooLarge('test', new Error('test')))
-
-        // Add a person update to cache
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            person,
-            { new_value: 'new_value' },
-            [],
-            {},
-            'test'
-        )
-
-        // Flush should handle the error and capture warning
-        await personStoreForBatch.flush()
-
-        expect(mockRepo.updatePerson).toHaveBeenCalled()
-        expect(captureIngestionWarning).toHaveBeenCalledWith(
-            db.kafkaProducer,
-            teamId,
-            'person_upsert_message_size_too_large',
-            {
-                personId: person.id,
-                distinctId: 'test',
-            }
-        )
-    })
-
-    describe('dbWriteMode functionality', () => {
-        describe('flush with NO_ASSERT mode', () => {
-            it('should call updatePersonNoAssert directly without retries', async () => {
-                const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer, {
-                    dbWriteMode: 'NO_ASSERT',
-                })
-                const personStoreForBatch = testPersonStore.forBatch()
-
-                await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                    person,
-                    { new_value: 'new_value' },
-                    [],
-                    {},
-                    'test'
-                )
-                await personStoreForBatch.flush()
-
-                expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-                expect(mockRepo.updatePersonAssertVersion).not.toHaveBeenCalled()
-                expect(db.postgres.transaction).not.toHaveBeenCalled()
-            })
-
-            it('should fallback with NO_ASSERT mode', async () => {
-                const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer, {
-                    dbWriteMode: 'NO_ASSERT',
-                    maxOptimisticUpdateRetries: 5,
-                })
-                const personStoreForBatch = testPersonStore.forBatch()
-
-                mockRepo.updatePerson = jest.fn().mockRejectedValue(new Error('Database error'))
-
-                await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                    person,
-                    { new_value: 'new_value' },
-                    [],
-                    {},
-                    'test'
-                )
-
-                await expect(personStoreForBatch.flush()).rejects.toThrow('Database error')
-                expect(mockRepo.updatePerson).toHaveBeenCalledTimes(6) // 6 for update (1 fallback + 5 retries)
-                expect(mockRepo.updatePersonAssertVersion).not.toHaveBeenCalled()
-            })
-        })
-
-        describe('flush with ASSERT_VERSION mode', () => {
-            it('should call updatePersonAssertVersion with retries', async () => {
-                const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer, {
-                    dbWriteMode: 'ASSERT_VERSION',
-                })
-                const personStoreForBatch = testPersonStore.forBatch()
-
-                mockRepo.updatePersonAssertVersion = jest.fn().mockResolvedValue([5, []]) // success
-
-                await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                    person,
-                    { new_value: 'new_value' },
-                    [],
-                    {},
-                    'test'
-                )
-                await personStoreForBatch.flush()
-
-                expect(mockRepo.updatePersonAssertVersion).toHaveBeenCalledTimes(1)
-                expect(mockRepo.updatePerson).not.toHaveBeenCalled()
-                expect(db.postgres.transaction).not.toHaveBeenCalled()
-            })
-
-            it('should retry on version conflicts and eventually fallback', async () => {
-                const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer, {
-                    dbWriteMode: 'ASSERT_VERSION',
-                    maxOptimisticUpdateRetries: 2,
-                })
-                const personStoreForBatch = testPersonStore.forBatch()
-
-                // Mock to always fail optimistic updates
-                mockRepo.updatePersonAssertVersion = jest.fn().mockResolvedValue([undefined, []])
-
-                await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                    person,
-                    { new_value: 'new_value' },
-                    [],
-                    {},
-                    'test'
-                )
-                await personStoreForBatch.flush()
-
-                expect(mockRepo.updatePersonAssertVersion).toHaveBeenCalledTimes(3) // 1 initial + 2 retries
-                expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1) // fallback
-            })
-
-            it('should handle MessageSizeTooLarge in ASSERT_VERSION mode', async () => {
-                const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer, {
-                    dbWriteMode: 'ASSERT_VERSION',
-                })
-                const personStoreForBatch = testPersonStore.forBatch()
-
-                mockRepo.updatePersonAssertVersion = jest
-                    .fn()
-                    .mockRejectedValue(new MessageSizeTooLarge('test', new Error('test')))
-
-                await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                    person,
-                    { new_value: 'new_value' },
-                    [],
-                    {},
-                    'test'
-                )
-                await personStoreForBatch.flush()
-
-                expect(mockRepo.updatePersonAssertVersion).toHaveBeenCalled()
-                expect(captureIngestionWarning).toHaveBeenCalledWith(
-                    db.kafkaProducer,
-                    teamId,
-                    'person_upsert_message_size_too_large',
-                    {
-                        personId: person.id,
-                        distinctId: 'test',
-                    }
-                )
-                expect(mockRepo.updatePerson).not.toHaveBeenCalled() // No fallback for MessageSizeTooLarge
-            })
-        })
-
-        describe('concurrent updates with different dbWriteModes', () => {
-            it('should handle multiple updates with different modes correctly', async () => {
-                const noAssertMockRepo = createMockRepository()
-                const assertVersionMockRepo = createMockRepository()
-
-                const noAssertStore = new BatchWritingPersonsStore(noAssertMockRepo, db.kafkaProducer, {
-                    dbWriteMode: 'NO_ASSERT',
-                })
-                const assertVersionStore = new BatchWritingPersonsStore(assertVersionMockRepo, db.kafkaProducer, {
-                    dbWriteMode: 'ASSERT_VERSION',
-                })
-
-                const noAssertBatch = noAssertStore.forBatch()
-                const assertVersionBatch = assertVersionStore.forBatch()
-
-                const person2 = { ...person, id: '2', uuid: '2' }
-
-                // Mock successful updates
-                assertVersionMockRepo.updatePersonAssertVersion = jest.fn().mockResolvedValue([5, []])
-
-                await Promise.all([
-                    noAssertBatch.updatePersonWithPropertiesDiffForUpdate(
-                        person,
-                        { mode: 'no_assert' },
-                        [],
-                        {},
-                        'test1'
-                    ),
-                    assertVersionBatch.updatePersonWithPropertiesDiffForUpdate(
-                        person2,
-                        { mode: 'assert_version' },
-                        [],
-                        {},
-                        'test2'
-                    ),
-                ])
-
-                await Promise.all([noAssertBatch.flush(), assertVersionBatch.flush()])
-
-                expect(noAssertMockRepo.updatePerson).toHaveBeenCalledTimes(1) // NO_ASSERT mode
-                expect(assertVersionMockRepo.updatePersonAssertVersion).toHaveBeenCalledTimes(1) // ASSERT_VERSION mode
-            })
-        })
-    })
-
-    it('should handle concurrent updates with ASSERT_VERSION mode and preserve both properties', async () => {
-        // Use ASSERT_VERSION mode for this test since it tests optimistic behavior
-        const mockRepo = createMockRepository()
-        const assertVersionStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer, {
-            dbWriteMode: 'ASSERT_VERSION',
-        })
-        const personStoreForBatch = assertVersionStore.forBatch()
-
-        // Initial person in database with 2 properties
-        const initialPerson = {
-            ...person,
-            version: 1,
-            properties: {
-                existing_prop1: 'initial_value1',
-                existing_prop2: 'initial_value2',
-            },
-        }
-
-        // Simulate that another pod directly writes to the database
-        // This increases the version and updates one property
-        const updatedByOtherPod = {
-            ...initialPerson,
-            version: 2,
-            properties: {
-                existing_prop1: 'updated_by_other_pod',
-                existing_prop2: 'initial_value2', // This property stays the same
-            },
-        }
-
-        // Mock optimistic update to fail on first try, succeed on retry
-        // Completely replace the mock from beforeEach
-        mockRepo.updatePersonAssertVersion = jest
-            .fn()
-            .mockResolvedValueOnce([undefined, []]) // First call fails (version mismatch)
-            .mockResolvedValueOnce([3, []]) // Second call succeeds with new version
-
-        // Mock fetchPerson to return the updated person when called during conflict resolution
-        mockRepo.fetchPerson = jest.fn().mockResolvedValue(updatedByOtherPod)
-
-        // Process an event that will override one of the properties
-        // We pass the initial person directly, so no initial fetch is needed
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            initialPerson,
-            { existing_prop2: 'updated_by_this_pod' },
-            [],
-            {},
-            'test'
-        )
-
-        // Flush should trigger optimistic update, fail, then merge and retry
-        await personStoreForBatch.flush()
-
-        // Verify the optimistic update was attempted (should be called twice: once initially, once on retry)
-        expect(mockRepo.updatePersonAssertVersion).toHaveBeenCalledTimes(2)
-
-        // Verify fetchPerson was called once during conflict resolution
-        expect(mockRepo.fetchPerson).toHaveBeenCalledTimes(1)
-
-        // Since the second retry succeeds, there should be no fallback to updatePerson
-        expect(mockRepo.updatePerson).not.toHaveBeenCalled()
-
-        // Verify the second call to updatePersonAssertVersion had the merged properties
-        expect(mockRepo.updatePersonAssertVersion).toHaveBeenLastCalledWith(
-            expect.objectContaining({
-                version: 2, // Should use the latest version from the database (updatedByOtherPod has version 2)
-                properties: {
-                    existing_prop1: 'updated_by_other_pod', // Preserved from other pod's update
-                    existing_prop2: 'updated_by_this_pod', // Updated by this pod
-                },
-                properties_to_set: {
-                    existing_prop2: 'updated_by_this_pod', // Only the changed property should be in properties_to_set
-                },
-                properties_to_unset: [], // No properties to unset
-            })
-        )
-    })
-
-    it('should consolidate updates for same person via different distinct IDs', async () => {
-        // This test validates that when two distinct IDs point to the same person,
-        // updates via both distinct IDs should be merged into a single person update
-        const distinctId1 = 'user-email@example.com'
-        const distinctId2 = 'user-device-abc123'
-
-        // Both distinct IDs point to the same person
-        const sharedPerson = {
-            ...person,
-            properties: {
-                initial_prop: 'initial_value',
-            },
-        }
-
-        // Mock fetchPerson to return the same person for both distinct IDs
-        const mockRepo = createMockRepository()
-        mockRepo.fetchPerson = jest.fn().mockImplementation(() => {
-            return Promise.resolve(sharedPerson)
-        })
-        const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-        const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-        // Update via first distinct ID
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            sharedPerson,
-            { prop_from_distinctId1: 'value1' },
-            [],
-            {},
-            distinctId1
-        )
-
-        // Update via second distinct ID
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            sharedPerson,
-            { prop_from_distinctId2: 'value2' },
-            [],
-            {},
-            distinctId2
-        )
-
-        const cache = personStoreForBatch.getUpdateCache()
-
-        const cacheKey = `${teamId}:${sharedPerson.id}`
-        const cacheValue = cache.get(cacheKey)
-        // Currently both cache entries exist, which is the problem
-        expect(cacheValue).toBeDefined()
-
-        // Both cache entries have the same person id but different properties
-        expect(cacheValue?.id).toBe(sharedPerson.id)
-        expect(cacheValue?.properties).toEqual({
-            initial_prop: 'initial_value',
-        }) // Original properties from database
-        expect(cacheValue?.properties_to_set).toEqual({
-            initial_prop: 'initial_value',
-            prop_from_distinctId1: 'value1',
-            prop_from_distinctId2: 'value2',
-        }) // Properties to set
-        expect(cacheValue?.properties_to_unset).toEqual([]) // Properties to unset
-
-        expect(cache.size).toBe(1)
-
-        // Flush should consolidate these into a single DB update
-        await personStoreForBatch.flush()
-
-        // ISSUE: Currently this will likely result in 2 separate DB calls for the same person
-        // or only one of the updates will be applied, leading to incomplete data
-        // expect(db.updatePerson).toHaveBeenCalledTimes(1)
-
-        // The updatePerson call should have the correct properties
-        expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-        expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-            expect.objectContaining({
-                id: sharedPerson.id,
-                properties: {
-                    initial_prop: 'initial_value',
-                    prop_from_distinctId1: 'value1',
-                    prop_from_distinctId2: 'value2',
-                },
-            }),
-            // Only mutable fields should be in the update object
-            expect.objectContaining({
-                properties: {
-                    initial_prop: 'initial_value',
-                    prop_from_distinctId1: 'value1',
-                    prop_from_distinctId2: 'value2',
-                },
-                is_identified: expect.any(Boolean),
-            }),
-            'updatePersonNoAssert'
-        )
-    })
-
-    it('should handle set/unset conflicts when merging updates for same person via different distinct IDs', async () => {
-        // This test validates that when two distinct IDs point to the same person,
-        // and one unsets a property while the other sets it, the conflict is resolved correctly
-        const distinctId1 = 'user-email@example.com'
-        const distinctId2 = 'user-device-abc123'
-
-        const sharedPerson = {
-            ...person,
-            properties: {
-                existing_prop: 'existing_value',
-            },
-        }
-
-        const mockRepo = createMockRepository()
-        mockRepo.fetchPerson = jest.fn().mockImplementation(() => {
-            return Promise.resolve(sharedPerson)
-        })
-        const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-        const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-        // Update via first distinct ID - unset 'conflicting_prop'
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            sharedPerson,
-            {},
-            ['conflicting_prop'],
-            {},
-            distinctId1
-        )
-
-        // Update via second distinct ID - set 'conflicting_prop' (should win over unset)
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            sharedPerson,
-            { conflicting_prop: 'new_value' },
-            [],
-            {},
-            distinctId2
-        )
-
-        const cache = personStoreForBatch.getUpdateCache()
-        const cacheValue = cache.get(`${teamId}:${sharedPerson.id}`)
-
-        expect(cacheValue).toBeDefined()
-        // The set should win - property should be in properties_to_set and NOT in properties_to_unset
-        expect(cacheValue?.properties_to_set).toEqual({
-            existing_prop: 'existing_value',
-            conflicting_prop: 'new_value',
-        })
-        expect(cacheValue?.properties_to_unset).toEqual([])
-
-        await personStoreForBatch.flush()
-
-        expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-        expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-            expect.objectContaining({
-                properties: {
-                    existing_prop: 'existing_value',
-                    conflicting_prop: 'new_value',
-                },
-            }),
-            expect.anything(),
-            'updatePersonNoAssert'
-        )
-    })
-
-    it('should handle unset after set conflicts when merging updates for same person via different distinct IDs', async () => {
-        // This test validates that when two distinct IDs point to the same person,
-        // and one sets a property while the other unsets it (in that order), the unset wins
-        const distinctId1 = 'user-email@example.com'
-        const distinctId2 = 'user-device-abc123'
-
-        const sharedPerson = {
-            ...person,
-            properties: {
-                existing_prop: 'existing_value',
-            },
-        }
-
-        const mockRepo = createMockRepository()
-        mockRepo.fetchPerson = jest.fn().mockImplementation(() => {
-            return Promise.resolve(sharedPerson)
-        })
-        const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-        const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-        // Update via first distinct ID - set 'conflicting_prop'
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            sharedPerson,
-            { conflicting_prop: 'some_value' },
-            [],
-            {},
-            distinctId1
-        )
-
-        // Update via second distinct ID - unset 'conflicting_prop' (should win over set)
-        await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-            sharedPerson,
-            {},
-            ['conflicting_prop'],
-            {},
-            distinctId2
-        )
-
-        const cache = personStoreForBatch.getUpdateCache()
-        const cacheValue = cache.get(`${teamId}:${sharedPerson.id}`)
-
-        expect(cacheValue).toBeDefined()
-        // The unset should win - property should be in properties_to_unset and NOT in properties_to_set
-        expect(cacheValue?.properties_to_set).toEqual({
-            existing_prop: 'existing_value',
-        })
-        expect(cacheValue?.properties_to_unset).toEqual(['conflicting_prop'])
-
-        await personStoreForBatch.flush()
-
-        expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-        expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-            expect.objectContaining({
-                properties: {
-                    existing_prop: 'existing_value',
-                },
-            }),
-            expect.anything(),
-            'updatePersonNoAssert'
-        )
-    })
-
-    describe('moveDistinctIds', () => {
-        it('should preserve cached merged properties when moving distinct IDs', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Create target person with some initial properties
-            const targetPerson: InternalPerson = {
-                ...person,
-                id: 'target-id',
-                properties: {
-                    target_prop: 'target_value',
-                    existing_target_prop: 'existing_target_value',
-                },
-                version: 5,
-                is_identified: false,
-            }
-
-            const sourcePerson: InternalPerson = {
-                ...person,
-                id: 'source-id',
-                properties: {
-                    source_prop: 'source_value',
-                },
-                version: 4,
-                is_identified: true,
-            }
-
-            // Step 1: Cache the target person (simulating fetchForUpdate)
-            personStoreForBatch.setCachedPersonForUpdate(
-                teamId,
-                'target-distinct',
-                fromInternalPerson(targetPerson, 'target-distinct')
-            )
-
-            // Step 2: Update target person with merged properties (simulating updatePersonForMerge)
-            const mergeUpdate = {
-                properties: {
-                    source_prop: 'source_value',
-                    rich_property: 'rich_value',
-                    merged_from_source: 'merged_value',
-                },
-                is_identified: true,
-            }
-            await personStoreForBatch.updatePersonForMerge(targetPerson, mergeUpdate, 'target-distinct')
-
-            // Verify the merge worked - check the final computed result
-            const cacheAfterMerge = personStoreForBatch.getCachedPersonForUpdateByDistinctId(teamId, 'target-distinct')
-            expect(cacheAfterMerge?.properties).toEqual({
-                target_prop: 'target_value',
-                existing_target_prop: 'existing_target_value',
-            }) // Original properties from database
-            expect(cacheAfterMerge?.properties_to_set).toEqual({
-                source_prop: 'source_value',
-                rich_property: 'rich_value',
-                merged_from_source: 'merged_value',
-                target_prop: 'target_value',
-                existing_target_prop: 'existing_target_value',
-            }) // Properties to set
-            expect(cacheAfterMerge?.properties_to_unset).toEqual([]) // Properties to unset
-            expect(cacheAfterMerge?.is_identified).toBe(true)
-
-            // Step 3: moveDistinctIds - this should preserve the merged cache
-            const tx = createMockTransaction() as any
-            await personStoreForBatch.moveDistinctIds(sourcePerson, targetPerson, 'target-distinct', undefined, tx)
-
-            // Verify the repository method was called
-            // moveDistinctIds is executed via tx, not repo
-            expect(tx.moveDistinctIds).toHaveBeenCalledTimes(1)
-            expect(tx.moveDistinctIds).toHaveBeenCalledWith(sourcePerson, targetPerson, undefined)
-
-            // Step 4: Verify that cached merged properties are preserved
-            const cacheAfterMove = personStoreForBatch.getCachedPersonForUpdateByDistinctId(teamId, 'target-distinct')
-            expect(cacheAfterMove?.properties).toEqual({
-                target_prop: 'target_value',
-                existing_target_prop: 'existing_target_value',
-            })
-            expect(cacheAfterMove?.properties_to_set).toEqual({
-                source_prop: 'source_value',
-                rich_property: 'rich_value',
-                merged_from_source: 'merged_value',
-                target_prop: 'target_value',
-                existing_target_prop: 'existing_target_value',
-            })
-            expect(cacheAfterMove?.properties_to_unset).toEqual([])
-            expect(cacheAfterMove?.is_identified).toBe(true)
-            expect(cacheAfterMove?.distinct_id).toBe('target-distinct')
-
-            // Verify the source cache is cleared
-            const sourceCacheAfterMove = personStoreForBatch.getCachedPersonForUpdateByPersonId(teamId, sourcePerson.id)
-            expect(sourceCacheAfterMove).toBeUndefined()
-        })
-
-        it('should create fresh cache when no existing cache exists', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const targetPerson: InternalPerson = {
-                ...person,
-                id: 'target-id',
-                properties: {
-                    target_prop: 'target_value',
-                },
-                version: 5,
-            }
-
-            const sourcePerson: InternalPerson = {
-                ...person,
-                id: 'source-id',
-                properties: {
-                    source_prop: 'source_value',
-                },
-                version: 4,
-            }
-
-            // No existing cache for target person
-            expect(personStoreForBatch.getCachedPersonForUpdateByPersonId(teamId, targetPerson.id)).toBeUndefined()
-
-            // Move distinct IDs
-            const tx = createMockTransaction() as any
-            await personStoreForBatch.moveDistinctIds(sourcePerson, targetPerson, 'target-distinct', undefined, tx)
-
-            // Verify the repository method was called
-            expect(tx.moveDistinctIds).toHaveBeenCalledTimes(1)
-            expect(tx.moveDistinctIds).toHaveBeenCalledWith(sourcePerson, targetPerson, undefined)
-
-            // Should create fresh cache from target person
-            const cacheAfterMove = personStoreForBatch.getCachedPersonForUpdateByDistinctId(teamId, 'target-distinct')
-            expect(cacheAfterMove?.properties).toEqual({
-                target_prop: 'target_value',
-            })
-            expect(cacheAfterMove?.id).toBe(targetPerson.id)
-            expect(cacheAfterMove?.distinct_id).toBe('target-distinct')
-        })
-
-        it('should clear source person cache', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const targetPerson: InternalPerson = {
-                ...person,
-                id: 'target-id',
-                properties: { target_prop: 'target_value' },
-                version: 5,
-            }
-
-            const sourcePerson: InternalPerson = {
-                ...person,
-                id: 'source-id',
-                properties: { source_prop: 'source_value' },
-                version: 4,
-            }
-
-            // Set up cache for source person
-            personStoreForBatch.setCachedPersonForUpdate(
-                teamId,
-                'source-distinct',
-                fromInternalPerson(sourcePerson, 'source-distinct')
-            )
-
-            // Verify source cache exists
-            expect(personStoreForBatch.getCachedPersonForUpdateByPersonId(teamId, sourcePerson.id)).toBeDefined()
-
-            // Move distinct IDs
-            const tx = createMockTransaction() as any
-            await personStoreForBatch.moveDistinctIds(sourcePerson, targetPerson, 'target-distinct', undefined, tx)
-
-            // Verify the repository method was called
-            expect(tx.moveDistinctIds).toHaveBeenCalledTimes(1)
-            expect(tx.moveDistinctIds).toHaveBeenCalledWith(sourcePerson, targetPerson, undefined)
-
-            // Verify source cache is cleared
-            expect(personStoreForBatch.getCachedPersonForUpdateByPersonId(teamId, sourcePerson.id)).toBeUndefined()
-        })
-
-        it('should handle complex merge scenario with multiple properties', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const targetPerson: InternalPerson = {
-                ...person,
-                id: 'target-id',
-                properties: {
-                    target_prop: 'target_value',
-                    shared_prop: 'original_value',
-                    target_only: 'target_only_value',
-                },
-                version: 5,
-                is_identified: false,
-            }
-
-            const sourcePerson: InternalPerson = {
-                ...person,
-                id: 'source-id',
-                properties: {
-                    source_prop: 'source_value',
-                    shared_prop: 'updated_value',
-                    source_only: 'source_only_value',
-                },
-                version: 4,
-                is_identified: true,
-            }
-
-            // Step 1: Cache target person
-            personStoreForBatch.setCachedPersonForUpdate(
-                teamId,
-                'target-distinct',
-                fromInternalPerson(targetPerson, 'target-distinct')
-            )
-
-            // Step 2: Multiple merge operations
-            await personStoreForBatch.updatePersonForMerge(
-                targetPerson,
-                {
-                    properties: {
-                        source_prop: 'source_value',
-                        shared_prop: 'updated_value', // This should override
-                    },
-                    is_identified: true,
-                },
-                'target-distinct'
-            )
-
-            await personStoreForBatch.updatePersonForMerge(
-                targetPerson,
-                {
-                    properties: {
-                        additional_prop: 'additional_value',
-                        source_only: 'source_only_value',
-                    },
-                },
-                'target-distinct'
-            )
-
-            // Step 3: moveDistinctIds
-            const tx = createMockTransaction() as any
-            await personStoreForBatch.moveDistinctIds(sourcePerson, targetPerson, 'target-distinct', undefined, tx)
-
-            // Verify the repository method was called
-            expect(tx.moveDistinctIds).toHaveBeenCalledTimes(1)
-            expect(tx.moveDistinctIds).toHaveBeenCalledWith(sourcePerson, targetPerson, undefined)
-
-            // Step 4: Verify all merged properties are preserved
-            const finalCache = personStoreForBatch.getCachedPersonForUpdateByDistinctId(teamId, 'target-distinct')
-            expect(finalCache?.properties).toEqual({
-                target_prop: 'target_value',
-                shared_prop: 'original_value',
-                target_only: 'target_only_value',
-            })
-            expect(finalCache?.is_identified).toBe(true)
-            expect(finalCache?.properties_to_set).toEqual({
-                source_prop: 'source_value',
-                shared_prop: 'updated_value',
-                additional_prop: 'additional_value',
-                source_only: 'source_only_value',
-                target_only: 'target_only_value',
-                target_prop: 'target_value',
-            })
-            expect(finalCache?.properties_to_unset).toEqual([])
-        })
-    })
-
-    describe('addPersonlessDistinctId', () => {
-        it('should call repository method and return result', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const result = await personStoreForBatch.addPersonlessDistinctId(teamId, 'test-distinct')
-
-            expect(mockRepo.addPersonlessDistinctId).toHaveBeenCalledWith(teamId, 'test-distinct')
-            expect(result).toBe(true)
-        })
-
-        it('should handle repository returning false', async () => {
-            const mockRepo = createMockRepository()
-            mockRepo.addPersonlessDistinctId = jest.fn().mockResolvedValue(false)
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const result = await personStoreForBatch.addPersonlessDistinctId(teamId, 'test-distinct')
-
-            expect(mockRepo.addPersonlessDistinctId).toHaveBeenCalledWith(teamId, 'test-distinct')
-            expect(result).toBe(false)
-        })
-    })
-
-    describe('addPersonlessDistinctIdForMerge', () => {
-        it('should call repository method and return result', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const result = await personStoreForBatch.addPersonlessDistinctIdForMerge(teamId, 'test-distinct')
-
-            expect(mockRepo.addPersonlessDistinctIdForMerge).toHaveBeenCalledWith(teamId, 'test-distinct')
-            expect(result).toBe(true)
-        })
-
-        it('should handle repository returning false', async () => {
-            const mockRepo = createMockRepository()
-            mockRepo.addPersonlessDistinctIdForMerge = jest.fn().mockResolvedValue(false)
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const result = await personStoreForBatch.addPersonlessDistinctIdForMerge(teamId, 'test-distinct')
-
-            expect(mockRepo.addPersonlessDistinctIdForMerge).toHaveBeenCalledWith(teamId, 'test-distinct')
-            expect(result).toBe(false)
-        })
-    })
-
-    describe('personPropertiesSize', () => {
-        it('should call repository method and return result', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-            const personId = 'test-person-id'
-            const teamId = 1
-
-            const result = await personStoreForBatch.personPropertiesSize(personId, teamId)
-
-            expect(mockRepo.personPropertiesSize).toHaveBeenCalledWith(personId, teamId)
-            expect(result).toBe(1024)
-        })
-
-        it('should handle repository returning 0', async () => {
-            const mockRepo = createMockRepository()
-            mockRepo.personPropertiesSize = jest.fn().mockResolvedValue(0)
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-            const personId = 'test-person-id'
-            const teamId = 1
-
-            const result = await personStoreForBatch.personPropertiesSize(personId, teamId)
-
-            expect(mockRepo.personPropertiesSize).toHaveBeenCalledWith(personId, teamId)
-            expect(result).toBe(0)
-        })
-    })
-
-    describe('updateCohortsAndFeatureFlagsForMerge', () => {
-        it('should call repository method with correct arguments', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const teamID = 1
-            const sourcePersonID = 'source-person-id'
-            const targetPersonID = 'target-person-id'
-            const distinctId = 'test-distinct'
-
-            await personStoreForBatch.updateCohortsAndFeatureFlagsForMerge(
-                teamID,
-                sourcePersonID,
-                targetPersonID,
-                distinctId
-            )
-
-            expect(mockRepo.updateCohortsAndFeatureFlagsForMerge).toHaveBeenCalledWith(
-                teamID,
-                sourcePersonID,
-                targetPersonID
-            )
-        })
-
-        it('should call repository method with transaction when provided', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const teamID = 1
-            const sourcePersonID = 'source-person-id'
-            const targetPersonID = 'target-person-id'
-            const distinctId = 'test-distinct'
-
-            const mockTransaction = createMockTransaction()
-
-            await personStoreForBatch.updateCohortsAndFeatureFlagsForMerge(
-                teamID,
-                sourcePersonID,
-                targetPersonID,
-                distinctId,
-                mockTransaction
-            )
-
-            // Verify the transaction was called instead of the repository
-            expect(mockTransaction.updateCohortsAndFeatureFlagsForMerge).toHaveBeenCalledWith(
-                teamID,
-                sourcePersonID,
-                targetPersonID
-            )
-
-            // Verify the repository was NOT called
-            expect(mockRepo.updateCohortsAndFeatureFlagsForMerge).not.toHaveBeenCalled()
-        })
-    })
-
-    describe('property filtering at batch level', () => {
-        const mockPersonProfileBatchUpdateOutcomeCounter = personProfileBatchUpdateOutcomeCounter as jest.Mocked<
-            typeof personProfileBatchUpdateOutcomeCounter
-        >
-        const mockPersonProfileBatchIgnoredPropertiesCounter =
-            personProfileBatchIgnoredPropertiesCounter as jest.Mocked<typeof personProfileBatchIgnoredPropertiesCounter>
-        const mockPersonPropertyKeyUpdateCounter = personPropertyKeyUpdateCounter as jest.Mocked<
-            typeof personPropertyKeyUpdateCounter
-        >
-
-        it('should skip database write when only filtered properties are updated', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Update person with only filtered properties (existing properties being updated)
-            // Using $current_url and $pathname which are in FILTERED_PERSON_UPDATE_PROPERTIES
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                { ...person, properties: { $current_url: 'https://old.com', $pathname: '/old' } },
-                { $current_url: 'https://new.com', $pathname: '/new' },
-                [],
-                {},
-                'test'
-            )
-
-            // Flush should skip the database write since only filtered properties changed
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).not.toHaveBeenCalled()
-            expect(mockRepo.updatePersonAssertVersion).not.toHaveBeenCalled()
-
-            // Verify metrics
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'ignored' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'ignored' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledTimes(2)
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
-                property: '$current_url',
-            })
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
-                property: '$pathname',
-            })
-            // personPropertyKeyUpdateCounter should NOT be called for 'ignored' outcomes
-            expect(mockPersonPropertyKeyUpdateCounter.labels).not.toHaveBeenCalled()
-        })
-
-        it('should skip database write when only blocked $geoip_* properties are updated', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Update person with only blocked geoip properties (existing properties being updated)
-            // Note: $geoip_country_name and $geoip_city_name are allowed, but $geoip_latitude is blocked
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                { ...person, properties: { $geoip_latitude: 40.7128, $geoip_longitude: -74.006 } },
-                { $geoip_latitude: 37.7749, $geoip_longitude: -74.006 },
-                [],
-                {},
-                'test'
-            )
-
-            // Flush should skip the database write
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).not.toHaveBeenCalled()
-            expect(mockRepo.updatePersonAssertVersion).not.toHaveBeenCalled()
-
-            // Verify metrics
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'ignored' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'ignored' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
-                property: '$geoip_latitude',
-            })
-            // personPropertyKeyUpdateCounter should NOT be called for 'ignored' outcomes
-            expect(mockPersonPropertyKeyUpdateCounter.labels).not.toHaveBeenCalled()
-        })
-
-        it('should write to database when filtered properties are NEW (not in existing properties)', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Person without browser property
-            const personWithoutBrowser = { ...person, properties: { name: 'John' } }
-
-            // Update person with NEW eventToPersonProperty
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithoutBrowser,
-                { $browser: 'Chrome' },
-                [],
-                {},
-                'test'
-            )
-
-            // Flush SHOULD write to database since it's a new property
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-            expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-                expect.objectContaining({
-                    properties: { name: 'John', $browser: 'Chrome' },
-                }),
-                expect.anything(),
-                'updatePersonNoAssert'
-            )
-
-            // Verify metrics - should be 'changed' since new property triggers write
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'changed' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            // Note: $browser would be ignored at event level (see person-update.ts), but filtering happens at batch level
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            // personPropertyKeyUpdateCounter should be called for new property
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: '$browser' })
-            expect(mockPersonPropertyKeyUpdateCounter.labels({ key: '$browser' }).inc).toHaveBeenCalledTimes(1)
-        })
-
-        it('should write to database when mixing filtered and non-filtered properties', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Update person with both filtered and non-filtered properties
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                { ...person, properties: { $browser: 'Firefox', name: 'Jane' } },
-                { $browser: 'Chrome', name: 'John' },
-                [],
-                {},
-                'test'
-            )
-
-            // Flush SHOULD write to database because name is not filtered
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-
-            // Verify metrics - should be 'changed' since non-filtered property triggers write
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'changed' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            // Note: $browser would be ignored at event level (see person-update.ts), but filtering happens at batch level
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            // personPropertyKeyUpdateCounter should be called for both properties
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledTimes(2)
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: '$browser' })
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: 'other' })
-        })
-
-        it('should write to database when unsetting any property', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Unset a filtered property
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                { ...person, properties: { $browser: 'Chrome' } },
-                {},
-                ['$browser'],
-                {},
-                'test'
-            )
-
-            // Flush SHOULD write to database because unsetting always triggers a write
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-
-            // Verify metrics - should be 'changed' since unsetting triggers write
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'changed' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            // Note: $browser would be ignored at event level (see person-update.ts), but filtering happens at batch level
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            // personPropertyKeyUpdateCounter should be called for unset property
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: '$browser' })
-            expect(mockPersonPropertyKeyUpdateCounter.labels({ key: '$browser' }).inc).toHaveBeenCalledTimes(1)
-        })
-
-        it('should write to database when force_update is set even with only filtered properties', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Update person with only filtered properties but with force_update=true (simulating $identify/$set events)
-            // Using $current_url and $pathname which are in FILTERED_PERSON_UPDATE_PROPERTIES
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                { ...person, properties: { $current_url: 'https://old.com', $pathname: '/old' } },
-                { $current_url: 'https://new.com', $pathname: '/new' },
-                [],
-                {},
-                'test',
-                true // force_update=true for $identify/$set events
-            )
-
-            // Flush SHOULD write to database because force_update bypasses filtering
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-
-            // Verify metrics - should be 'changed' because force_update bypasses filtering
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'changed' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            // With force_update, properties should not be marked as ignored
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            // personPropertyKeyUpdateCounter should be called for the updated properties
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledTimes(2)
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: '$current_url' })
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: '$pathname' })
-        })
-
-        it('integration: multiple events with only filtered properties should not trigger database write', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Using properties that are in FILTERED_PERSON_UPDATE_PROPERTIES
-            const personWithFiltered = {
-                ...person,
-                properties: {
-                    $current_url: 'https://old.com',
-                    $pathname: '/old',
-                    $geoip_latitude: 40.7128,
-                },
-            }
-
-            // Event 1: Update current_url (filtered)
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $current_url: 'https://new.com' },
-                [],
-                {},
-                'test'
-            )
-
-            // Event 2: Update pathname (filtered)
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $pathname: '/new' },
-                [],
-                {},
-                'test'
-            )
-
-            // Event 3: Update blocked geoip property (latitude is blocked, city_name is allowed)
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $geoip_latitude: 37.7749 },
-                [],
-                {},
-                'test'
-            )
-
-            // Flush should NOT write to database - all properties are filtered
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).not.toHaveBeenCalled()
-            expect(mockRepo.updatePersonAssertVersion).not.toHaveBeenCalled()
-
-            // Verify metrics - should be 'ignored' since all properties are filtered
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'ignored' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'ignored' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledTimes(3)
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
-                property: '$current_url',
-            })
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
-                property: '$pathname',
-            })
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
-                property: '$geoip_latitude',
-            })
-            // personPropertyKeyUpdateCounter should NOT be called for 'ignored' outcomes
-            expect(mockPersonPropertyKeyUpdateCounter.labels).not.toHaveBeenCalled()
-        })
-
-        it('should write to database when allowed geoip property ($geoip_country_name) is updated alongside blocked ones', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Person with existing geoip properties
-            const personWithGeoip = {
-                ...person,
-                properties: {
-                    $geoip_country_name: 'Canada',
-                    $geoip_city_name: 'Toronto',
-                    $geoip_latitude: 43.6532,
-                    $geoip_longitude: -79.3832,
-                },
-            }
-
-            // Update all geoip properties including allowed ones (country_name, city_name)
-            // Since $geoip_country_name is allowed, all properties should be updated
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithGeoip,
-                {
-                    $geoip_country_name: 'United States',
-                    $geoip_city_name: 'San Francisco',
-                    $geoip_latitude: 37.7749,
-                    $geoip_longitude: -122.4194,
-                },
-                [],
-                {},
-                'test'
-            )
-
-            // Flush SHOULD write to database because $geoip_country_name is allowed
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-            expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-                expect.objectContaining({
-                    properties: {
-                        $geoip_country_name: 'United States',
-                        $geoip_city_name: 'San Francisco',
-                        $geoip_latitude: 37.7749,
-                        $geoip_longitude: -122.4194,
-                    },
-                }),
-                expect.anything(),
-                'updatePersonNoAssert'
-            )
-
-            // Verify metrics - should be 'changed' since allowed geoip property triggers write
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            // personPropertyKeyUpdateCounter uses getMetricKey which returns 'geoIP' for all $geoip_* properties
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: 'geoIP' })
-        })
-
-        it('integration: filtered properties then non-filtered property should trigger database write', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const personWithFiltered = {
-                ...person,
-                properties: {
-                    $browser: 'Firefox',
-                    $app_build: '100',
-                    $os: 'Windows',
-                    name: 'Jane',
-                },
-            }
-
-            // Event 1: Update browser (filtered)
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $browser: 'Chrome' },
-                [],
-                {},
-                'test'
-            )
-
-            // Event 2: Update app build (filtered)
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $app_build: '200' },
-                [],
-                {},
-                'test'
-            )
-
-            // Event 3: Update name (NOT filtered)
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { name: 'John' },
-                [],
-                {},
-                'test'
-            )
-
-            // Event 4: Update more filtered properties
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $os: 'macOS' },
-                [],
-                {},
-                'test'
-            )
-
-            // Flush SHOULD write to database because event 3 has non-filtered property
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-            expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-                expect.objectContaining({
-                    properties: {
-                        $browser: 'Chrome',
-                        $app_build: '200',
-                        $os: 'macOS',
-                        name: 'John',
-                    },
-                }),
-                expect.anything(),
-                'updatePersonNoAssert'
-            )
-
-            // Verify metrics - should be 'changed' since non-filtered property triggers write
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'changed' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            // Note: some properties would be ignored at event level (see person-update.ts), but filtering happens at batch level
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            // personPropertyKeyUpdateCounter should be called for all 4 properties
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledTimes(4)
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: '$browser' })
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: '$app_build' })
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: '$os' })
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: 'other' })
-        })
-
-        it('integration: normal events for regression - custom properties always trigger writes', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Event 1: Normal custom property
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                person,
-                { plan: 'premium' },
-                [],
-                {},
-                'test'
-            )
-
-            // Event 2: Another custom property
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                person,
-                { subscription_status: 'active' },
-                [],
-                {},
-                'test'
-            )
-
-            // Flush SHOULD write to database
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-            expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-                expect.objectContaining({
-                    properties: {
-                        test: 'test',
-                        plan: 'premium',
-                        subscription_status: 'active',
-                    },
-                }),
-                expect.anything(),
-                'updatePersonNoAssert'
-            )
-
-            // Verify metrics - should be 'changed' since custom properties trigger write
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'changed' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            // Note: custom properties are never ignored at event level (see person-update.ts)
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            // personPropertyKeyUpdateCounter should be called once for 'other' (deduplicated by Set)
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonPropertyKeyUpdateCounter.labels).toHaveBeenCalledWith({ key: 'other' })
-            expect(mockPersonPropertyKeyUpdateCounter.labels({ key: 'other' }).inc).toHaveBeenCalledTimes(1)
-        })
-
-        it('integration: chain of events - normal event (ignored), $identify event (forces update), then normal event (also written)', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            const personWithFiltered = {
-                ...person,
-                properties: {
-                    $browser: 'Firefox',
-                    utm_source: 'twitter',
-                    $geoip_city_name: 'New York',
-                },
-            }
-
-            // Event 1: Normal pageview event with filtered properties
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $browser: 'Chrome', utm_source: 'google' },
-                [],
-                {},
-                'test'
-            )
-
-            // Event 2: $identify event with ONLY filtered properties - should force update
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $geoip_city_name: 'San Francisco', $browser: 'Safari' },
-                [],
-                {},
-                'test',
-                true // forceUpdate=true ($identify event)
-            )
-
-            // Event 3: Another normal event with filtered properties
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { utm_source: 'facebook' },
-                [],
-                {},
-                'test'
-            )
-
-            // Flush SHOULD write to database because $identify event set force_update=true
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).toHaveBeenCalledTimes(1)
-
-            // Verify that ALL property changes from all three events are written
-            expect(mockRepo.updatePerson).toHaveBeenCalledWith(
-                expect.objectContaining({
-                    properties: expect.objectContaining({
-                        $browser: 'Safari',
-                        utm_source: 'facebook',
-                        $geoip_city_name: 'San Francisco',
-                    }),
-                }),
-                expect.anything(),
-                'updatePersonNoAssert'
-            )
-        })
-
-        it('integration: chain without $identify/$set should not trigger update', async () => {
-            const mockRepo = createMockRepository()
-            const testPersonStore = new BatchWritingPersonsStore(mockRepo, db.kafkaProducer)
-            const personStoreForBatch = testPersonStore.forBatch() as BatchWritingPersonsStoreForBatch
-
-            // Using properties that are in FILTERED_PERSON_UPDATE_PROPERTIES
-            const personWithFiltered = {
-                ...person,
-                properties: {
-                    $current_url: 'https://old.com',
-                    $pathname: '/old',
-                },
-            }
-
-            // Event 1: Normal event with filtered properties
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $current_url: 'https://new.com' },
-                [],
-                {},
-                'test'
-                // forceUpdate not set
-            )
-
-            // Event 2: Another normal event with filtered properties
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $pathname: '/new' },
-                [],
-                {},
-                'test'
-                // forceUpdate not set
-            )
-
-            // Event 3: Yet another normal event with filtered properties
-            await personStoreForBatch.updatePersonWithPropertiesDiffForUpdate(
-                personWithFiltered,
-                { $current_url: 'https://another.com' },
-                [],
-                {},
-                'test'
-                // forceUpdate not set
-            )
-
-            // Flush should NOT write to database - all events are normal with only filtered properties
-            await personStoreForBatch.flush()
-
-            expect(mockRepo.updatePerson).not.toHaveBeenCalled()
-            expect(mockRepo.updatePersonAssertVersion).not.toHaveBeenCalled()
-
-            // Verify metrics - should be 'ignored' since all properties are filtered and no force_update
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledTimes(1)
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'ignored' })
-            expect(mockPersonProfileBatchUpdateOutcomeCounter.labels({ outcome: 'ignored' }).inc).toHaveBeenCalledTimes(
-                1
-            )
-            // Properties should be marked as ignored
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledTimes(2)
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
-                property: '$current_url',
-            })
-            expect(mockPersonProfileBatchIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
-                property: '$pathname',
-            })
-            // personPropertyKeyUpdateCounter should NOT be called for 'ignored' outcomes
-            expect(mockPersonPropertyKeyUpdateCounter.labels).not.toHaveBeenCalled()
-        })
-    })
-})
+        await personStoreForBatch.updatePersonWithPropertiesDiffFor
\ No newline at end of file
diff --git a/plugin-server/src/worker/ingestion/persons/batch-writing-person-store.ts b/plugin-server/src/worker/ingestion/persons/batch-writing-person-store.ts
index 958f258..0560552 100644
--- a/plugin-server/src/worker/ingestion/persons/batch-writing-person-store.ts
+++ b/plugin-server/src/worker/ingestion/persons/batch-writing-person-store.ts
@@ -384,944 +384,4 @@ export class BatchWritingPersonsStoreForBatch implements PersonsStoreForBatch, B
                                     fallback_reason: 'max_retries',
                                 })
 
-                                const fallbackResult = await this.updatePersonNoAssert(error.latestPersonUpdate)
-                                const fallbackMessages = fallbackResult.success ? fallbackResult.messages : []
-
-                                personWriteMethodAttemptCounter.inc({
-                                    db_write_mode: this.options.dbWriteMode,
-                                    method: 'fallback',
-                                    outcome: 'success',
-                                })
-
-                                return fallbackMessages.map((message) => ({
-                                    topicMessage: message,
-                                    teamId: error.latestPersonUpdate.team_id,
-                                    uuid: error.latestPersonUpdate.uuid,
-                                    distinctId: error.latestPersonUpdate.distinct_id,
-                                }))
-                            }
-
-                            // Re-throw any other errors
-                            throw error
-                        }
-                    }).catch((error) => {
-                        logger.error('Failed to update person after max retries and direct update fallback', {
-                            error,
-                            cacheKey,
-                            teamId: update.team_id,
-                            personId: update.id,
-                            distinctId: update.distinct_id,
-                            errorMessage: error instanceof Error ? error.message : String(error),
-                            errorStack: error instanceof Error ? error.stack : undefined,
-                        })
-
-                        personWriteMethodAttemptCounter.inc({
-                            db_write_mode: this.options.dbWriteMode,
-                            method: 'fallback',
-                            outcome: 'error',
-                        })
-                        throw error
-                    })
-                )
-            )
-
-            // Flatten all Kafka messages from all operations
-            const allKafkaMessages = results.flat()
-
-            // Record successful flush
-            const flushLatency = (performance.now() - flushStartTime) / 1000
-            personFlushLatencyHistogram.observe({ db_write_mode: this.options.dbWriteMode }, flushLatency)
-            personFlushOperationsCounter.inc({ db_write_mode: this.options.dbWriteMode, outcome: 'success' })
-
-            return allKafkaMessages
-        } catch (error) {
-            // Record failed flush
-            const flushLatency = (performance.now() - flushStartTime) / 1000
-            personFlushLatencyHistogram.observe({ db_write_mode: this.options.dbWriteMode }, flushLatency)
-            personFlushOperationsCounter.inc({ db_write_mode: this.options.dbWriteMode, outcome: 'error' })
-
-            logger.error('Failed to flush person updates', {
-                error,
-                errorMessage: error instanceof Error ? error.message : String(error),
-                errorStack: error instanceof Error ? error.stack : undefined,
-            })
-            throw error
-        }
-    }
-
-    async inTransaction<T>(description: string, transaction: (tx: PersonsStoreTransaction) => Promise<T>): Promise<T> {
-        return await this.personRepository.inTransaction(description, async (tx) => {
-            const transactionWrapper = new PersonsStoreTransaction(this, tx)
-            return await transaction(transactionWrapper)
-        })
-    }
-
-    async fetchForChecking(teamId: Team['id'], distinctId: string): Promise<InternalPerson | null> {
-        this.incrementCount('fetchForChecking', distinctId)
-
-        // First check the main cache
-        const cachedPerson = this.getCachedPersonForUpdateByDistinctId(teamId, distinctId)
-        if (cachedPerson !== undefined) {
-            return cachedPerson === null ? null : toInternalPerson(cachedPerson)
-        }
-
-        // Then check the checking-specific cache
-        const checkCachedPerson = this.getCheckCachedPerson(teamId, distinctId)
-        if (checkCachedPerson !== undefined) {
-            return checkCachedPerson
-        }
-
-        const cacheKey = this.getDistinctCacheKey(teamId, distinctId)
-        let fetchPromise = this.fetchPromisesForChecking.get(cacheKey)
-        if (!fetchPromise) {
-            personFetchForCheckingCacheOperationsCounter.inc({ operation: 'miss' })
-            fetchPromise = (async () => {
-                try {
-                    this.incrementDatabaseOperation('fetchForChecking', distinctId)
-                    const start = performance.now()
-                    const person = await this.personRepository.fetchPerson(teamId, distinctId, { useReadReplica: true })
-                    observeLatencyByVersion(person, start, 'fetchForChecking')
-                    this.setCheckCachedPerson(teamId, distinctId, person ?? null)
-                    return person ?? null
-                } finally {
-                    this.fetchPromisesForChecking.delete(cacheKey)
-                }
-            })()
-            this.fetchPromisesForChecking.set(cacheKey, fetchPromise)
-        } else {
-            personFetchForCheckingCacheOperationsCounter.inc({ operation: 'hit' })
-        }
-        return fetchPromise
-    }
-
-    async fetchForUpdate(teamId: Team['id'], distinctId: string): Promise<InternalPerson | null> {
-        this.incrementCount('fetchForUpdate', distinctId)
-
-        const cachedPerson = this.getCachedPersonForUpdateByDistinctId(teamId, distinctId)
-        if (cachedPerson !== undefined) {
-            return cachedPerson === null ? null : toInternalPerson(cachedPerson)
-        }
-
-        const cacheKey = this.getDistinctCacheKey(teamId, distinctId)
-        let fetchPromise = this.fetchPromisesForUpdate.get(cacheKey)
-        if (!fetchPromise) {
-            personFetchForUpdateCacheOperationsCounter.inc({ operation: 'miss' })
-            fetchPromise = (async () => {
-                try {
-                    this.incrementDatabaseOperation('fetchForUpdate', distinctId)
-                    const start = performance.now()
-                    const person = await this.personRepository.fetchPerson(teamId, distinctId, {
-                        useReadReplica: false,
-                    })
-                    observeLatencyByVersion(person, start, 'fetchForUpdate')
-                    if (person !== undefined) {
-                        const personUpdate = fromInternalPerson(person, distinctId)
-                        this.setCachedPersonForUpdate(teamId, distinctId, personUpdate)
-                        return person
-                    } else {
-                        // Before caching null, check if another async operation populated
-                        // the cache while we were awaiting the DB query. This can happen when:
-                        // 1. This operation starts DB query for a distinct ID (cache empty)
-                        // 2. Another operation creates a person for that distinct ID and caches it
-                        // 3. This DB query returns null (person didn't exist when query started)
-                        // 4. Without this check, we would overwrite the other operation's cached person
-                        //
-                        // From this point, all operations are synchronous to avoid further race conditions.
-                        const currentCache = this.getCachedPersonForUpdateByDistinctId(teamId, distinctId)
-                        if (currentCache === undefined) {
-                            this.setCachedPersonForUpdate(teamId, distinctId, null)
-                            return null
-                        }
-                        return currentCache === null ? null : toInternalPerson(currentCache)
-                    }
-                } finally {
-                    this.fetchPromisesForUpdate.delete(cacheKey)
-                }
-            })()
-            this.fetchPromisesForUpdate.set(cacheKey, fetchPromise)
-        } else {
-            personFetchForUpdateCacheOperationsCounter.inc({ operation: 'hit' })
-        }
-        return fetchPromise
-    }
-
-    updatePersonForMerge(
-        person: InternalPerson,
-        update: Partial<InternalPerson>,
-        distinctId: string,
-        _tx?: PersonRepositoryTransaction
-    ): Promise<[InternalPerson, TopicMessage[], boolean]> {
-        this.incrementCount('updatePersonForMerge', distinctId)
-        return Promise.resolve(this.addPersonUpdateToBatch(person, update, distinctId))
-    }
-
-    updatePersonWithPropertiesDiffForUpdate(
-        person: InternalPerson,
-        propertiesToSet: Properties,
-        propertiesToUnset: string[],
-        otherUpdates: Partial<InternalPerson>,
-        distinctId: string,
-        forceUpdate?: boolean,
-        _tx?: PersonRepositoryTransaction
-    ): Promise<[InternalPerson, TopicMessage[], boolean]> {
-        const [updatedPerson, kafkaMessages] = this.addPersonPropertiesUpdateToBatch(
-            person,
-            propertiesToSet,
-            propertiesToUnset,
-            otherUpdates,
-            distinctId,
-            forceUpdate
-        )
-        return Promise.resolve([updatedPerson, kafkaMessages, false])
-    }
-
-    async deletePerson(
-        person: InternalPerson,
-        distinctId: string,
-        tx?: PersonRepositoryTransaction
-    ): Promise<TopicMessage[]> {
-        this.incrementCount('deletePerson', distinctId)
-        this.incrementDatabaseOperation('deletePerson', distinctId)
-        const start = performance.now()
-        const cachedPersonUpdate = this.getCachedPersonForUpdateByPersonId(person.team_id, person.id)
-        const personToDelete = cachedPersonUpdate ? toInternalPerson(cachedPersonUpdate) : person
-
-        const response = await (tx || this.personRepository).deletePerson(personToDelete)
-        observeLatencyByVersion(person, start, 'deletePerson')
-
-        // Clear ALL caches related to this person id
-        this.clearAllCachesForPersonId(person.team_id, person.id)
-
-        return response
-    }
-
-    async addDistinctId(
-        person: InternalPerson,
-        distinctId: string,
-        version: number,
-        tx?: PersonRepositoryTransaction
-    ): Promise<TopicMessage[]> {
-        this.incrementCount('addDistinctId', distinctId)
-        this.incrementDatabaseOperation('addDistinctId', distinctId)
-        const start = performance.now()
-        const response = await (tx || this.personRepository).addDistinctId(person, distinctId, version)
-        observeLatencyByVersion(person, start, 'addDistinctId')
-        this.setDistinctIdToPersonId(person.team_id, distinctId, person.id)
-        return response
-    }
-
-    async moveDistinctIds(
-        source: InternalPerson,
-        target: InternalPerson,
-        distinctId: string,
-        limit: number | undefined,
-        tx: PersonRepositoryTransaction
-    ): Promise<MoveDistinctIdsResult> {
-        this.incrementCount('moveDistinctIds', distinctId)
-        this.incrementDatabaseOperation('moveDistinctIds', distinctId)
-        const start = performance.now()
-        const response = await tx.moveDistinctIds(source, target, limit)
-        observeLatencyByVersion(target, start, 'moveDistinctIds')
-
-        // Clear the cache for the source person id to ensure deleted person isn't cached
-        this.clearAllCachesForPersonId(source.team_id, source.id)
-
-        // Update cache for the target person for the current distinct ID
-        // Check if we already have cached data for the target person that includes merged properties
-        const existingTargetCache = this.getCachedPersonForUpdateByPersonId(target.team_id, target.id)
-        if (existingTargetCache) {
-            // We have existing cached data with merged properties - preserve it
-            // Create a new PersonUpdate for this distinctId that preserves the merged data
-            const mergedPersonUpdate = { ...existingTargetCache, distinct_id: distinctId }
-            this.setCachedPersonForUpdate(target.team_id, distinctId, mergedPersonUpdate)
-        } else {
-            // No existing cache, create fresh cache from target person
-            this.setCachedPersonForUpdate(target.team_id, distinctId, fromInternalPerson(target, distinctId))
-        }
-        if (response.success) {
-            for (const distinctId of response.distinctIdsMoved) {
-                this.setDistinctIdToPersonId(target.team_id, distinctId, target.id)
-            }
-        }
-
-        return response
-    }
-
-    async fetchPersonDistinctIds(
-        person: InternalPerson,
-        distinctId: string,
-        limit: number | undefined,
-        tx: PersonRepositoryTransaction
-    ): Promise<string[]> {
-        this.incrementCount('fetchPersonDistinctIds', distinctId)
-        this.incrementDatabaseOperation('fetchPersonDistinctIds', distinctId)
-        const start = performance.now()
-        const response = await tx.fetchPersonDistinctIds(person, limit)
-        observeLatencyByVersion(person, start, 'fetchPersonDistinctIds')
-
-        return response
-    }
-
-    async updateCohortsAndFeatureFlagsForMerge(
-        teamID: Team['id'],
-        sourcePersonID: InternalPerson['id'],
-        targetPersonID: InternalPerson['id'],
-        distinctId: string,
-        tx?: PersonRepositoryTransaction
-    ): Promise<void> {
-        this.incrementCount('updateCohortsAndFeatureFlagsForMerge', distinctId)
-        await (tx || this.personRepository).updateCohortsAndFeatureFlagsForMerge(teamID, sourcePersonID, targetPersonID)
-    }
-
-    async addPersonlessDistinctId(teamId: Team['id'], distinctId: string): Promise<boolean> {
-        this.incrementCount('addPersonlessDistinctId', distinctId)
-        return await this.personRepository.addPersonlessDistinctId(teamId, distinctId)
-    }
-
-    async addPersonlessDistinctIdForMerge(
-        teamId: Team['id'],
-        distinctId: string,
-        tx?: PersonRepositoryTransaction
-    ): Promise<boolean> {
-        this.incrementCount('addPersonlessDistinctIdForMerge', distinctId)
-        return await (tx || this.personRepository).addPersonlessDistinctIdForMerge(teamId, distinctId)
-    }
-
-    async personPropertiesSize(personId: string, teamId: number): Promise<number> {
-        return await this.personRepository.personPropertiesSize(personId, teamId)
-    }
-
-    reportBatch(): void {
-        for (const [_, methodCounts] of this.methodCountsPerDistinctId.entries()) {
-            for (const [method, count] of methodCounts.entries()) {
-                personMethodCallsPerBatchHistogram.observe({ method }, count)
-            }
-        }
-
-        for (const [_, databaseOperationCounts] of this.databaseOperationCountsPerDistinctId.entries()) {
-            for (const [operation, count] of databaseOperationCounts.entries()) {
-                personDatabaseOperationsPerBatchHistogram.observe({ operation }, count)
-            }
-        }
-
-        for (const [_, updateLatencyPerDistinctIdSeconds] of this.updateLatencyPerDistinctIdSeconds.entries()) {
-            for (const [updateType, latency] of updateLatencyPerDistinctIdSeconds.entries()) {
-                totalPersonUpdateLatencyPerBatchHistogram.observe({ update_type: updateType }, latency)
-            }
-        }
-
-        personCacheOperationsCounter.inc({ cache: 'update', operation: 'hit' }, this.cacheMetrics.updateCacheHits)
-        personCacheOperationsCounter.inc({ cache: 'update', operation: 'miss' }, this.cacheMetrics.updateCacheMisses)
-        personCacheOperationsCounter.inc({ cache: 'check', operation: 'hit' }, this.cacheMetrics.checkCacheHits)
-        personCacheOperationsCounter.inc({ cache: 'check', operation: 'miss' }, this.cacheMetrics.checkCacheMisses)
-    }
-
-    // Private implementation methods
-
-    getCheckCache(): Map<string, InternalPerson | null> {
-        return this.personCheckCache
-    }
-
-    getUpdateCache(): Map<string, PersonUpdate | null> {
-        return this.personUpdateCache
-    }
-
-    private getDistinctCacheKey(teamId: number, distinctId: string): string {
-        return `${teamId}:${distinctId}`
-    }
-
-    private getPersonIdCacheKey(teamId: number, personId: string): string {
-        return `${teamId}:${personId}`
-    }
-
-    clearPersonCacheForPersonId(teamId: number, personId: string): void {
-        this.personUpdateCache.delete(this.getPersonIdCacheKey(teamId, personId))
-    }
-
-    clearAllCachesForPersonId(teamId: number, personId: string): void {
-        // Clear the person id cache
-        this.clearPersonCacheForPersonId(teamId, personId)
-
-        // Find and clear all distinct ID mappings that point to this person id
-        const distinctIdsToRemove: string[] = []
-        for (const [distinctCacheKey, mappedPersonId] of this.distinctIdToPersonId.entries()) {
-            if (mappedPersonId === personId && distinctCacheKey.startsWith(`${teamId}:`)) {
-                distinctIdsToRemove.push(distinctCacheKey)
-            }
-        }
-
-        // Remove all distinct ID mappings and their check cache entries
-        for (const distinctCacheKey of distinctIdsToRemove) {
-            this.distinctIdToPersonId.delete(distinctCacheKey)
-            this.personCheckCache.delete(distinctCacheKey)
-        }
-    }
-
-    removeDistinctIdFromCache(teamId: number, distinctId: string): void {
-        this.distinctIdToPersonId.delete(this.getDistinctCacheKey(teamId, distinctId))
-    }
-
-    clearAllCachesForDistinctId(teamId: number, distinctId: string): void {
-        const cacheKey = this.getDistinctCacheKey(teamId, distinctId)
-        const personId = this.distinctIdToPersonId.get(cacheKey)
-
-        // Clear the distinct ID mapping
-        this.distinctIdToPersonId.delete(cacheKey)
-
-        // Clear the person data if we have the id
-        if (personId) {
-            this.clearPersonCacheForPersonId(teamId, personId)
-        }
-
-        // Clear the check cache
-        this.personCheckCache.delete(cacheKey)
-    }
-
-    private getCheckCachedPerson(teamId: number, distinctId: string): InternalPerson | null | undefined {
-        const cacheKey = this.getDistinctCacheKey(teamId, distinctId)
-        const result = this.personCheckCache.get(cacheKey)
-        if (result !== undefined) {
-            this.cacheMetrics.checkCacheHits++
-            // Return a deep copy to prevent modifications from affecting the cached object
-            return result === null
-                ? null
-                : {
-                      ...result,
-                      properties: { ...result.properties },
-                      created_at: result.created_at,
-                  }
-        } else {
-            this.cacheMetrics.checkCacheMisses++
-        }
-        return result
-    }
-
-    getCachedPersonForUpdateByPersonId(teamId: number, personId: string | undefined): PersonUpdate | null | undefined {
-        if (personId === undefined) {
-            this.cacheMetrics.updateCacheMisses++
-            return undefined
-        }
-
-        const result = this.personUpdateCache.get(this.getPersonIdCacheKey(teamId, personId))
-        if (result !== undefined) {
-            this.cacheMetrics.updateCacheHits++
-            // Return a deep copy to prevent modifications from affecting the cached object
-            if (result === null) {
-                return null
-            }
-
-            return {
-                ...result,
-                properties: { ...result.properties },
-                properties_to_set: { ...result.properties_to_set },
-                properties_to_unset: [...result.properties_to_unset],
-            }
-        } else {
-            this.cacheMetrics.updateCacheMisses++
-            return undefined
-        }
-    }
-
-    getCachedPersonForUpdateByDistinctId(teamId: number, distinctId: string): PersonUpdate | null | undefined {
-        const cacheKey = this.getDistinctCacheKey(teamId, distinctId)
-        const personId = this.distinctIdToPersonId.get(cacheKey)
-
-        return this.getCachedPersonForUpdateByPersonId(teamId, personId)
-    }
-
-    setCachedPersonForUpdate(teamId: number, distinctId: string, person: PersonUpdate | null): void {
-        const cacheKey = this.getDistinctCacheKey(teamId, distinctId)
-
-        if (person === null) {
-            // Remove mappings when person is null
-            const existingPersonId = this.distinctIdToPersonId.get(cacheKey)
-            this.distinctIdToPersonId.delete(cacheKey)
-            if (existingPersonId) {
-                this.personUpdateCache.set(this.getPersonIdCacheKey(teamId, existingPersonId), null)
-            }
-            return
-        }
-
-        // Set the distinct ID -> person id mapping
-        this.distinctIdToPersonId.set(cacheKey, person.id)
-
-        // Check if we already have cached data for this person id
-        const existingPersonUpdate = this.personUpdateCache.get(this.getPersonIdCacheKey(teamId, person.id))
-
-        if (existingPersonUpdate) {
-            // Merge the properties and changesets from both updates
-            const mergedPersonUpdate = this.mergeUpdateIntoPersonUpdate(
-                existingPersonUpdate,
-                {
-                    properties: person.properties,
-                    is_identified: person.is_identified,
-                } as Partial<InternalPerson>,
-                false
-            )
-
-            // Handle fields that are specific to PersonUpdate - merge properties_to_set and properties_to_unset
-            // with proper conflict resolution (last write wins)
-            mergedPersonUpdate.properties_to_set = {
-                ...existingPersonUpdate.properties_to_set,
-                ...person.properties_to_set,
-            }
-            // Remove from properties_to_set any keys that are in the incoming properties_to_unset
-            for (const key of person.properties_to_unset) {
-                delete mergedPersonUpdate.properties_to_set[key]
-            }
-
-            mergedPersonUpdate.properties_to_unset = [
-                ...new Set([...existingPersonUpdate.properties_to_unset, ...person.properties_to_unset]),
-            ]
-            // Remove from properties_to_unset any keys that are in the incoming properties_to_set
-            const keysToSet = new Set(Object.keys(person.properties_to_set))
-            mergedPersonUpdate.properties_to_unset = mergedPersonUpdate.properties_to_unset.filter(
-                (key) => !keysToSet.has(key)
-            )
-
-            mergedPersonUpdate.created_at = DateTime.min(existingPersonUpdate.created_at, person.created_at)
-            mergedPersonUpdate.needs_write = existingPersonUpdate.needs_write || person.needs_write
-
-            // Handle force_update with || operator - once true, stays true
-            mergedPersonUpdate.force_update = existingPersonUpdate.force_update || person.force_update
-
-            this.personUpdateCache.set(this.getPersonIdCacheKey(teamId, person.id), mergedPersonUpdate)
-        } else {
-            // First time we're caching this person id
-            this.personUpdateCache.set(this.getPersonIdCacheKey(teamId, person.id), person)
-        }
-    }
-
-    setCheckCachedPerson(teamId: number, distinctId: string, person: InternalPerson | null): void {
-        const cacheKey = this.getDistinctCacheKey(teamId, distinctId)
-        this.personCheckCache.set(cacheKey, person)
-    }
-
-    setDistinctIdToPersonId(teamId: number, distinctId: string, personId: string): void {
-        const cacheKey = this.getDistinctCacheKey(teamId, distinctId)
-        this.distinctIdToPersonId.set(cacheKey, personId)
-    }
-
-    async createPerson(
-        createdAt: DateTime,
-        properties: Properties,
-        propertiesLastUpdatedAt: PropertiesLastUpdatedAt,
-        propertiesLastOperation: PropertiesLastOperation,
-        teamId: Team['id'],
-        isUserId: number | null,
-        isIdentified: boolean,
-        uuid: string,
-        distinctIds?: { distinctId: string; version?: number }[],
-        tx?: PersonRepositoryTransaction
-    ): Promise<CreatePersonResult> {
-        this.incrementCount('createPerson', distinctIds?.[0].distinctId ?? '')
-        this.incrementDatabaseOperation('createPerson', distinctIds?.[0]?.distinctId ?? '')
-        const result = await (tx || this.personRepository).createPerson(
-            createdAt,
-            properties,
-            propertiesLastUpdatedAt,
-            propertiesLastOperation,
-            teamId,
-            isUserId,
-            isIdentified,
-            uuid,
-            distinctIds
-        )
-
-        if (result.success) {
-            const { person } = result
-            this.setCheckCachedPerson(teamId, distinctIds?.[0]?.distinctId ?? '', person)
-            this.setCachedPersonForUpdate(
-                teamId,
-                distinctIds?.[0]?.distinctId ?? '',
-                fromInternalPerson(person, distinctIds?.[0]?.distinctId ?? '')
-            )
-            if (distinctIds?.[1]) {
-                this.setDistinctIdToPersonId(teamId, distinctIds[1].distinctId, person.id)
-                this.setCachedPersonForUpdate(
-                    teamId,
-                    distinctIds[1].distinctId,
-                    fromInternalPerson(person, distinctIds[1].distinctId)
-                )
-            }
-        }
-
-        return result
-    }
-
-    private addPersonUpdateToBatch(
-        person: InternalPerson,
-        update: Partial<InternalPerson>,
-        distinctId: string
-    ): [InternalPerson, TopicMessage[], boolean] {
-        const existingUpdate = this.getCachedPersonForUpdateByDistinctId(person.team_id, distinctId)
-
-        let personUpdate: PersonUpdate
-        if (!existingUpdate) {
-            // Create new PersonUpdate from the person and apply the update
-            personUpdate = fromInternalPerson(person, distinctId)
-            personUpdate = this.mergeUpdateIntoPersonUpdate(personUpdate, update, true)
-            personUpdate.id = person.id
-            this.setCachedPersonForUpdate(person.team_id, distinctId, personUpdate)
-        } else {
-            // Merge updates into existing cached PersonUpdate
-            personUpdate = this.mergeUpdateIntoPersonUpdate(existingUpdate, update, true)
-            personUpdate.id = person.id
-            this.setCachedPersonForUpdate(person.team_id, distinctId, personUpdate)
-        }
-        // Return the merged person from the cache
-        return [toInternalPerson(personUpdate), [], false]
-    }
-
-    /**
-     * Helper method to merge an update into a PersonUpdate
-     * Handles properties and is_identified merging with proper logic
-     */
-    private mergeUpdateIntoPersonUpdate(
-        personUpdate: PersonUpdate,
-        update: Partial<InternalPerson>,
-        allowCreatedAtUpdate: boolean = false
-    ): PersonUpdate {
-        // For properties, we track them in the fine-grained properties_to_set/unset
-        if (update.properties) {
-            // Add all properties from the update to properties_to_set
-            Object.entries(update.properties).forEach(([key, value]) => {
-                personUpdate.properties_to_set[key] = value
-                // Remove from unset list if it was there
-                const unsetIndex = personUpdate.properties_to_unset.indexOf(key)
-                if (unsetIndex !== -1) {
-                    personUpdate.properties_to_unset.splice(unsetIndex, 1)
-                }
-            })
-        }
-
-        // Apply other updates (excluding properties which we handled above)
-        const fieldsToExclude = ['properties', 'is_identified']
-        if (!allowCreatedAtUpdate) {
-            fieldsToExclude.push('created_at')
-        }
-
-        const otherUpdates = Object.fromEntries(
-            Object.entries(update).filter(([key]) => !fieldsToExclude.includes(key))
-        )
-        if (allowCreatedAtUpdate) {
-            // Get minimum of existing and new created_at
-            if (update.created_at) {
-                if (personUpdate.created_at) {
-                    otherUpdates.created_at =
-                        personUpdate.created_at < update.created_at ? personUpdate.created_at : update.created_at
-                } else {
-                    otherUpdates.created_at = update.created_at
-                }
-            }
-        }
-        Object.assign(personUpdate, otherUpdates)
-
-        // Handle is_identified specially with || operator
-        if (update.is_identified !== undefined) {
-            personUpdate.is_identified = personUpdate.is_identified || update.is_identified
-        }
-
-        personUpdate.needs_write = true
-
-        return personUpdate
-    }
-
-    private addPersonPropertiesUpdateToBatch(
-        person: InternalPerson,
-        propertiesToSet: Properties,
-        propertiesToUnset: string[],
-        otherUpdates: Partial<InternalPerson>,
-        distinctId: string,
-        forceUpdate?: boolean
-    ): [InternalPerson, TopicMessage[]] {
-        const existingUpdate = this.getCachedPersonForUpdateByDistinctId(person.team_id, distinctId)
-
-        let personUpdate: PersonUpdate
-        if (!existingUpdate) {
-            // Create new PersonUpdate from the person
-            personUpdate = fromInternalPerson(person, distinctId)
-        } else {
-            // Use existing cached PersonUpdate
-            personUpdate = { ...existingUpdate }
-        }
-
-        // Add properties to set (merge with existing properties_to_set)
-        Object.entries(propertiesToSet).forEach(([key, value]) => {
-            personUpdate.properties_to_set[key] = value
-            // Remove from unset list if it was there
-            const unsetIndex = personUpdate.properties_to_unset.indexOf(key)
-            if (unsetIndex !== -1) {
-                personUpdate.properties_to_unset.splice(unsetIndex, 1)
-            }
-        })
-
-        // Add properties to unset (merge with existing properties_to_unset)
-        propertiesToUnset.forEach((key) => {
-            if (!personUpdate.properties_to_unset.includes(key)) {
-                personUpdate.properties_to_unset.push(key)
-            }
-            // Remove from set list if it was there
-            delete personUpdate.properties_to_set[key]
-        })
-
-        // Handle is_identified specially with || operator
-        if (otherUpdates.is_identified !== undefined) {
-            personUpdate.is_identified = personUpdate.is_identified || otherUpdates.is_identified
-        }
-
-        personUpdate.needs_write = true
-
-        // Set force_update flag with || operator - once set to true by a $identify/$set event, it stays true
-        // This ensures that if any event in the batch requires forcing an update, the whole batch is written
-        if (forceUpdate !== undefined) {
-            personUpdate.force_update = personUpdate.force_update || forceUpdate
-        }
-
-        this.setCachedPersonForUpdate(person.team_id, distinctId, personUpdate)
-        return [toInternalPerson(personUpdate), []]
-    }
-
-    private async updatePersonNoAssert(personUpdate: PersonUpdate): Promise<PersonUpdateResult> {
-        const operation = 'updatePersonNoAssert'
-        this.incrementDatabaseOperation(operation as MethodName, personUpdate.distinct_id)
-        // Convert PersonUpdate back to InternalPerson for database call
-        const person = toInternalPerson(personUpdate)
-        // Always pass all mutable fields for consistent query plans
-        const updateFields = {
-            properties: person.properties,
-            properties_last_updated_at: person.properties_last_updated_at,
-            properties_last_operation: person.properties_last_operation,
-            is_identified: person.is_identified,
-            created_at: person.created_at,
-        }
-
-        this.incrementCount('updatePersonNoAssert', personUpdate.distinct_id)
-        this.incrementDatabaseOperation('updatePersonNoAssert', personUpdate.distinct_id)
-        const start = performance.now()
-
-        const [_, messages] = await this.personRepository.updatePerson(person, updateFields, 'updatePersonNoAssert')
-        this.recordUpdateLatency('updatePersonNoAssert', (performance.now() - start) / 1000, personUpdate.distinct_id)
-        observeLatencyByVersion(person, start, 'updatePersonNoAssert')
-
-        // updatePersonNoAssert always succeeds (no version conflicts)
-        return { success: true, messages }
-    }
-
-    /**
-     * Updates the person in the database by attempting to write to a column where the version is the stored cached
-     * version. If no rows to update are found, the update fails and we retry by reading again from the database.
-     * This method uses no locks but can cause multiple reads from the database.
-     * @param personUpdate the personUpdate to write
-     * @returns the actual version of the person after the write
-     */
-    private async updatePersonAssertVersion(personUpdate: PersonUpdate): Promise<PersonUpdateResult> {
-        this.incrementDatabaseOperation('updatePersonAssertVersion', personUpdate.distinct_id)
-
-        const start = performance.now()
-
-        const [actualVersion, kafkaMessages] = await this.personRepository.updatePersonAssertVersion(personUpdate)
-        this.recordUpdateLatency(
-            'updatePersonAssertVersion',
-            (performance.now() - start) / 1000,
-            personUpdate.distinct_id
-        )
-        observeLatencyByVersion(personUpdate, start, 'updatePersonAssertVersion')
-
-        if (actualVersion !== undefined) {
-            // Success - optimistic update worked, create updated PersonUpdate with new version
-            const updatedPersonUpdate: PersonUpdate = {
-                ...personUpdate,
-                version: actualVersion,
-            }
-            return { success: true, messages: kafkaMessages, personUpdate: updatedPersonUpdate }
-        }
-
-        // Optimistic update failed due to version mismatch
-        personOptimisticUpdateConflictsPerBatchCounter.inc()
-
-        // Fetch latest person data to get current version and properties
-        this.incrementDatabaseOperation('fetchPerson', personUpdate.distinct_id)
-        const latestPerson = await this.personRepository.fetchPerson(personUpdate.team_id, personUpdate.distinct_id)
-
-        if (latestPerson) {
-            // Use fine-grained merge: start with latest properties from DB and apply our specific changes
-            const mergedProperties = { ...latestPerson.properties }
-
-            // Apply our properties_to_set
-            Object.entries(personUpdate.properties_to_set).forEach(([key, value]) => {
-                mergedProperties[key] = value
-            })
-
-            // Apply our properties_to_unset
-            personUpdate.properties_to_unset.forEach((key) => {
-                delete mergedProperties[key]
-            })
-
-            // Create updated PersonUpdate with latest data and merged properties (without mutating input)
-            const updatedPersonUpdate: PersonUpdate = {
-                ...personUpdate,
-                properties: mergedProperties,
-                version: latestPerson.version,
-                uuid: latestPerson.uuid,
-                created_at: latestPerson.created_at,
-                is_identified: latestPerson.is_identified || personUpdate.is_identified,
-            }
-
-            return { success: false, messages: [], personUpdate: updatedPersonUpdate }
-        }
-
-        // If we couldn't fetch the latest person, return failure without a person update
-        return { success: false, messages: [] }
-    }
-
-    private incrementCount(method: MethodName, distinctId: string): void {
-        const methodCounts = this.methodCountsPerDistinctId.get(distinctId) || new Map()
-        methodCounts.set(method, (methodCounts.get(method) || 0) + 1)
-        this.methodCountsPerDistinctId.set(distinctId, methodCounts)
-    }
-
-    private incrementDatabaseOperation(operation: MethodName, distinctId: string): void {
-        const databaseOperationCounts = this.databaseOperationCountsPerDistinctId.get(distinctId) || new Map()
-        databaseOperationCounts.set(operation, (databaseOperationCounts.get(operation) || 0) + 1)
-        this.databaseOperationCountsPerDistinctId.set(distinctId, databaseOperationCounts)
-    }
-
-    private recordUpdateLatency(updateType: UpdateType, latencySeconds: number, distinctId: string): void {
-        const updateLatencyPerDistinctIdSeconds = this.updateLatencyPerDistinctIdSeconds.get(distinctId) || new Map()
-        updateLatencyPerDistinctIdSeconds.set(
-            updateType,
-            (updateLatencyPerDistinctIdSeconds.get(updateType) || 0) + latencySeconds
-        )
-        this.updateLatencyPerDistinctIdSeconds.set(distinctId, updateLatencyPerDistinctIdSeconds)
-    }
-
-    /**
-     * Retry wrapper that handles both update conflicts and person merges.
-     */
-    private async withMergeRetry(
-        personUpdate: PersonUpdate,
-        updateFn: (personUpdate: PersonUpdate) => Promise<PersonUpdateResult>,
-        operation: string,
-        maxRetries: number,
-        retryInterval: number
-    ): Promise<PersonUpdateResult> {
-        let attempt = 0
-        let currentPersonUpdate = personUpdate
-
-        while (attempt <= maxRetries) {
-            try {
-                const result = await updateFn(currentPersonUpdate)
-
-                if (result.success) {
-                    return result
-                }
-
-                // Update failed, handle retry logic
-                attempt++
-                // If there's a person update, we need to update the cache with the latest version
-                if (result.personUpdate) {
-                    currentPersonUpdate = result.personUpdate
-                }
-
-                if (attempt <= maxRetries) {
-                    logger.debug(`Optimistic update conflict for ${operation}, retrying...`, {
-                        attempt,
-                        maxRetries,
-                        teamId: currentPersonUpdate.team_id,
-                        personId: currentPersonUpdate.id,
-                        distinctId: currentPersonUpdate.distinct_id,
-                    })
-
-                    await new Promise((resolve) => setTimeout(resolve, retryInterval))
-                    continue
-                }
-
-                // Max retries reached, throw error to trigger fallback
-                throw new MaxRetriesError(`Max retries reached for ${operation}`, currentPersonUpdate)
-            } catch (error) {
-                attempt++
-
-                if (attempt <= maxRetries) {
-                    // Handle person merge scenarios with special logic
-                    if (error instanceof NoRowsUpdatedError) {
-                        const refreshedPersonUpdate = await this.refreshPersonIdAfterMerge(currentPersonUpdate)
-                        if (refreshedPersonUpdate) {
-                            currentPersonUpdate = refreshedPersonUpdate
-                            continue
-                        }
-                        // If we can't refresh the person ID, we can't retry, fail gracefully
-                        return { success: true, messages: [] }
-                    }
-
-                    // Don't retry size violations - they will never succeed
-                    // throw the error so that we capture an ingestion warning
-                    if (error instanceof PersonPropertiesSizeViolationError) {
-                        throw error
-                    }
-
-                    // For any other error type, still retry but with generic logging
-                    logger.warn(`Database error for ${operation}, retrying...`, {
-                        attempt,
-                        maxRetries,
-                        teamId: currentPersonUpdate.team_id,
-                        personId: currentPersonUpdate.id,
-                        distinctId: currentPersonUpdate.distinct_id,
-                        error: error instanceof Error ? error.message : String(error),
-                    })
-
-                    await new Promise((resolve) => setTimeout(resolve, retryInterval))
-                    continue
-                }
-
-                throw error
-            }
-        }
-
-        // This should never be reached, but TypeScript requires it
-        throw new Error('Unexpected end of retry loop')
-    }
-
-    /**
-     * Refreshes the person ID for a given distinct ID by fetching from the database.
-     * This handles cases where the person was merged and the ID changed.
-     * @param personUpdate the PersonUpdate that failed to update
-     * @returns updated PersonUpdate with new person ID if found, null if person no longer exists
-     */
-    private async refreshPersonIdAfterMerge(personUpdate: PersonUpdate): Promise<PersonUpdate | null> {
-        const currentPerson = await this.personRepository.fetchPerson(personUpdate.team_id, personUpdate.distinct_id)
-
-        if (!currentPerson) {
-            // Person truly doesn't exist anymore
-            return null
-        }
-
-        // Clear the old person ID from cache since it's been merged
-        this.clearPersonCacheForPersonId(personUpdate.team_id, personUpdate.id)
-
-        // Update our cache mapping to reflect the new person ID
-        this.setDistinctIdToPersonId(personUpdate.team_id, personUpdate.distinct_id, currentPerson.id)
-
-        // Create updated PersonUpdate with the new person ID and version
-        const updatedPersonUpdate: PersonUpdate = {
-            id: currentPerson.id,
-            team_id: personUpdate.team_id,
-            uuid: currentPerson.uuid,
-            distinct_id: personUpdate.distinct_id,
-            properties: currentPerson.properties,
-            properties_last_updated_at: personUpdate.properties_last_updated_at,
-            properties_last_operation: personUpdate.properties_last_operation,
-            created_at: currentPerson.created_at,
-            version: currentPerson.version,
-            is_identified: currentPerson.is_identified || personUpdate.is_identified,
-            is_user_id: personUpdate.is_user_id,
-            needs_write: personUpdate.needs_write,
-            properties_to_set: personUpdate.properties_to_set,
-            properties_to_unset: personUpdate.properties_to_unset,
-            original_is_identified: personUpdate.original_is_identified,
-            original_created_at: personUpdate.original_created_at,
-        }
-
-        return updatedPersonUpdate
-    }
-}
+                                const fallbackResult
\ No newline at end of file
diff --git a/plugin-server/src/worker/ingestion/persons/person-update.test.ts b/plugin-server/src/worker/ingestion/persons/person-update.test.ts
index 9790636..114e513 100644
--- a/plugin-server/src/worker/ingestion/persons/person-update.test.ts
+++ b/plugin-server/src/worker/ingestion/persons/person-update.test.ts
@@ -153,49 +153,7 @@ describe('person-update', () => {
                 }
             )
 
-            it('should accept blocked $geoip_* property updates at event level (filtering happens at batch level)', () => {
-                const event: PluginEvent = {
-                    event: 'pageview',
-                    properties: {
-                        $set: { $geoip_latitude: 37.7749 },
-                    },
-                } as any
-
-                const personProperties = { $geoip_latitude: 40.7128 }
-
-                const result = computeEventPropertyUpdates(event, personProperties)
-
-                expect(result.hasChanges).toBe(true)
-                expect(result.toSet).toEqual({ $geoip_latitude: 37.7749 })
-                expect(result.shouldForceUpdate).toBe(false)
-                // At event level, blocked geoip properties would be marked as ignored
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'ignored' })
-                expect(mockPersonProfileIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
-                    property: '$geoip_latitude',
-                })
-            })
-
-            it('should trigger update when $geoip_country_name changes (allowed geoip property)', () => {
-                const event: PluginEvent = {
-                    event: 'pageview',
-                    properties: {
-                        $set: { $geoip_country_name: 'United States' },
-                    },
-                } as any
-
-                const personProperties = { $geoip_country_name: 'Canada' }
-
-                const result = computeEventPropertyUpdates(event, personProperties)
-
-                expect(result.hasChanges).toBe(true)
-                expect(result.toSet).toEqual({ $geoip_country_name: 'United States' })
-                expect(result.shouldForceUpdate).toBe(false)
-                // $geoip_country_name is allowed so should be marked as changed
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-                expect(mockPersonProfileIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            })
-
-            it('should trigger update when $geoip_city_name changes (allowed geoip property)', () => {
+            it('should accept $geoip_* property updates at event level (filtering happens at batch level)', () => {
                 const event: PluginEvent = {
                     event: 'pageview',
                     properties: {
@@ -210,46 +168,14 @@ describe('person-update', () => {
                 expect(result.hasChanges).toBe(true)
                 expect(result.toSet).toEqual({ $geoip_city_name: 'San Francisco' })
                 expect(result.shouldForceUpdate).toBe(false)
-                // $geoip_city_name is allowed so should be marked as changed
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-                expect(mockPersonProfileIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            })
-
-            it('should update all geoip properties when allowed property ($geoip_country_name) changes alongside blocked ones', () => {
-                const event: PluginEvent = {
-                    event: 'pageview',
-                    properties: {
-                        $set: {
-                            $geoip_country_name: 'United States',
-                            $geoip_latitude: 37.7749,
-                            $geoip_longitude: -122.4194,
-                            $geoip_postal_code: '94102',
-                        },
-                    },
-                } as any
-
-                const personProperties = {
-                    $geoip_country_name: 'Canada',
-                    $geoip_latitude: 43.6532,
-                    $geoip_longitude: -79.3832,
-                    $geoip_postal_code: 'M5V',
-                }
-
-                const result = computeEventPropertyUpdates(event, personProperties)
-
-                expect(result.hasChanges).toBe(true)
-                expect(result.toSet).toEqual({
-                    $geoip_country_name: 'United States',
-                    $geoip_latitude: 37.7749,
-                    $geoip_longitude: -122.4194,
-                    $geoip_postal_code: '94102',
+                // At event level, geoip properties would be marked as ignored
+                expect(mockPersonProfileUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'ignored' })
+                expect(mockPersonProfileIgnoredPropertiesCounter.labels).toHaveBeenCalledWith({
+                    property: '$geoip_city_name',
                 })
-                expect(result.shouldForceUpdate).toBe(false)
-                // Since $geoip_country_name is allowed, the update is marked as changed (not ignored)
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
             })
 
-            it('should accept filtered properties even when mixed with unchanged custom properties', () => {
+            it('should accept eventToPersonProperties even when mixed with unchanged custom properties', () => {
                 const event: PluginEvent = {
                     event: 'pageview',
                     properties: {
@@ -450,255 +376,4 @@ describe('person-update', () => {
                 const event: PluginEvent = {
                     event: '$$heatmap',
                     properties: {
-                        $set: { custom_prop: 'new_value' },
-                    },
-                } as any
-
-                const personProperties = {}
-
-                const result = computeEventPropertyUpdates(event, personProperties)
-
-                expect(result.hasChanges).toBe(false)
-                expect(result.shouldForceUpdate).toBe(false)
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'unsupported' })
-            })
-        })
-
-        describe('mixed scenarios', () => {
-            it('should compute updates when both custom and allowed properties change', () => {
-                const event: PluginEvent = {
-                    event: 'pageview',
-                    properties: {
-                        $set: { custom_prop: 'new_value', $browser: 'Chrome' },
-                    },
-                } as any
-
-                const personProperties = { custom_prop: 'old_value', $browser: 'Firefox' }
-
-                const result = computeEventPropertyUpdates(event, personProperties)
-
-                expect(result.hasChanges).toBe(true)
-                expect(result.toSet).toEqual({ custom_prop: 'new_value', $browser: 'Chrome' })
-                expect(result.shouldForceUpdate).toBe(false)
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'changed' })
-            })
-        })
-
-        describe('updateAllProperties flag enabled', () => {
-            it.each(Array.from(FILTERED_PERSON_UPDATE_PROPERTIES))(
-                'should trigger update for filtered property "%s" when updateAllProperties is true',
-                (propertyName) => {
-                    const event: PluginEvent = {
-                        event: 'pageview',
-                        properties: {
-                            $set: { [propertyName]: 'new_value' },
-                        },
-                    } as any
-
-                    const personProperties = { [propertyName]: 'old_value' }
-
-                    const result = computeEventPropertyUpdates(event, personProperties, true)
-
-                    expect(result.hasChanges).toBe(true)
-                    expect(result.toSet).toEqual({ [propertyName]: 'new_value' })
-                    expect(result.shouldForceUpdate).toBe(true) // updateAllProperties forces updates
-                    // With updateAllProperties=true, no metrics should be tracked
-                    expect(mockPersonProfileUpdateOutcomeCounter.labels).not.toHaveBeenCalled()
-                    expect(mockPersonProfileIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-                }
-            )
-
-            it('should trigger update for $geoip_* properties when updateAllProperties is true', () => {
-                const event: PluginEvent = {
-                    event: 'pageview',
-                    properties: {
-                        $set: { $geoip_city_name: 'San Francisco' },
-                    },
-                } as any
-
-                const personProperties = { $geoip_city_name: 'New York' }
-
-                const result = computeEventPropertyUpdates(event, personProperties, true)
-
-                expect(result.hasChanges).toBe(true)
-                expect(result.toSet).toEqual({ $geoip_city_name: 'San Francisco' })
-                expect(result.shouldForceUpdate).toBe(true) // updateAllProperties forces updates
-                // With updateAllProperties=true, no metrics should be tracked
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).not.toHaveBeenCalled()
-                expect(mockPersonProfileIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            })
-
-            it('should trigger update for multiple allowed properties when updateAllProperties is true', () => {
-                const event: PluginEvent = {
-                    event: 'pageview',
-                    properties: {
-                        $set: {
-                            $browser: 'Chrome',
-                            $os: 'macOS',
-                        },
-                    },
-                } as any
-
-                const personProperties = {
-                    $browser: 'Firefox',
-                    $os: 'Windows',
-                }
-
-                const result = computeEventPropertyUpdates(event, personProperties, true)
-
-                expect(result.hasChanges).toBe(true)
-                expect(result.toSet).toEqual({ $browser: 'Chrome', $os: 'macOS' })
-                expect(result.shouldForceUpdate).toBe(true) // updateAllProperties forces updates
-                // With updateAllProperties=true, no metrics should be tracked
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).not.toHaveBeenCalled()
-                expect(mockPersonProfileIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            })
-
-            it('should trigger update for mixed allowed and custom properties when updateAllProperties is true', () => {
-                const event: PluginEvent = {
-                    event: 'pageview',
-                    properties: {
-                        $set: { $browser: 'Chrome', custom_prop: 'same_value' },
-                    },
-                } as any
-
-                const personProperties = { $browser: 'Firefox', custom_prop: 'same_value' }
-
-                const result = computeEventPropertyUpdates(event, personProperties, true)
-
-                expect(result.hasChanges).toBe(true)
-                expect(result.toSet).toEqual({ $browser: 'Chrome' })
-                expect(result.shouldForceUpdate).toBe(true) // updateAllProperties forces updates
-                // With updateAllProperties=true, no metrics should be tracked
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).not.toHaveBeenCalled()
-                expect(mockPersonProfileIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            })
-
-            it('should trigger update for mixed $geoip_* and allowed properties when updateAllProperties is true', () => {
-                const event: PluginEvent = {
-                    event: 'pageview',
-                    properties: {
-                        $set: {
-                            $browser: 'Chrome',
-                            $geoip_city_name: 'San Francisco',
-                            $geoip_country_code: 'US',
-                        },
-                    },
-                } as any
-
-                const personProperties = {
-                    $browser: 'Firefox',
-                    $geoip_city_name: 'New York',
-                    $geoip_country_code: 'CA',
-                }
-
-                const result = computeEventPropertyUpdates(event, personProperties, true)
-
-                expect(result.hasChanges).toBe(true)
-                expect(result.toSet).toEqual({
-                    $browser: 'Chrome',
-                    $geoip_city_name: 'San Francisco',
-                    $geoip_country_code: 'US',
-                })
-                expect(result.shouldForceUpdate).toBe(true) // updateAllProperties forces updates
-                // With updateAllProperties=true, no metrics should be tracked
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).not.toHaveBeenCalled()
-                expect(mockPersonProfileIgnoredPropertiesCounter.labels).not.toHaveBeenCalled()
-            })
-
-            it('should not change behavior for NO_PERSON_UPDATE_EVENTS when updateAllProperties is true', () => {
-                const event: PluginEvent = {
-                    event: '$exception',
-                    properties: {
-                        $set: { $browser: 'Chrome' },
-                    },
-                } as any
-
-                const personProperties = { $browser: 'Firefox' }
-
-                const result = computeEventPropertyUpdates(event, personProperties, true)
-
-                // NO_PERSON_UPDATE_EVENTS should still be skipped regardless of flag
-                expect(result.hasChanges).toBe(false)
-                expect(result.toSet).toEqual({})
-                expect(result.shouldForceUpdate).toBe(false)
-                expect(mockPersonProfileUpdateOutcomeCounter.labels).toHaveBeenCalledWith({ outcome: 'unsupported' })
-            })
-        })
-    })
-
-    describe('applyEventPropertyUpdates', () => {
-        it('should apply property updates and return updated person', () => {
-            const propertyUpdates = {
-                hasChanges: true,
-                toSet: { name: 'John', email: 'john@example.com' },
-                toUnset: ['old_prop'],
-                shouldForceUpdate: false,
-            }
-
-            const person = {
-                id: '1',
-                team_id: 123,
-                uuid: 'test-uuid',
-                properties: { old_prop: 'value', name: 'Jane' },
-                created_at: new Date(),
-                version: 0,
-                is_identified: false,
-            }
-
-            const [updatedPerson, wasUpdated] = applyEventPropertyUpdates(propertyUpdates, person as any)
-
-            expect(wasUpdated).toBe(true)
-            expect(updatedPerson.properties).toEqual({ name: 'John', email: 'john@example.com' })
-            expect(updatedPerson.properties.old_prop).toBeUndefined()
-        })
-
-        it('should not modify original person object', () => {
-            const propertyUpdates = {
-                hasChanges: true,
-                toSet: { name: 'John' },
-                toUnset: [],
-                shouldForceUpdate: false,
-            }
-
-            const person = {
-                id: '1',
-                team_id: 123,
-                uuid: 'test-uuid',
-                properties: { name: 'Jane' },
-                created_at: new Date(),
-                version: 0,
-                is_identified: false,
-            }
-
-            const [updatedPerson, _] = applyEventPropertyUpdates(propertyUpdates, person as any)
-
-            expect(person.properties.name).toBe('Jane')
-            expect(updatedPerson.properties.name).toBe('John')
-            expect(person).not.toBe(updatedPerson)
-        })
-
-        it('should return false for wasUpdated when no actual changes occur', () => {
-            const propertyUpdates = {
-                hasChanges: false,
-                toSet: { name: 'John' },
-                toUnset: [],
-                shouldForceUpdate: false,
-            }
-
-            const person = {
-                id: '1',
-                team_id: 123,
-                uuid: 'test-uuid',
-                properties: { name: 'John' },
-                created_at: new Date(),
-                version: 0,
-                is_identified: false,
-            }
-
-            const [_, wasUpdated] = applyEventPropertyUpdates(propertyUpdates, person as any)
-
-            expect(wasUpdated).toBe(false)
-        })
-    })
-})
+                        $set: {
\ No newline at end of file
diff --git a/plugin-server/src/worker/ingestion/persons/person-update.ts b/plugin-server/src/worker/ingestion/persons/person-update.ts
index 1b2ebca..6ed334f 100644
--- a/plugin-server/src/worker/ingestion/persons/person-update.ts
+++ b/plugin-server/src/worker/ingestion/persons/person-update.ts
@@ -80,30 +80,20 @@ export function computeEventPropertyUpdates(
         }
     })
 
-    // First pass: detect if any property would trigger an update
-    // If so, all changed properties in this $set should be updated together
-    let anyPropertyTriggersUpdate = false
-    const changedProperties: Array<[string, unknown]> = []
-
     Object.entries(properties).forEach(([key, value]) => {
         if (personProperties[key] !== value) {
-            changedProperties.push([key, value])
             const isNewProperty = typeof personProperties[key] === 'undefined'
-            if (isNewProperty || shouldUpdatePersonIfOnlyChange(event, key, updateAllProperties)) {
-                anyPropertyTriggersUpdate = true
-            }
-        }
-    })
+            const shouldUpdate = isNewProperty || shouldUpdatePersonIfOnlyChange(event, key, updateAllProperties)
 
-    // Second pass: apply changes - if any property triggers update, all do
-    changedProperties.forEach(([key, value]) => {
-        hasChanges = true
-        if (anyPropertyTriggersUpdate) {
-            hasNonFilteredChanges = true
-        } else {
-            ignoredProperties.push(key)
+            if (shouldUpdate) {
+                hasChanges = true
+                hasNonFilteredChanges = true
+            } else {
+                hasChanges = true
+                ignoredProperties.push(key)
+            }
+            toSet[key] = value
         }
-        toSet[key] = value
     })
 
     unsetProperties.forEach((propertyKey) => {
@@ -184,5 +174,13 @@ function shouldUpdatePersonIfOnlyChange(event: PluginEvent, key: string, updateA
         // for person events always update everything
         return true
     }
-    return !isFilteredPersonUpdateProperty(key)
-}
+    // These are properties we add from the event and some change often, it's useless to update person always
+    if (eventToPersonProperties.has(key)) {
+        return false
+    }
+    // same as above, coming from GeoIP plugin
+    if (key.startsWith('$geoip_')) {
+        return false
+    }
+    return true
+}
\ No newline at end of file
